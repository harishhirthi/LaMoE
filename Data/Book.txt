Inventors have long dreamed of creating machines that think. This desire dates
back to at least the time of ancient Greece. The mythical figures Pygmalion,
Daedalus, and Hephaestus may all be interpreted as legendary inventors, and
Galatea, Talos, and Pandora may all be regarded as artificial life (Ovid and Martin, 2004; Sparkes, 1996; Tandy, 1997). When programmable computers were first conceived, people wondered whether
such machines might become intelligent, over a hundred years before one was
built (Lovelace, 1842). Today, artificial intelligence (AI) is a thriving field with
many practical applications and active research topics. We look to intelligent
software to automate routine labor, understand speech or images, make diagnoses
in medicine and support basic scientific research.
In the early days of artificial intelligence, the field rapidly tackled and solved
problems that are intellectually difficult for human beings but relatively straightforward for computers—problems that can be described by a list of formal, mathematical rules. The true challenge to artificial intelligence proved to be solving
the tasks that are easy for people to perform but hard for people to describe
formally—problems that we solve intuitively, that feel automatic, like recognizing
spoken words or faces in images. This book is about a solution to these more intuitive problems. This solution is
to allow computers to learn from experience and understand the world in terms of a
hierarchy of concepts, with each concept defined in terms of its relation to simpler
concepts. By gathering knowledge from experience, this approach avoids the need
for human operators to formally specify all of the knowledge that the computer
needs. The hierarchy of concepts allows the computer to learn complicated concepts
by building them out of simpler ones. If we draw a graph showing how these
concepts are built on top of each other, the graph is deep, with many layers. For
this reason, we call this approach to AI deep learning. Many of the early successes of AI took place in relatively sterile and formal
environments and did not require computers to have much knowledge about
the world. For example, IBM’s Deep Blue chess-playing system defeated world
champion Garry Kasparov in 1997 (Hsu, 2002). Chess is of course a very simple
world, containing only sixty-four locations and thirty-two pieces that can move
in only rigidly circumscribed ways. Devising a successful chess strategy is a
tremendous accomplishment, but the challenge is not due to the difficulty of
describing the set of chess pieces and allowable moves to the computer. Chess
can be completely described by a very brief list of completely formal rules, easily
provided ahead of time by the programmer.
Ironically, abstract and formal tasks that are among the most difficult mental
undertakings for a human being are among the easiest for a computer. Computers
have long been able to defeat even the best human chess player, but are only
recently matching some of the abilities of average human beings to recognize objects
or speech. A person’s everyday life requires an immense amount of knowledge
about the world. Much of this knowledge is subjective and intuitive, and therefore
difficult to articulate in a formal way. Computers need to capture this same
knowledge in order to behave in an intelligent way. One of the key challenges in
artificial intelligence is how to get this informal knowledge into a computer.
Several artificial intelligence projects have sought to hard-code knowledge about
the world in formal languages. A computer can reason about statements in these
formal languages automatically using logical inference rules. This is known as the
knowledge base approach to artificial intelligence. None of these projects has led
to a major success. One of the most famous such projects is Cyc (Lenat and Guha, 1989). Cyc is an inference engine and a database of statements in a language
called CycL. These statements are entered by a staff of human supervisors. It is an
unwieldy process. People struggle to devise formal rules with enough complexity
to accurately describe the world. For example, Cyc failed to understand a story
about a person named Fred shaving in the morning (Linde, 1992). Its inference
engine detected an inconsistency in the story: it knew that people do not have
electrical parts, but because Fred was holding an electric razor, it believed the
entity “FredWhileShaving” contained electrical parts. It therefore asked whether
Fred was still a person while he was shaving.
The difficulties faced by systems relying on hard-coded knowledge suggest
that AI systems need the ability to acquire their own knowledge, by extracting
patterns from raw data. This capability is known as machine learning. The
introduction of machine learning allowed computers to tackle problems involving
knowledge of the real world and make decisions that appear subjective. A simple
machine learning algorithm called logistic regression can determine whether to
recommend cesarean delivery (Mor-Yosef et al., 1990). A simple machine learning
algorithm called naive Bayes can separate legitimate e-mail from spam e-mail.
The performance of these simple machine learning algorithms depends heavily
on the representation of the data they are given. For example, when logistic
regression is used to recommend cesarean delivery, the AI system does not examine
the patient directly. Instead, the doctor tells the system several pieces of relevant
information, such as the presence or absence of a uterine scar. Each piece of
information included in the representation of the patient is known as a feature. Logistic regression learns how each of these features of the patient correlates with
various outcomes. However, it cannot influence the way that the features are
defined in any way. If logistic regression was given an MRI scan of the patient,
rather than the doctor’s formalized report, it would not be able to make useful
predictions. Individual pixels in an MRI scan have negligible correlation with any
complications that might occur during delivery.This dependence on representations is a general phenomenon that appears
throughout computer science and even daily life. In computer science, operations such as searching a collection of data can proceed exponentially faster if
the collection is structured and indexed intelligently. People can easily perform
arithmetic on Arabic numerals, but find arithmetic on Roman numerals much
more time-consuming. It is not surprising that the choice of representation has an
enormous effect on the performance of machine learning algorithms. For a simple
visual example, see figure 1.1. Many artificial intelligence tasks can be solved by designing the right set of
features to extract for that task, then providing these features to a simple machine
learning algorithm. For example, a useful feature for speaker identification from
sound is an estimate of the size of speaker’s vocal tract. It therefore gives a strong
clue as to whether the speaker is a man, woman, or child.
However, for many tasks, it is difficult to know what features should be extracted. For example, suppose that we would like to write a program to detect cars in
photographs. We know that cars have wheels, so we might like to use the presence
of a wheel as a feature. Unfortunately, it is difficult to describe exactly what a
wheel looks like in terms of pixel values. A wheel has a simple geometric shape but
its image may be complicated by shadows falling on the wheel, the sun glaring off
the metal parts of the wheel, the fender of the car or an object in the foreground
obscuring part of the wheel, and so on
In which we try to explain why we consider artificial intelligence to be a subject
most worthy of study, and in which we try to decide what exactly it is, this being a
good thing to decide before embarking.
INTELLIGENCE We call ourselves Homo sapiens—man the wise—because our intelligence is so important
to us. For thousands of years, we have tried to understand how we think; that is, how a mere
handful of matter can perceive, understand, predict, and manipulate a world far larger and
more complicated than itself. The field of artificial intelligence, or AI, goes further still: it ARTIFICIAL
INTELLIGENCE
attempts not just to understand but also to build intelligent entities.
AI is one of the newest fields in science and engineering. Work started in earnest soon
after World War II, and the name itself was coined in 1956. Along with molecular biology,
AI is regularly cited as the “field I would most like to be in” by scientists in other disciplines.
A student in physics might reasonably feel that all the good ideas have already been taken by
Galileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings for several
full-time Einsteins and Edisons.
AI currently encompasses a huge variety of subfields, ranging from the general (learning
and perception) to the specific, such as playing chess, proving mathematical theorems, writing
poetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any
intellectual task; it is truly a universal field.
1.1 WHAT IS AI?
We have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see
eight definitions of AI, laid out along two dimensions. The definitions on top are concerned
with thought processes and reasoning, whereas the ones on the bottom address behavior. The
definitions on the left measure success in terms of fidelity to human performance, whereas
RATIONALITY the ones on the right measure against an ideal performance measure, called rationality. A
system is rational if it does the “right thing,” given what it knows.
Historically, all four approaches to AI have been followed, each by different people
with different methods. A human-centered approach must be in part an empirical science
Thinking Humanly Thinking Rationally
“The exciting new effort to make computers think ... machines with minds, in the
full and literal sense.” (Haugeland, 1985)
“The study of mental faculties through the
use of computational models.”
(Charniak and McDermott, 1985)
“[The automation of] activities that we
associate with human thinking, activities
such as decision-making, problem solving, learning ...” (Bellman, 1978)
“The study of the computations that make
it possible to perceive, reason, and act.”
(Winston, 1992)
Acting Humanly Acting Rationally
“The art of creating machines that perform functions that require intelligence
when performed by people.” (Kurzweil,
1990)
“Computational Intelligence is the study
of the design of intelligent agents.” (Poole
et al., 1998)
“The study of how to make computers do
things at which, at the moment, people are
better.” (Rich and Knight, 1991)
“AI . . . is concerned with intelligent behavior in artifacts.” (Nilsson, 1998)
Figure 1.1 Some definitions of artificial intelligence, organized into four categories.
volving observations and hypotheses about human behavior. A rationalist1 approach involves
a combination of mathematics and engineering. The various group have both disparaged and
helped each other. Let us look at the four approaches in more detail.
1.1.1 Acting humanly: The Turing Test approach
TURING TEST The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory
operational definition of intelligence. A computer passes the test if a human interrogator, after
posing some written questions, cannot tell whether the written responses come from a person
or from a computer. Chapter 26 discusses the details of the test and whether a computer would
really be intelligent if it passed. For now, we note that programming a computer to pass a
rigorously applied test provides plenty to work on. The computer would need to possess the
following capabilities:
• natural language processing to enable it to communicate successfully in English; NATURAL LANGUAGE
PROCESSING
• knowledge representation to store what it knows or hears; KNOWLEDGE
REPRESENTATION
AUTOMATED
REASONING • automated reasoning to use the stored information to answer questions and to draw
new conclusions;
MACHINE LEARNING • machine learning to adapt to new circumstances and to detect and extrapolate patterns.
1 By distinguishing between human and rational behavior, we are not suggesting that humans are necessarily
“irrational” in the sense of “emotionally unstable” or “insane.” One merely need note that we are not perfect:
not all chess players are grandmasters; and, unfortunately, not everyone gets an A on the exam. Some systematic
errors in human reasoning are cataloged by Kahneman et al. (1982).
Section 1.1. What Is AI? 3
Turing’s test deliberately avoided direct physical interaction between the interrogator and the
computer, because physical simulation of a person is unnecessary for intelligence. However,
TOTAL TURING TEST the so-called total Turing Test includes a video signal so that the interrogator can test the
subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical
objects “through the hatch.” To pass the total Turing Test, the computer will need
COMPUTER VISION • computer vision to perceive objects, and
ROBOTICS • robotics to manipulate objects and move about.
These six disciplines compose most of AI, and Turing deserves credit for designing a test
that remains relevant 60 years later. Yet AI researchers have devoted little effort to passing
the Turing Test, believing that it is more important to study the underlying principles of intelligence than to duplicate an exemplar. The quest for “artificial flight” succeeded when the
Wright brothers and others stopped imitating birds and started using wind tunnels and learning about aerodynamics. Aeronautical engineering texts do not define the goal of their field
as making “machines that fly so exactly like pigeons that they can fool even other pigeons.”
1.1.2 Thinking humanly: The cognitive modeling approach
If we are going to say that a given program thinks like a human, we must have some way of
determining how humans think. We need to get inside the actual workings of human minds.
There are three ways to do this: through introspection—trying to catch our own thoughts as
they go by; through psychological experiments—observing a person in action; and through
brain imaging—observing the brain in action. Once we have a sufficiently precise theory of
the mind, it becomes possible to express the theory as a computer program. If the program’s
input–output behavior matches corresponding human behavior, that is evidence that some of
the program’s mechanisms could also be operating in humans. For example, Allen Newell
and Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon,
1961), were not content merely to have their program solve problems correctly. They were
more concerned with comparing the trace of its reasoning steps to traces of human subjects
COGNITIVE SCIENCE solving the same problems. The interdisciplinary field of cognitive science brings together
computer models from AI and experimental techniques from psychology to construct precise
and testable theories of the human mind.
Cognitive science is a fascinating field in itself, worthy of several textbooks and at least
one encyclopedia (Wilson and Keil, 1999). We will occasionally comment on similarities or
differences between AI techniques and human cognition. Real cognitive science, however, is
necessarily based on experimental investigation of actual humans or animals. We will leave
that for other books, as we assume the reader has only a computer for experimentation.
In the early days of AI there was often confusion between the approaches: an author
would argue that an algorithm performs well on a task and that it is therefore a good model
of human performance, or vice versa. Modern authors separate the two kinds of claims;
this distinction has allowed both AI and cognitive science to develop more rapidly. The two
fields continue to fertilize each other, most notably in computer vision, which incorporates
neurophysiological evidence into computational models.
1.1.3 Thinking rationally: The “laws of thought” approach
The Greek philosopher Aristotle was one of the first to attempt to codify “right thinking,” that
SYLLOGISM is, irrefutable reasoning processes. His syllogisms provided patterns for argument structures
that always yielded correct conclusions when given correct premises—for example, “Socrates
is a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were
LOGIC supposed to govern the operation of the mind; their study initiated the field called logic.
Logicians in the 19th century developed a precise notation for statements about all kinds
of objects in the world and the relations among them. (Contrast this with ordinary arithmetic
notation, which provides only for statements about numbers.) By 1965, programs existed
that could, in principle, solve any solvable problem described in logical notation. (Although
LOGICIST if no solution exists, the program might loop forever.) The so-called logicist tradition within
artificial intelligence hopes to build on such programs to create intelligent systems.
There are two main obstacles to this approach. First, it is not easy to take informal
knowledge and state it in the formal terms required by logical notation, particularly when
the knowledge is less than 100% certain. Second, there is a big difference between solving
a problem “in principle” and solving it in practice. Even problems with just a few hundred
facts can exhaust the computational resources of any computer unless it has some guidance
as to which reasoning steps to try first. Although both of these obstacles apply to any attempt
to build computational reasoning systems, they appeared first in the logicist tradition.
1.1.4 Acting rationally: The rational agent approach
AGENT An agent is just something that acts (agent comes from the Latin agere, to do). Of course,
all computer programs do something, but computer agents are expected to do more: operate
autonomously, perceive their environment, persist over a prolonged time period, adapt to
RATIONAL AGENT change, and create and pursue goals. A rational agent is one that acts so as to achieve the
best outcome or, when there is uncertainty, the best expected outcome.
In the “laws of thought” approach to AI, the emphasis was on correct inferences. Making correct inferences is sometimes part of being a rational agent, because one way to act
rationally is to reason logically to the conclusion that a given action will achieve one’s goals
and then to act on that conclusion. On the other hand, correct inference is not all of rationality; in some situations, there is no provably correct thing to do, but something must still be
done. There are also ways of acting rationally that cannot be said to involve inference. For
example, recoiling from a hot stove is a reflex action that is usually more successful than a
slower action taken after careful deliberation.
All the skills needed for the Turing Test also allow an agent to act rationally. Knowledge
representation and reasoning enable agents to reach good decisions. We need to be able to
generate comprehensible sentences in natural language to get by in a complex society. We
need learning not only for erudition, but also because it improves our ability to generate
effective behavior.
The rational-agent approach has two advantages over the other approaches. First, it
is more general than the “laws of thought” approach because correct inference is just one
of several possible mechanisms for achieving rationality. Second, it is more amenable to
Section 1.2. The Foundations of Artificial Intelligence 5
scientific development than are approaches based on human behavior or human thought. The
standard of rationality is mathematically well defined and completely general, and can be
“unpacked” to generate agent designs that provably achieve it. Human behavior, on the other
hand, is well adapted for one specific environment and is defined by, well, the sum total
of all the things that humans do. This book therefore concentrates on general principles
of rational agents and on components for constructing them. We will see that despite the
apparent simplicity with which the problem can be stated, an enormous variety of issues
come up when we try to solve it. Chapter 2 outlines some of these issues in more detail.
One important point to keep in mind: We will see before too long that achieving perfect
rationality—always doing the right thing—is not feasible in complicated environments. The
computational demands are just too high. For most of the book, however, we will adopt the
working hypothesis that perfect rationality is a good starting point for analysis. It simplifies
the problem and provides the appropriate setting for most of the foundational material in
the field. Chapters 5 and 17 deal explicitly with the issue of limited rationality—acting LIMITED
RATIONALITY
appropriately when there is not enough time to do all the computations one might like.
In this section, we provide a brief history of the disciplines that contributed ideas, viewpoints,
and techniques to AI. Like any history, this one is forced to concentrate on a small number
of people, events, and ideas and to ignore others that also were important. We organize the
history around a series of questions. We certainly would not wish to give the impression that
these questions are the only ones the disciplines address or that the disciplines have all been
working toward AI as their ultimate fruition.
• Can formal rules be used to draw valid conclusions?
• How does the mind arise from a physical brain?
• Where does knowledge come from?
• How does knowledge lead to action?
Aristotle (384–322 B.C.), whose bust appears on the front cover of this book, was the first
to formulate a precise set of laws governing the rational part of the mind. He developed an
informal system of syllogisms for proper reasoning, which in principle allowed one to generate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had
the idea that useful reasoning could actually be carried out by a mechanical artifact. Thomas
Hobbes (1588–1679) proposed that reasoning was like numerical computation, that “we add
and subtract in our silent thoughts.” The automation of computation itself was already well
under way. Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a mechanical calculator; recent reconstructions have shown the design to be functional. The first
known calculating machine was constructed around 1623 by the German scientist Wilhelm
Schickard (1592–1635), although the Pascaline, built in 1642 by Blaise Pascal (1623–1662),is more famous. Pascal wrote that “the arithmetical machine produces effects which appear
nearer to thought than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)
built a mechanical device intended to carry out operations on concepts rather than numbers,
but its scope was rather limited. Leibniz did surpass Pascal by building a calculator that
could add, subtract, multiply, and take roots, whereas the Pascaline could only add and subtract. Some speculated that machines might not just do calculations but actually be able to
think and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea
of an “artificial animal,” arguing “For what is the heart but a spring; and the nerves, but so
many strings; and the joints, but so many wheels.”
It’s one thing to say that the mind operates, at least in part, according to logical rules, and
to build physical systems that emulate some of those rules; it’s another to say that the mind
itself is such a physical system. Ren´e Descartes (1596–1650) gave the first clear discussion
of the distinction between mind and matter and of the problems that arise. One problem with
a purely physical conception of the mind is that it seems to leave little room for free will:
if the mind is governed entirely by physical laws, then it has no more free will than a rock
“deciding” to fall toward the center of the earth. Descartes was a strong advocate of the power
RATIONALISM of reasoning in understanding the world, a philosophy now called rationalism, and one that
DUALISM counts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.
He held that there is a part of the human mind (or soul or spirit) that is outside of nature,
exempt from physical laws. Animals, on the other hand, did not possess this dual quality;
MATERIALISM they could be treated as machines. An alternative to dualism is materialism, which holds
that the brain’s operation according to the laws of physics constitutes the mind. Free will is
simply the way that the perception of available choices appears to the choosing entity.
Given a physical mind that manipulates knowledge, the next problem is to establish
EMPIRICISM the source of knowledge. The empiricism movement, starting with Francis Bacon’s (1561–
1626) Novum Organum,
2 is characterized by a dictum of John Locke (1632–1704): “Nothing
is in the understanding, which was not first in the senses.” David Hume’s (1711–1776) A
Treatise of Human Nature (Hume, 1739) proposed what is now known as the principle of
INDUCTION induction: that general rules are acquired by exposure to repeated associations between their
elements. Building on the work of Ludwig Wittgenstein (1889–1951) and Bertrand Russell
(1872–1970), the famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed the
LOGICAL POSITIVISM doctrine of logical positivism. This doctrine holds that all knowledge can be characterized by
logical theories connected, ultimately, to observation sentences that correspond to sensory OBSERVATION
SENTENCES
inputs; thus logical positivism combines rationalism and empiricism.3 The confirmation theory of Carnap and Carl Hempel (1905–1997) attempted to analyze the acquisition of knowl- CONFIRMATION
THEORY
edge from experience. Carnap’s book The Logical Structure of the World (1928) defined an
explicit computational procedure for extracting knowledge from elementary experiences. It
was probably the first theory of mind as a computational process.
2 The Novum Organum is an update of Aristotle’s Organon, or instrument of thought. Thus Aristotle can be
seen as both an empiricist and a rationalist.
3 In this picture, all meaningful statements can be verified or falsified either by experimentation or by analysis
of the meaning of the words. Because this rules out most of metaphysics, as was the intention, logical positivism
was unpopular in some circles.
Section 1.2. The Foundations of Artificial Intelligence 7
The final element in the philosophical picture of the mind is the connection between
knowledge and action. This question is vital to AI because intelligence requires action as well
as reasoning. Moreover, only by understanding how actions are justified can we understand
how to build an agent whose actions are justifiable (or rational). Aristotle argued (in De Motu
Animalium) that actions are justified by a logical connection between goals and knowledge of
the action’s outcome (the last part of this extract also appears on the front cover of this book,
in the original Greek):
But how does it happen that thinking is sometimes accompanied by action and sometimes
not, sometimes by motion, and sometimes not? It looks as if almost the same thing
happens as in the case of reasoning and making inferences about unchanging objects. But
in that case the end is a speculative proposition ... whereas here the conclusion which
results from the two premises is an action. ... I need covering; a cloak is a covering. I
need a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And
the conclusion, the “I have to make a cloak,” is an action.
In the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,
suggesting an algorithm:
We deliberate not about ends, but about means. For a doctor does not deliberate whether
he shall heal, nor an orator whether he shall persuade, ... They assume the end and
consider how and by what means it is attained, and if it seems easily and best produced
thereby; while if it is achieved by one means only they consider how it will be achieved
by this and by what means this will be achieved, till they come to the first cause, ... and
what is last in the order of analysis seems to be first in the order of becoming. And if we
come on an impossibility, we give up the search, e.g., if we need money and this cannot
be got; but if a thing appears possible we try to do it.
Aristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS
program. We would now call it a regression planning system (see Chapter 10).
Goal-based analysis is useful, but does not say what to do when several actions will
achieve the goal or when no action will achieve it completely. Antoine Arnauld (1612–1694)
correctly described a quantitative formula for deciding what action to take in cases like this
(see Chapter 16). John Stuart Mill’s (1806–1873) book Utilitarianism (Mill, 1863) promoted
the idea of rational decision criteria in all spheres of human activity. The more formal theory
of decisions is discussed in the following section.
• What are the formal rules to draw valid conclusions?
• What can be computed?
• How do we reason with uncertain information?
Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science
required a level of mathematical formalization in three fundamental areas: logic, computation, and probability.
The idea of formal logic can be traced back to the philosophers of ancient Greece, but
its mathematical development really began with the work of George Boole (1815–1864), who
8 Chapter 1. Introduction
worked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob
Frege (1848–1925) extended Boole’s logic to include objects and relations, creating the firstorder logic that is used today.4 Alfred Tarski (1902–1983) introduced a theory of reference
that shows how to relate the objects in a logic to objects in the real world.
The next step was to determine the limits of what could be done with logic and comALGORITHM putation. The first nontrivial algorithm is thought to be Euclid’s algorithm for computing
greatest common divisors. The word algorithm (and the idea of studying them) comes from
al-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced
Arabic numerals and algebra to Europe. Boole and others discussed algorithms for logical
deduction, and, by the late 19th century, efforts were under way to formalize general mathematical reasoning as logical deduction. In 1930, Kurt G¨odel (1906–1978) showed that there
exists an effective procedure to prove any true statement in the first-order logic of Frege and
Russell, but that first-order logic could not capture the principle of mathematical induction
needed to characterize the natural numbers. In 1931, G¨odel showed that limits on deduction do exist. His incompleteness theorem showed that in any formal theory as strong as INCOMPLETENESS
THEOREM
Peano arithmetic (the elementary theory of natural numbers), there are true statements that
are undecidable in the sense that they have no proof within the theory.
This fundamental result can also be interpreted as showing that some functions on the
integers cannot be represented by an algorithm—that is, they cannot be computed. This
motivated Alan Turing (1912–1954) to try to characterize exactly which functions are comCOMPUTABLE putable—capable of being computed. This notion is actually slightly problematic because
the notion of a computation or effective procedure really cannot be given a formal definition.
However, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) is
capable of computing any computable function, is generally accepted as providing a sufficient
definition. Turing also showed that there were some functions that no Turing machine can
compute. For example, no machine can tell in general whether a given program will return
an answer on a given input or run forever.
Although decidability and computability are important to an understanding of computaTRACTABILITY tion, the notion of tractability has had an even greater impact. Roughly speaking, a problem
is called intractable if the time required to solve instances of the problem grows exponentially
with the size of the instances. The distinction between polynomial and exponential growth
in complexity was first emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965). It is
important because exponential growth means that even moderately large instances cannot be
solved in any reasonable time. Therefore, one should strive to divide the overall problem of
generating intelligent behavior into tractable subproblems rather than intractable ones.
NP-COMPLETENESS How can one recognize an intractable problem? The theory of NP-completeness, pioneered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp
showed the existence of large classes of canonical combinatorial search and reasoning problems that are NP-complete. Any problem class to which the class of NP-complete problems
can be reduced is likely to be intractable. (Although it has not been proved that NP-complete
4 Frege’s proposed notation for first-order logic—an arcane combination of textual and geometric features—
never became popular.
Section 1.2. The Foundations of Artificial Intelligence 9
problems are necessarily intractable, most theoreticians believe it.) These results contrast
with the optimism with which the popular press greeted the first computers—“Electronic
Super-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,
careful use of resources will characterize intelligent systems. Put crudely, the world is an
extremely large problem instance! Work in AI has helped explain why some instances of
NP-complete problems are hard, yet others are easy (Cheeseman et al., 1991).
Besides logic and computation, the third great contribution of mathematics to AI is the
PROBABILITY theory of probability. The Italian Gerolamo Cardano (1501–1576) first framed the idea of
probability, describing it in terms of the possible outcomes of gambling events. In 1654,
Blaise Pascal (1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to predict the future of an unfinished gambling game and assign average payoffs to the gamblers.
Probability quickly became an invaluable part of all the quantitative sciences, helping to deal
with uncertain measurements and incomplete theories. James Bernoulli (1654–1705), Pierre
Laplace (1749–1827), and others advanced the theory and introduced new statistical methods. Thomas Bayes (1702–1761), who appears on the front cover of this book, proposed
a rule for updating probabilities in the light of new evidence. Bayes’ rule underlies most
modern approaches to uncertain reasoning in AI systems.
• How should we make decisions so as to maximize payoff?
• How should we do this when others may not go along?
• How should we do this when the payoff may be far in the future?
The science of economics got its start in 1776, when Scottish philosopher Adam Smith
(1723–1790) published An Inquiry into the Nature and Causes of the Wealth of Nations.
While the ancient Greeks and others had made contributions to economic thought, Smith was
the first to treat it as a science, using the idea that economies can be thought of as consisting of individual agents maximizing their own economic well-being. Most people think of
economics as being about money, but economists will say that they are really studying how
people make choices that lead to preferred outcomes. When McDonald’s offers a hamburger
for a dollar, they are asserting that they would prefer the dollar and hoping that customers will
UTILITY prefer the hamburger. The mathematical treatment of “preferred outcomes” or utility was
first formalized by L´eon Walras (pronounced “Valrasse”) (1834-1910) and was improved by
Frank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their book
The Theory of Games and Economic Behavior (1944).
DECISION THEORY Decision theory, which combines probability theory with utility theory, provides a formal and complete framework for decisions (economic or otherwise) made under uncertainty—
that is, in cases where probabilistic descriptions appropriately capture the decision maker’s
environment. This is suitable for “large” economies where each agent need pay no attention
to the actions of other agents as individuals. For “small” economies, the situation is much
more like a game: the actions of one player can significantly affect the utility of another
(either positively or negatively). Von Neumann and Morgenstern’s development of game
GAME THEORY theory (see also Luce and Raiffa, 1957) included the surprising result that, for some games,
10 Chapter 1. Introduction
a rational agent should adopt policies that are (or least appear to be) randomized. Unlike decision theory, game theory does not offer an unambiguous prescription for selecting actions.
For the most part, economists did not address the third question listed above, namely,
how to make rational decisions when payoffs from actions are not immediate but instead result from several actions taken in sequence. This topic was pursued in the field of operations
research, which emerged in World War II from efforts in Britain to optimize radar installa- OPERATIONS
RESEARCH
tions, and later found civilian applications in complex management decisions. The work of
Richard Bellman (1957) formalized a class of sequential decision problems called Markov
decision processes, which we study in Chapters 17 and 21.
Work in economics and operations research has contributed much to our notion of rational agents, yet for many years AI research developed along entirely separate paths. One
reason was the apparent complexity of making rational decisions. The pioneering AI researcher Herbert Simon (1916–2001) won the Nobel Prize in economics in 1978 for his early
SATISFICING work showing that models based on satisficing—making decisions that are “good enough,”
rather than laboriously calculating an optimal decision—gave a better description of actual
human behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in
decision-theoretic techniques for agent systems (Wellman, 1995).
• How do brains process information?
NEUROSCIENCE Neuroscience is the study of the nervous system, particularly the brain. Although the exact
way in which the brain enables thought is one of the great mysteries of science, the fact that it
does enable thought has been appreciated for thousands of years because of the evidence that
strong blows to the head can lead to mental incapacitation. It has also long been known that
human brains are somehow different; in about 335 B.C. Aristotle wrote, “Of all the animals,
man has the largest brain in proportion to his size.”5 Still, it was not until the middle of the
18th century that the brain was widely recognized as the seat of consciousness. Before then,
candidate locations included the heart and the spleen.
Paul Broca’s (1824–1880) study of aphasia (speech deficit) in brain-damaged patients
in 1861 demonstrated the existence of localized areas of the brain responsible for specific
cognitive functions. In particular, he showed that speech production was localized to the
portion of the left hemisphere now called Broca’s area.6 By that time, it was known that
NEURON the brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo Golgi
(1843–1926) developed a staining technique allowing the observation of individual neurons
in the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852–
1934) in his pioneering studies of the brain’s neuronal structures.7 Nicolas Rashevsky (1936,
1938) was the first to apply mathematical models to the study of the nervous sytem.
• How do humans and animals think and act?
The origins of scientific psychology are usually traced to the work of the German physicist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).
Helmholtz applied the scientific method to the study of human vision, and his Handbook
of Physiological Optics is even now described as “the single most important treatise on the
physics and physiology of human vision” (Nalwa, 1993, p.15). In 1879, Wundt opened the
first laboratory of experimental psychology, at the University of Leipzig. Wundt insisted
on carefully controlled experiments in which his workers would perform a perceptual or associative task while introspecting on their thought processes. The careful controls went a
long way toward making psychology a science, but the subjective nature of the data made
it unlikely that an experimenter would ever disconfirm his or her own theories. Biologists
studying animal behavior, on the other hand, lacked introspective data and developed an objective methodology, as described by H. S. Jennings (1906) in his influential work Behavior of
BEHAVIORISM the Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led
by John Watson (1878–1958), rejected any theory involving mental processes on the grounds
Section 1.2. The Foundations of Artificial Intelligence 13
that introspection could not provide reliable evidence. Behaviorists insisted on studying only
objective measures of the percepts (or stimulus) given to an animal and its resulting actions
(or response). Behaviorism discovered a lot about rats and pigeons but had less success at
understanding humans.
Cognitive psychology, which views the brain as an information-processing device, COGNITIVE
PSYCHOLOGY
can be traced back at least to the works of William James (1842–1910). Helmholtz also
insisted that perception involved a form of unconscious logical inference. The cognitive
viewpoint was largely eclipsed by behaviorism in the United States, but at Cambridge’s Applied Psychology Unit, directed by Frederic Bartlett (1886–1969), cognitive modeling was
able to flourish. The Nature of Explanation, by Bartlett’s student and successor Kenneth
Craik (1943), forcefully reestablished the legitimacy of such “mental” terms as beliefs and
goals, arguing that they are just as scientific as, say, using pressure and temperature to talk
about gases, despite their being made of molecules that have neither. Craik specified the
three key steps of a knowledge-based agent: (1) the stimulus must be translated into an internal representation, (2) the representation is manipulated by cognitive processes to derive new
internal representations, and (3) these are in turn retranslated back into action. He clearly
explained why this was a good design for an agent:
If the organism carries a “small-scale model” of external reality and of its own possible
actions within its head, it is able to try out various alternatives, conclude which is the best
of them, react to future situations before they arise, utilize the knowledge of past events
in dealing with the present and future, and in every way to react in a much fuller, safer,
and more competent manner to the emergencies which face it. (Craik, 1943)
After Craik’s death in a bicycle accident in 1945, his work was continued by Donald Broadbent, whose book Perception and Communication (1958) was one of the first works to model
psychological phenomena as information processing. Meanwhile, in the United States, the
development of computer modeling led to the creation of the field of cognitive science. The
field can be said to have started at a workshop in September 1956 at MIT. (We shall see that
this is just two months after the conference at which AI itself was “born.”) At the workshop,
George Miller presented The Magic Number Seven, Noam Chomsky presented Three Models
of Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.
These three influential papers showed how computer models could be used to address the
psychology of memory, language, and logical thinking, respectively. It is now a common
(although far from universal) view among psychologists that “a cognitive theory should be
like a computer program” (Anderson, 1980); that is, it should describe a detailed informationprocessing mechanism whereby some cognitive function might be implemented.
• How can we build an efficient computer?
For artificial intelligence to succeed, we need two things: intelligence and an artifact. The
computer has been the artifact of choice. The modern digital electronic computer was invented independently and almost simultaneously by scientists in three countries embattled in
14 Chapter 1. Introduction
World War II. The first operational computer was the electromechanical Heath Robinson,8
built in 1940 by Alan Turing’s team for a single purpose: deciphering German messages. In
1943, the same group developed the Colossus, a powerful general-purpose machine based
on vacuum tubes.9 The first operational programmable computer was the Z-3, the invention of Konrad Zuse in Germany in 1941. Zuse also invented floating-point numbers and the
first high-level programming language, Plankalk¨ul. The first electronic computer, the ABC,
was assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942
at Iowa State University. Atanasoff’s research received little support or recognition; it was
the ENIAC, developed as part of a secret military project at the University of Pennsylvania
by a team including John Mauchly and John Eckert, that proved to be the most influential
forerunner of modern computers.
Since that time, each generation of computer hardware has brought an increase in speed
and capacity and a decrease in price. Performance doubled every 18 months or so until around
2005, when power dissipation problems led manufacturers to start multiplying the number of
CPU cores rather than the clock speed. Current expectations are that future increases in power
will come from massive parallelism—a curious convergence with the properties of the brain.
Of course, there were calculating devices before the electronic computer. The earliest
automated machines, dating from the 17th century, were discussed on page 6. The first programmable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752–1834),
that used punched cards to store instructions for the pattern to be woven. In the mid-19th
century, Charles Babbage (1792–1871) designed two machines, neither of which he completed. The Difference Engine was intended to compute mathematical tables for engineering
and scientific projects. It was finally built and shown to work in 1991 at the Science Museum
in London (Swade, 2000). Babbage’s Analytical Engine was far more ambitious: it included
addressable memory, stored programs, and conditional jumps and was the first artifact capable of universal computation. Babbage’s colleague Ada Lovelace, daughter of the poet Lord
Byron, was perhaps the world’s first programmer. (The programming language Ada is named
after her.) She wrote programs for the unfinished Analytical Engine and even speculated that
the machine could play chess or compose music.
AI also owes a debt to the software side of computer science, which has supplied the
operating systems, programming languages, and tools needed to write modern programs (and
papers about them). But this is one area where the debt has been repaid: work in AI has pioneered many ideas that have made their way back to mainstream computer science, including
time sharing, interactive interpreters, personal computers with windows and mice, rapid development environments, the linked list data type, automatic storage management, and key
concepts of symbolic, functional, declarative, and object-oriented programming.
8 Heath Robinson was a cartoonist famous for his depictions of whimsical and absurdly complicated contraptions for everyday tasks such as buttering toast.
9 In the postwar period, Turing wanted to use these computers for AI research—for example, one of the first
chess programs (Turing et al., 1953). His efforts were blocked by the British government.
Section 1.2. The Foundations of Artificial Intelligence 15
• How can artifacts operate under their own control?
Ktesibios of Alexandria (c. 250 B.C.) built the first self-controlling machine: a water clock
with a regulator that maintained a constant flow rate. This invention changed the definition
of what an artifact could do. Previously, only living things could modify their behavior in
response to changes in the environment. Other examples of self-regulating feedback control
systems include the steam engine governor, created by James Watt (1736–1819), and the
thermostat, invented by Cornelis Drebbel (1572–1633), who also invented the submarine.
We call ourselves Homo sapiens—man the wise—because our intelligence is so important
to us. For thousands of years, we have tried to understand how we think; that is, how a mere
handful of matter can perceive, understand, predict, and manipulate a world far larger and
more complicated than itself. The field of artificial intelligence, or AI, goes further still: it
attempts not just to understand but also to build intelligent entities.
AI is one of the newest fields in science and engineering. Work started in earnest soon
after World War II, and the name itself was coined in 1956. Along with molecular biology,
AI is regularly cited as the “field I would most like to be in” by scientists in other disciplines.
A student in physics might reasonably feel that all the good ideas have already been taken by
Galileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings for several
full-time Einsteins and Edisons.AI currently encompasses a huge variety of subfields, ranging from the general (learning
and perception) to the specific, such as playing chess, proving mathematical theorems, writing
poetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any
intellectual task; it is truly a universal field.  We have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see eight definitions of AI, laid out along two dimensions. The definitions on top are concerned
with thought processes and reasoning, whereas the ones on the bottom address behavior.The
definitions on the left measure success in terms of fidelity to human performance, whereas
the ones on the right measure against an ideal performance measure, called rationality. A
system is rational if it does the “right thing,” given what it knows.  We have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see eight definitions of AI, laid out along two dimensions. The definitions on top are concerned
with thought processes and reasoning, whereas the ones on the bottom address behavior. The
definitions on the left measure success in terms of fidelity to human performance, whereas
the ones on the right measure against an ideal performance measure, called rationality. A
system is rational if it does the “right thing,” given what it knows.
Thinking Humanly
“The exciting new effort to make comput
ers think ...machines with minds, inthe
full and literal sense.” (Haugeland, 1985)
“[The automation of] activities that we
associate with human thinking, activities
such as decision-making, problem solving, learning ...” (Bellman, 1978)
The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory
operational definition of intelligence. A computer passes the test if a human interrogator, after
posing some written questions, cannot tell whether the written responses come from a person
or from acomputer. Chapter 26 discusses the details of the test and whether a computer would
really be intelligent if it passed. For now, we note that programming a computer to pass a
rigorously applied test provides plenty to work on. The computer would need to possess the
following capabilities:
• natural language processing to enable it to communicate successfully in English;
• knowledge representation to store what it knows or hears;
• automated reasoning to use the stored information to answer questions and to draw
new conclusions;
• machinelearning to adapt to new circumstances and to detect and extrapolate patterns.
Turing’s test deliberately avoided direct physical interaction between the interrogator and the
 computer, because physical simulation of a person is unnecessary for intelligence. However,
 the so-called total Turing Test includes a video signal so that the interrogator can test the
 subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical
 objects “through the hatch.” To pass the total Turing Test, the computer will need
 • computer vision to perceive objects, and
 • robotics to manipulate objects and move about.
 These six disciplines compose most of AI, and Turing deserves credit for designing a test
 that remains relevant 60 years later. Yet AI researchers have devoted little effort to passing
 the Turing Test, believing that it is more important to study the underlying principles of in
telligence than to duplicate an exemplar. The quest for “artificial flight” succeeded when the
 Wright brothers and others stopped imitating birds and started using wind tunnels and learn
ing about aerodynamics. Aeronautical engineering texts do not define the goal of their field
 as making “machines that fly so exactly like pigeons that they can fool even other pigeons.”
Thinking humanly: The cognitive modeling approach
 If we are going to say that a given program thinks like a human, we must have some way of
 determining how humans think. We need to get inside the actual workings of human minds.
 There are three ways to do this: through introspection—trying to catch our own thoughts as
 they go by; through psychological experiments—observing a person in action; and through
 brain imaging—observing the brain in action. Once we have a sufficiently precise theory of
 the mind, it becomes possible to express the theory as a computer program. If the program’s
 input–output behavior matches corresponding human behavior, that is evidence that some of
 the program’s mechanisms could also be operating in humans. For example, Allen Newell
 and Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon,
 1961), were not content merely to have their program solve problems correctly. They were
 more concerned with comparing the trace of its reasoning steps to traces of human subjects
 solving the same problems. The interdisciplinary field of cognitive science brings together
 computer models from AI and experimental techniques from psychology to construct precise
 and testable theories of the human mind.
 Cognitive science is a fascinating field in itself, worthy of several textbooks and at least
 one encyclopedia (Wilson and Keil, 1999). We will occasionally comment on similarities or
 differences between AI techniques and human cognition. Real cognitive science, however, is
 necessarily based on experimental investigation of actual humans or animals. We will leave
 that for other books, as we assume the reader has only a computer for experimentation.
 In the early days of AI there was often confusion between the approaches: an author
 would argue that an algorithm performs well on a task and that it is therefore a good model
 of human performance, or vice versa. Modern authors separate the two kinds of claims;
 this distinction has allowed both AI and cognitive science to develop more rapidly. The two
 fields continue to fertilize each other, most notably in computer vision, which incorporates
 neurophysiological evidence into computational models.
Thinking rationally: The "laws of thought" approach
The Greek philosopher Aristotle was one of the first to attempt to codify “right thinking,” that
 is, irrefutable reasoning processes. His syllogisms provided patterns for argument structures
 that always yielded correct conclusions when given correct premises—for example, “Socrates
 is a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were
 supposed to govern the operation of the mind; their study initiated the field called logic.
 Logicians in the 19th century developed a precise notation for statements about all kinds
 of objects in the world and the relations among them. (Contrast this with ordinary arithmetic
 notation, which provides only for statements about numbers.) By 1965, programs existed
 that could, in principle, solve any solvable problem described in logical notation. (Although
 if no solution exists, the program might loop forever.) The so-called logicist tradition within
 artificial intelligence hopes to build on such programs to create intelligent systems.
 There are two main obstacles to this approach. First, it is not easy to take informal
 knowledge and state it in the formal terms required by logical notation, particularly when
 the knowledge is less than 100% certain. Second, there is a big difference between solving
 a problem “in principle” and solving it in practice. Even problems with just a few hundred
 facts can exhaust the computational resources of any computer unless it has some guidance
 as to which reasoning steps to try first. Although both of these obstacles apply to any attempt
 to build computational reasoning systems, they appeared first in the logicist tradition.
Acting rationally: The rational agent approach
 An agent is just something that acts (agent comes from the Latin agere, to do). Of course,
 all computer programs do something, but computer agents are expected to do more: operate
 autonomously, perceive their environment, persist over a prolonged time period, adapt to
 change, and create and pursue goals. A rational agent is one that acts so as to achieve the
 best outcome or, when there is uncertainty, the best expected outcome.
 In the “laws of thought” approach to AI, the emphasis was on correct inferences. Mak
ing correct inferences is sometimes part of being a rational agent, because one way to act
 rationally is to reason logically to the conclusion that a given action will achieve one’s goals
 and then to act on that conclusion. On the other hand, correct inference is not all of ration
ality; in some situations, there is no provably correct thing to do, but something must still be
 done. There are also ways of acting rationally that cannot be said to involve inference. For
 example, recoiling from a hot stove is a reflex action that is usually more successful than a
 slower action taken after careful deliberation.
 All the skills needed for the Turing Test also allow an agent to act rationally. Knowledge
 representation and reasoning enable agents to reach good decisions. We need to be able to
 generate comprehensible sentences in natural language to get by in a complex society. We
 need learning not only for erudition, but also because it improves our ability to generate
 effective behavior.
 The rational-agent approach has two advantages over the other approaches. First, it
 is more general than the “laws of thought” approach because correct inference is just one
 of several possible mechanisms for achieving rationality. Second, it is more amenable to scientific development than are approaches based on human behavior or human thought. The
 standard of rationality is mathematically well defined and completely general, and can be
 “unpacked” to generate agent designs that provably achieve it. Human behavior, on the other
 hand, is well adapted for one specific environment and is defined by, well, the sum total
 of all the things that humans do. This book therefore concentrates on general principles
 of rational agents and on components for constructing them. We will see that despite the
 apparent simplicity with which the problem can be stated, an enormous variety of issues
 come up when we try to solve it. Chapter 2 outlines some of these issues in more detail.
 One important point to keep in mind: We will see before too long that achieving perfect
 rationality—always doing the right thing—is not feasible in complicated environments. The
 computational demands are just too high. For most of the book, however, we will adopt the
 working hypothesis that perfect rationality is a good starting point for analysis. It simplifies
 the problem and provides the appropriate setting for most of the foundational material in
 the field. Chapters 5 and 17 deal explicitly with the issue of limited rationality—acting
 appropriately when there is not enough time to do all the computations one might like.
Philosophy
 • Canformal rules be used to draw valid conclusions?
 • Howdoes the mind arise from a physical brain?
 • Where does knowledge come from?
 • Howdoes knowledge lead to action?
 Aristotle (384–322 B.C.), whose bust appears on the front cover of this book, was the first
 to formulate a precise set of laws governing the rational part of the mind. He developed an
 informal system of syllogisms for proper reasoning, which in principle allowed one to gener
ate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had
 the idea that useful reasoning could actually be carried out by a mechanical artifact. Thomas
 Hobbes (1588–1679) proposed that reasoning was like numerical computation, that “we add
 and subtract in our silent thoughts.” The automation of computation itself was already well
 under way. Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a me
chanical calculator; recent reconstructions have shown the design to be functional. The first
 known calculating machine was constructed around 1623 by the German scientist Wilhelm
 Schickard (1592–1635), although the Pascaline, built in 1642 by Blaise Pascal (1623–1662),  is more famous. Pascal wrote that “the arithmetical machine produces effects which appear
 nearer to thought than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)
 built a mechanical device intended to carry out operations on concepts rather than numbers,
 but its scope was rather limited. Leibniz did surpass Pascal by building a calculator that
 could add, subtract, multiply, and take roots, whereas the Pascaline could only add and sub
tract. Some speculated that machines might not just do calculations but actually be able to
 think and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea
 of an “artificial animal,” arguing “For what is the heart but a spring; and the nerves, but so
 many strings; and the joints, but so many wheels.”  It’s one thing to say that the mind operates, at least in part, according to logical rules, and
 to build physical systems that emulate some of those rules; it’s another to say that the mind
 itself is such a physical system. Ren´e Descartes (1596–1650) gave the first clear discussion
 of the distinction between mind and matter and of the problems that arise. One problem with
 a purely physical conception of the mind is that it seems to leave little room for free will:
 if the mind is governed entirely by physical laws, then it has no more free will than a rock
 “deciding” to fall toward the center of the earth. Descartes was a strong advocate of the power
 of reasoning in understanding the world, a philosophy now called rationalism, and one that
 counts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.
 He held that there is a part of the human mind (or soul or spirit) that is outside of nature,
 exempt from physical laws. Animals, on the other hand, did not possess this dual quality;
 they could be treated as machines. An alternative to dualism is materialism, which holds
 that the brain’s operation according to the laws of physics constitutes the mind. Free will is
 simply the way that the perception of available choices appears to the choosing entity.
 Given a physical mind that manipulates knowledge, the next problem is to establish
 the source of knowledge. The empiricism movement, starting with Francis Bacon’s (1561
1626) Novum Organum,2 is characterized by a dictum of John Locke (1632–1704): “Nothing
 is in the understanding, which was not first in the senses.” David Hume’s (1711–1776) A
 Treatise of Human Nature (Hume, 1739) proposed what is now known as the principle of
 induction: that general rules are acquired by exposure to repeated associations between their
 elements. Building on the work of Ludwig Wittgenstein (1889–1951) and Bertrand Russell
 (1872–1970), the famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed the
 doctrine of logical positivism. This doctrine holds that all knowledge can be characterized by
 logical theories connected, ultimately, to observation sentences that correspond to sensory
 inputs; thus logical positivism combines rationalism and empiricism.3 The confirmation the
ory of Carnap and Carl Hempel (1905–1997) attempted to analyze the acquisition of knowl
edge from experience. Carnap’s book The Logical Structure of the World (1928) defined an
 explicit computational procedure for extracting knowledge from elementary experiences. It
 was probably the first theory of mind as a computational process. The final element in the philosophical picture of the mind is the connection between
 knowledge and action. This question is vital to AI because intelligence requires action as well
 as reasoning. Moreover, only by understanding how actions are justified can we understand
 how to build an agent whose actions are justifiable (or rational). Aristotle argued (in De Motu
 Animalium) that actions are justified by a logical connection between goals and knowledge of
 the action’s outcome (the last part of this extract also appears on the front cover of this book,
 in the original Greek):
 But howdoesit happenthat thinking is sometimes accompaniedby action and sometimes
 not, sometimes by motion, and sometimes not? It looks as if almost the same thing
 happensas in the case of reasoning and making inferences about unchangingobjects. But
 in that case the end is a speculative proposition ...whereas here the conclusion which
 results from the two premises is an action. ...I need covering; a cloak is a covering. I
 need a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And
 the conclusion, the “I have to make a cloak,” is an action.
 In the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,
 suggesting an algorithm:
 We deliberate not about ends, but about means. For a doctor does not deliberate whether
 he shall heal, nor an orator whether he shall persuade, ... They assume the end and
 consider how and by what means it is attained, and if it seems easily and best produced
 thereby; while if it is achieved by one means only they consider how it will be achieved
 by this and by what means this will be achieved, till they come to the first cause, ...and
 what is last in the order of analysis seems to be first in the order of becoming. And if we
 come on an impossibility, we give up the search, e.g., if we need money and this cannot
 be got; but if a thing appears possible we try to do it.
 Aristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS
 program. We would now call it a regression planning system (see Chapter 10).
 Goal-based analysis is useful, but does not say what to do when several actions will
 achieve the goal or when no action will achieve it completely. Antoine Arnauld (1612–1694)
 correctly described a quantitative formula for deciding what action to take in cases like this
 (see Chapter 16). John Stuart Mill’s (1806–1873) book Utilitarianism (Mill, 1863) promoted
 the idea of rational decision criteria in all spheres of human activity. The more formal theory
 of decisions is discussed in the following section. What are the formal rules to draw valid conclusions?
 • What can be computed?
 • Howdowereason with uncertain information?
 Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science
 required a level of mathematical formalization in three fundamental areas: logic, computation, and probability.
 The idea of formal logic can be traced back to the philosophers of ancient Greece, but
 its mathematical development really began with the work of George Boole (1815–1864), whoworked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob
 Frege (1848–1925) extended Boole’s logic to include objects and relations, creating the first
order logic that is used today.4 Alfred Tarski (1902–1983) introduced a theory of reference
 that shows how to relate the objects in a logic to objects in the real world. The next step was to determine the limits of what could be done with logic and com
putation. The first nontrivial algorithm is thought to be Euclid’s algorithm for computing
 greatest common divisors. The word algorithm (and the idea of studying them) comes from
 al-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced
 Arabic numerals and algebra to Europe. Boole and others discussed algorithms for logical
 deduction, and, by the late 19th century, efforts were under way to formalize general mathe
matical reasoning as logical deduction. In 1930, Kurt G¨ odel (1906–1978) showed that there
 exists an effective procedure to prove any true statement in the first-order logic of Frege and
 Russell, but that first-order logic could not capture the principle of mathematical induction
 needed to characterize the natural numbers. In 1931, G¨ odel showed that limits on deduc
tion do exist. His incompleteness theorem showed that in any formal theory as strong as
 Peano arithmetic (the elementary theory of natural numbers), there are true statements that
 are undecidable in the sense that they have no proof within the theory.
 This fundamental result can also be interpreted as showing that some functions on the
 integers cannot be represented by an algorithm—that is, they cannot be computed. This
 motivated Alan Turing (1912–1954) to try to characterize exactly which functions are com
putable—capable of being computed. This notion is actually slightly problematic because
 the notion of a computation or effective procedure really cannot be given a formal definition.
 However, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) is
 capable of computing any computable function, is generally accepted as providing a sufficient
 definition. Turing also showed that there were some functions that no Turing machine can
 compute. For example, no machine can tell in general whether a given program will return
 an answer on a given input or run forever.
 Although decidability and computability are important to an understanding of computa
tion, the notion of tractability has had an even greater impact. Roughly speaking, a problem
 is called intractable if the time required to solve instances of the problem grows exponentially
 with the size of the instances. The distinction between polynomial and exponential growth
 in complexity was first emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965). It is
 important because exponential growth means that even moderately large instances cannot be
 solved in any reasonable time. Therefore, one should strive to divide the overall problem of
 generating intelligent behavior into tractable subproblems rather than intractable ones.
 How can one recognize an intractable problem? The theory of NP-completeness,pio
neered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp
 showed the existence of large classes of canonical combinatorial search and reasoning prob
lems that are NP-complete. Any problem class to which the class of NP-complete problems
 can be reduced is likely to be intractable. (Although it has not been proved that NP-complete  problems are necessarily intractable, most theoreticians believe it.) These results contrast
 with the optimism with which the popular press greeted the first computers—“Electronic
 Super-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,
 careful use of resources will characterize intelligent systems. Put crudely, the world is an
 extremely large problem instance! Work in AI has helped explain why some instances of
 NP-complete problems are hard, yet others are easy (Cheeseman et al., 1991). Besides logic and computation, the third great contribution of mathematics to AI is the
 theory of probability. The Italian Gerolamo Cardano (1501–1576) first framed the idea of
 probability, describing it in terms of the possible outcomes of gambling events. In 1654,
 Blaise Pascal (1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to pre
dict the future of an unfinished gambling game and assign average payoffs to the gamblers.
 Probability quickly became an invaluable part of all the quantitative sciences, helping to deal
 with uncertain measurements and incomplete theories. James Bernoulli (1654–1705), Pierre
 Laplace (1749–1827), and others advanced the theory and introduced new statistical meth
ods. Thomas Bayes (1702–1761), who appears on the front cover of this book, proposed
 a rule for updating probabilities in the light of new evidence. Bayes’ rule underlies most
 modern approaches to uncertain reasoning in AI systems.
 1.2.3 Economics
 • Howshould we make decisions so as to maximize payoff?
 • Howshould we do this when others may not go along?
 • Howshould we do this when the payoff may be far in the future?
 The science of economics got its start in 1776, when Scottish philosopher Adam Smith
 (1723–1790) published An Inquiry into the Nature and Causes of the Wealth of Nations.
 While the ancient Greeks and others had made contributions to economic thought, Smith was
 the first to treat it as a science, using the idea that economies can be thought of as consist
ing of individual agents maximizing their own economic well-being. Most people think of
 economics as being about money, but economists will say that they are really studying how
 people make choices that lead to preferred outcomes. When McDonald’s offers a hamburger
 for a dollar, they are asserting that they would prefer the dollar and hoping that customers will
 prefer the hamburger. The mathematical treatment of “preferred outcomes” or utility was
 first formalized by L´eon Walras (pronounced “Valrasse”) (1834-1910) and was improved by
 Frank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their book
 The Theory of Games and Economic Behavior (1944).
 Decision theory, which combines probability theory with utility theory, provides a for
malandcomplete framework fordecisions (economic orotherwise) made under uncertainty—
 that is, in cases where probabilistic descriptions appropriately capture the decision maker’s
 environment. This is suitable for “large” economies where each agent need pay no attention
 to the actions of other agents as individuals. For “small” economies, the situation is much
 more like a game: the actions of one player can significantly affect the utility of another
 (either positively or negatively). Von Neumann and Morgenstern’s development of game
 theory (see also Luce and Raiffa, 1957) included the surprising result that, for some games,  a rational agent should adopt policies that are (or least appear to be) randomized. Unlike de
cision theory, game theory does not offer an unambiguous prescription for selecting actions.
 For the most part, economists did not address the third question listed above, namely,
 how to make rational decisions when payoffs from actions are not immediate but instead re
sult from several actions taken in sequence. This topic was pursued in the field of operations
 research, which emerged in World War II from efforts in Britain to optimize radar installa
tions, and later found civilian applications in complex management decisions. The work of
 Richard Bellman (1957) formalized a class of sequential decision problems called Markov
 decision processes, which we study in Chapters 17 and 21.
 Work in economics and operations research has contributed much to our notion of ra
tional agents, yet for many years AI research developed along entirely separate paths. One
 reason was the apparent complexity of making rational decisions. The pioneering AI re
searcher Herbert Simon (1916–2001) won the Nobel Prize in economics in 1978 for his early
 work showing that models based on satisficing—making decisions that are “good enough,”
 rather than laboriously calculating an optimal decision—gave a better description of actual
 human behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in
 decision-theoretic techniques for agent systems (Wellman, 1995).
 1.2.4 Neuroscience
 • How do brains process information?
 Neuroscience is the study of the nervous system, particularly the brain. Although the exact
 way in which the brain enables thought is one of the great mysteries of science, the fact that it
 does enable thought has been appreciated for thousands of years because of the evidence that
 strong blows to the head can lead to mental incapacitation. It has also long been known that
 human brains are somehow different; in about 335 B.C. Aristotle wrote, “Of all the animals,
 man has the largest brain in proportion to his size.”5 Still, it was not until the middle of the
 18th century that the brain was widely recognized as the seat of consciousness. Before then,
 candidate locations included the heart and the spleen.
 Paul Broca’s (1824–1880) study of aphasia (speech deficit) in brain-damaged patients
 in 1861 demonstrated the existence of localized areas of the brain responsible for specific
 cognitive functions. In particular, he showed that speech production was localized to the
 portion of the left hemisphere now called Broca’s area.6 By that time, it was known that
 the brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo Golgi
 (1843–1926) developed a staining technique allowing the observation of individual neurons
 in the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852
1934) in his pioneering studies of the brain’s neuronal structures.7 Nicolas Rashevsky (1936,
 1938) was the first to apply mathematical models to the study of the nervous sytem.
Wenow have some data on the mapping between areas of the brain and the parts of the
 body that they control or from which they receive sensory input. Such mappings are able to
 change radically over the course of a few weeks, and some animals seem to have multiple
 maps. Moreover, we do not fully understand how other areas can take over functions when
 one area is damaged. There is almost no theory on how an individual memory is stored.
 The measurement of intact brain activity began in 1929 with the invention by Hans
 Berger of the electroencephalograph (EEG). The recent development of functional magnetic
 resonance imaging (fMRI) (Ogawa et al., 1990; Cabeza and Nyberg, 2001) is giving neu
roscientists unprecedentedly detailed images of brain activity, enabling measurements that
 correspond in interesting ways to ongoing cognitive processes. These are augmented by
 advances in single-cell recording of neuron activity. Individual neurons can be stimulated
 electrically, chemically, or even optically (Han and Boyden, 2007), allowing neuronal input
output relationships to be mapped. Despite these advances, we are still a long way from
 understanding how cognitive processes actually work.
 The truly amazing conclusion is that a collection of simple cells can lead to thought,
 action, and consciousness or, in the pithy words of John Searle (1992), brains cause minds.
The only real alternative theory is mysticism: that minds operate in some mystical realm that
 is beyond physical science. Brains and digital computers have somewhat different properties. Figure 1.3 shows that
 computers have a cycle time that is a million times faster than a brain. The brain makes up
 for that with far more storage and interconnection than even a high-end personal computer,
 although the largest supercomputers have a capacity that is similar to the brain’s. (It should
 be noted, however, that the brain does not seem to use all of its neurons simultaneously.)
 Futurists make much of these numbers, pointing to an approaching singularity at which
 computers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but the
 raw comparisons are not especially informative. Even with a computer of virtually unlimited
 capacity, we still would not know how to achieve the brain’s level of intelligence.
The first work that is now generally recognized as AI was done by Warren McCulloch and
 Walter Pitts (1943). They drew on three sources: knowledge of the basic physiology and
 function of neurons in the brain; a formal analysis of propositional logic due to Russell and
 Whitehead; and Turing’s theory of computation. They proposed a model of artificial neurons
 in which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring
 in response to stimulation by a sufficient number of neighboring neurons. The state of a
 neuron wasconceived of as “factually equivalent to a proposition which proposed its adequate
 stimulus.” They showed, for example, that any computable function could be computed by
 some network of connected neurons, and that all the logical connectives (and, or, not, etc.)
 could be implemented by simple net structures. McCulloch and Pitts also suggested that
 suitably defined networks could learn. Donald Hebb (1949) demonstrated a simple updating
 rule for modifying the connection strengths between neurons. His rule, now called Hebbian
 learning, remains an influential model to this day.  Two undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the
 first neural network computer in 1950. The SNARC, as it was called, used 3000 vacuum
 tubes and a surplus automatic pilot mechanism from a B-24 bomber to simulate a network of
 40 neurons. Later, at Princeton, Minsky studied universal computation in neural networks.
 His Ph.D. committee was skeptical about whether this kind of work should be considered  mathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.” Minsky
 was later to prove influential theorems showing the limitations of neural network research.
 There were a number of early examples of work that can be characterized as AI, but
 Alan Turing’s vision was perhaps the most influential. He gave lectures on the topic as early
 as 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950
 article “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test,
 machine learning, genetic algorithms, and reinforcement learning. He proposed the Child
 Programme idea, explaining “Instead of trying to produce a programme to simulate the adult
 mind, why not rather try to produce one which simulated the child’s?”
 1.3.2 The birth of artificial intelligence (1956)
 Princeton was home to another influential figure in AI, John McCarthy. After receiving his
 PhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan
ford and then to Dartmouth College, which was to become the official birthplace of the field.
 McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring
 together U.S. researchers interested in automata theory, neural nets, and the study of intel
ligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The
 proposal states:10
 We propose that a 2 month, 10 man study of artificial intelligence be carried
 out during the summer of 1956 at Dartmouth College in Hanover, New Hamp
shire. The study is to proceed on the basis of the conjecture that every aspect of
 learning or any other feature of intelligence can in principle be so precisely de
scribed that a machine can be made to simulate it. An attempt will be made to find
 how to make machines use language, form abstractions and concepts, solve kinds
 of problems now reserved for humans, and improve themselves. We think that a
 significant advance can be made in one or more of these problems if a carefully
 selected group of scientists work on it together for a summer.
 There were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel
 from IBM, and Ray Solomonoff and Oliver Selfridge from MIT.
 Two researchers from Carnegie Tech,11 Allen Newell and Herbert Simon, rather stole
 the show. Although the others had ideas and in some cases programs for particular appli
cations such as checkers, Newell and Simon already had a reasoning program, the Logic
 Theorist (LT), about which Simon claimed, “We have invented a computer program capable
 of thinking non-numerically, and thereby solved the venerable mind–body problem.” Russell was reportedly delighted when Simon
 showed him that the program had come up with a proof for one theorem that was shorter than
 the one in Principia. The editors of the Journal of Symbolic Logic were less impressed; they
 rejected a paper coauthored by Newell, Simon, and Logic Theorist.
 The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce
 all the major figures to each other. For the next 20 years, the field would be dominated by
 these people and their students and colleagues at MIT, CMU, Stanford, and IBM.
 Looking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can
 see why it was necessary for AI to become a separate field. Why couldn’t all the work done
 in AI have taken place under the name of control theory or operations research or decision
 theory, which, after all, have objectives similar to those of AI? Or why isn’t AI a branch
 of mathematics? The first answer is that AI from the start embraced the idea of duplicating
 human faculties such as creativity, self-improvement, and language use. None of the other
 fields were addressing these issues. The second answer is methodology. AI is the only one
 of these fields that is clearly a branch of computer science (although operations research does
 share an emphasis on computer simulations), and AI is the only field to attempt to build
 machines that will function autonomously in complex, changing environments.
 1.3.3 Early enthusiasm, great expectations (1952–1969)
 The early years of AI were full of successes—in a limited way. Given the primitive computers and programming tools of the time and the fact that only a few years earlier computers
 were seen as things that could do arithmetic and no more, it was astonishing whenever a com
puter did anything remotely clever. The intellectual establishment, by and large, preferred to
 believe that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered
 by Turing.) AI researchers naturally responded by demonstrating one X after another. John
 McCarthy referred to this period as the “Look, Ma, no hands!” era.  Newell and Simon’s early success was followed up with the General Problem Solver,
 or GPS. Unlike Logic Theorist, this program was designed from the start to imitate human
 problem-solving protocols. Within the limited class of puzzles it could handle, it turned out
 that the order in which the program considered subgoals and possible actions was similar to
 that in which humans approached the same problems. Thus, GPS was probably the first pro
gram to embody the “thinking humanly” approach. The success of GPS and subsequent pro
grams as models of cognition led Newell and Simon (1976) to formulate the famous physical
 symbol system hypothesis, which states that “a physical symbol system has the necessary and
 sufficient means for general intelligent action.” What they meant is that any system (human
 or machine) exhibiting intelligence must operate by manipulating data structures composed
 of symbols. We will see later that this hypothesis has been challenged from many directions.
 At IBM, Nathaniel Rochester and his colleagues produced some of the first AI pro
grams. Herbert Gelernter (1959) constructed the Geometry Theorem Prover, which was
 able to prove theorems that many students of mathematics would find quite tricky. Starting
 in 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually
 learned to play at a strong amateur level. Along the way, he disproved the idea that comput ers can do only what they are told to: his program quickly learned to play a better game than
 its creator. The program was demonstrated on television in February 1956, creating a strong
 impression. Like Turing, Samuel had trouble finding computer time. Working at night, he
 used machines that were still on the testing floor at IBM’s manufacturing plant. Chapter 5
 covers game playing, and Chapter 21 explains the learning techniques used by Samuel.
 John McCarthy moved from Dartmouth to MIT and there made three crucial contribu
tions in one historic year: 1958. In MIT AILab MemoNo.1, McCarthydefined the high-level
 language Lisp, which was to become the dominant AI programming language for the next 30
 years. With Lisp, McCarthy had the tool he needed, but access to scarce and expensive com
puting resources was also a serious problem. In response, he and others at MIT invented time
 sharing. Also in 1958, McCarthy published a paper entitled Programs with Common Sense,
 in which he described the Advice Taker, a hypothetical program that can be seen as the first
 complete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy’s
 program was designed to use knowledge to search for solutions to problems. But unlike the
 others, it was to embody general knowledge of the world. For example, he showed how
 some simple axioms would enable the program to generate a plan to drive to the airport. The
 program was also designed to accept new axioms in the normal course of operation, thereby
 allowing it to achieve competence in new areas without being reprogrammed. The Advice
 Taker thus embodied the central principles of knowledge representation and reasoning: that
 it is useful to have a formal, explicit representation of the world and its workings and to be
 able to manipulate that representation with deductive processes. It is remarkable how much
 of the 1958 paper remains relevant today.
 1958 also marked the year that Marvin Minsky moved to MIT. His initial collaboration
 with McCarthy did not last, however. McCarthy stressed representation and reasoning in for
mal logic, whereas Minsky was more interested in getting programs to work and eventually
 developed an anti-logic outlook. In 1963, McCarthy started the AI lab at Stanford. His plan
 to use logic to build the ultimate Advice Taker was advanced by J. A. Robinson’s discov
ery in 1965 of the resolution method (a complete theorem-proving algorithm for first-order
 logic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical
 reasoning. Applications of logic included Cordell Green’s question-answering and planning
 systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute
 (SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the
 complete integration of logical reasoning and physical activity.
 Minsky supervised a series of students who chose limited problems that appeared to
 require intelligence to solve. These limited domains became known as microworlds. James
 Slagle’s SAINT program (1963) was able to solve closed-form calculus integration problems
 typical of first-year college courses. Tom Evans’s ANALOGY program (1968) solved geo
metric analogy problems that appear in IQ tests. Daniel Bobrow’s STUDENT program (1967)
 solved algebra story problems, such as the following:
 If the number of customers Tom gets is twice the square of 20 percent of the number
 of advertisements he runs, and the number of advertisements he runs is 45, what is the
 number of customers Tom gets?
The most famous microworld was the blocks world, which consists of a set of solid blocks
 placed on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.
 A typical task in this world is to rearrange the blocks in a certain way, using a robot hand
 that can pick up one block at a time. The blocks world was home to the vision project of
 David Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),
 the learning theory of Patrick Winston (1970), the natural-language-understanding program
 of Terry Winograd (1972), and the planner of Scott Fahlman (1974).
 Early work building on the neural networks of McCulloch and Pitts also flourished.
 The work of Winograd and Cowan (1963) showed how a large number of elements could
 collectively represent an individual concept, with a corresponding increase in robustness and
 parallelism. Hebb’s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,
 1960; Widrow, 1962), who called his networks adalines, and by Frank Rosenblatt (1962)
 with his perceptrons.Theperceptron convergence theorem (Block et al., 1962) says that
 the learning algorithm can adjust the connection strengths of a perceptron to match any input
 data, provided such a match exists.
 From the beginning, AI researchers were not shy about making predictions of their coming
 successes. The following statement by Herbert Simon in 1957 is often quoted:
 It is not my aim to surprise or shock you—butthe simplest way I can summarize is to say
 that there are now in the world machines that think, that learn and that create. Moreover,  their ability to do these things is going to increase rapidly until—in a visible future—the
 range of problems they can handle will be coextensivewith the range to which the human
 mind has been applied. Terms such as “visible future” can be interpreted in various ways, but Simon also made
 more concrete predictions: that within 10 years a computer would be chess champion, and
 a significant mathematical theorem would be proved by machine. These predictions came
 true (or approximately true) within 40 years rather than 10. Simon’s overconfidence was due
 to the promising performance of early AI systems on simple examples. In almost all cases,
 however, these early systems turned out to fail miserably when tried out on wider selections
 of problems and on more difficult problems.
 The first kind of difficulty arose because most early programs knew nothing of their
 subject matter; they succeeded by means of simple syntactic manipulations. A typical story
 occurred in early machine translation efforts, which were generously funded by the U.S. Na
tional Research Council in an attempt to speed up the translation of Russian scientific papers
 in the wake of the Sputnik launch in 1957. It was thought initially that simple syntactic trans
formations based on the grammars of Russian and English, and word replacement from an
 electronic dictionary, would suffice to preserve the exact meanings of sentences. The fact is
 that accurate translation requires background knowledge in order to resolve ambiguity and
 establish the content of the sentence. The famous retranslation of “the spirit is willing but
 the flesh is weak” as “the vodka is good but the meat is rotten” illustrates the difficulties en
countered. In 1966, a report by an advisory committee found that “there has been no machine
 translation of general scientific text, and none is in immediate prospect.” All U.S. government
 funding for academic translation projects was canceled. Today, machine translation is an im
perfect but widely used tool for technical, commercial, government, and Internet documents. The second kind of difficulty was the intractability of many of the problems that AI was
 attempting to solve. Most of the early AI programs solved problems by trying out different
 combinations of steps until the solution was found. This strategy worked initially because
 microworlds contained very few objects and hence very few possible actions and very short
 solution sequences. Before the theory of computational complexity was developed, it was
 widely thought that “scaling up” to larger problems was simply a matter of faster hardware
 and larger memories. The optimism that accompanied the development of resolution theorem
 proving, for example, was soon dampened when researchers failed to prove theorems involv
ing more than a few dozen facts. The fact that a program can find a solution in principle does
 not mean that the program contains any of the mechanisms needed to find it in practice.
 The illusion of unlimited computational power was not confined to problem-solving
 programs. Early experiments in machine evolution (now called genetic algorithms)(Fried
berg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by
 making an appropriate series of small mutations to a machine-code program, one can gen
erate a program with good performance for any particular task. The idea, then, was to try
 random mutations with a selection process to preserve mutations that seemed useful. De
spite thousands of hours of CPU time, almost no progress was demonstrated. Modern genetic
 algorithms use better representations and have shown more success. Failure to come to grips with the “combinatorial explosion” was one of the main criticisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the
 decision by the British government to end support for AI research in all but two universities.
 (Oral tradition paints a somewhat different and more colorful picture, with political ambitions
 and personal animosities whose description is beside the point.)
 Athird difficulty arose because of some fundamental limitations on the basic structures
 being used to generate intelligent behavior. For example, Minsky and Papert’s book Percep
trons (1969) proved that, although perceptrons (a simple form of neural network) could be
 shown to learn anything they were capable of representing, they could represent very little. In
 particular, a two-input perceptron (restricted to be simpler than the form Rosenblatt originally
 studied) could not be trained to recognize when its two inputs were different. Although their
 results did not apply to more complex, multilayer networks, research funding for neural-net
 research soon dwindled to almost nothing. Ironically, the new back-propagation learning al
gorithms for multilayer networks that were to cause an enormous resurgence in neural-net
 research in the late 1980s were actually discovered first in 1969 (Bryson and Ho, 1969).
AI adopts the scientific method (1987–present)
 Recent years have seen a revolution in both the content and the methodology of work in
 artificial intelligence.14 It is now more common to build on existing theories than to propose
 brand-new ones, to base claims on rigorous theorems or hard experimental evidence rather
 than on intuition, and to show relevance to real-world applications rather than toy examples.
 AIwasfounded in part asarebellion against the limitations of existing fields like control
 theory and statistics, but now it is embracing those fields. As David McAllester (1998) put it:
 In the early period of AI it seemed plausible that new forms of symbolic computation,
 e.g., frames and semantic networks, made much of classical theory obsolete. This led to
 a form of isolationism in which AI became largely separated from the rest of computer
 science. This isolationism is currently being abandoned. There is a recognition that
 machinelearningshould not be isolated from informationtheory, that uncertain reasoning
 should not be isolated from stochastic modeling, that search should not be isolated from
 classical optimization and control, and that automated reasoning should not be isolated
 from formal methods and static analysis.
 In terms of methodology, AI has finally come firmly under the scientific method. To be ac
cepted, hypotheses must be subjected to rigorous empirical experiments, and the results must
 be analyzed statistically for their importance (Cohen, 1995). It is now possible to replicate
 experiments by using shared repositories of test data and code.
The field of speech recognition illustrates the pattern. In the 1970s, a wide variety of
 different architectures and approaches were tried. Many of these were rather ad hoc and
 fragile, and were demonstrated on only a few specially selected examples. In recent years,
 approaches based on hiddenMarkov models (HMMs) have come to dominate the area. Two
 aspects of HMMs are relevant. First, they are based on a rigorous mathematical theory. This
 has allowed speech researchers to build on several decades of mathematical results developed
 in other fields. Second, they are generated by a process of training on a large corpus of
 real speech data. This ensures that the performance is robust, and in rigorous blind tests the
 HMMshave been improving their scores steadily. Speech technology and the related field of
 handwritten character recognition are already making the transition to widespread industrial and consumer applications. Note that there is no scientific claim that humans use HMMs to
 recognize speech; rather, HMMs provide a mathematical framework for understanding the
 problem and support the engineering claim that they work well in practice.
 Machine translation follows the same course as speech recognition. In the 1950s there
 was initial enthusiasm for an approach based on sequences of words, with models learned
 according to the principles of information theory. That approach fell out of favor in the
 1960s, but returned in the late 1990s and now dominates the field.
 Neural networks also fit this trend. Much of the work on neural nets in the 1980s was
 done in an attempt to scope out what could be done and to learn how neural nets differ from
 “traditional” techniques. Using improved methodology and theoretical frameworks, the field
 arrived at an understanding in which neural nets can now be compared with corresponding
 techniques from statistics, pattern recognition, and machine learning, and the most promising
 technique can be applied to each application. As a result of these developments, so-called
 data mining technology has spawned a vigorous new industry.
 Judea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new accep
tance of probability and decision theory in AI, following a resurgence of interest epitomized
 by Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network
 formalism was invented to allow efficient representation of, and rigorous reasoning with,
 uncertain knowledge. This approach largely overcomes many problems of the probabilistic
 reasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reason
ing and expert systems. The approach allows for learning from experience, and it combines
 the best of classical AI and neural nets. Work by Judea Pearl (1982a) and by Eric Horvitz and
 David Heckerman (Horvitz and Heckerman, 1986; Horvitz et al., 1986) promoted the idea of
 normative expert systems: ones that act rationally according to the laws of decision theory
 and do not try to imitate the thought steps of human experts. The WindowsTM operating sys
tem includes several normative diagnostic expert systems for correcting problems.
 Similar gentle revolutions have occurred in robotics, computer vision, and knowledge
 representation. A better understanding of the problems and their complexity properties, com
bined with increased mathematical sophistication, has led to workable research agendas and
 robust methods. Although increased formalization and specialization led fields such as vision
 and robotics to become somewhat isolated from “mainstream” AI in the 1990s, this trend has
 reversed in recent years as tools from machine learning in particular have proved effective for
 many problems. The process of reintegration is already yielding significant benefits.
The emergence of intelligent agents (1995–present)
 Perhaps encouraged by the progress in solving the subproblems of AI, researchers have also
 started to look at the “whole agent” problem again. The work of Allen Newell, John Laird,
 and Paul Rosenbloom on SOAR (Newell, 1990; Laird et al., 1987) is the best-known example
 of a complete agent architecture. One of the most important environments for intelligent
 agents is the Internet. AI systems have become so common in Web-based applications that
 the “-bot” suffix has entered everyday language. Moreover, AI technologies underlie many Internet tools, such as search engines, recommender systems, and Web site aggregators.
 Oneconsequence of trying to build complete agents is the realization that the previously
 isolated subfields of AI might need to be reorganized somewhat when their results are to be
 tied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,
 speech recognition, etc.) cannot deliver perfectly reliable information about the environment.
 Hence, reasoning and planning systems must be able to handle uncertainty. A second major
 consequence of the agent perspective is that AI has been drawn into much closer contact
 with other fields, such as control theory and economics, that also deal with agents. Recent
 progress in the control of robotic cars has derived from a mixture of approaches ranging from
 better sensors, control-theoretic integration of sensing, localization and mapping, as well as
 a degree of high-level planning.
Despite these successes, some influential founders of AI, including John McCarthy
 (2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and
 Winston, 2009), have expressed discontent with the progress of AI. They think that AI should
 put less emphasis on creating ever-improved versions of applications that are good at a spe
cific task, such as driving a car, playing chess, or recognizing speech. Instead, they believe
 AI should return to its roots of striving for, in Simon’s words, “machines that think, that learn
 and that create.” They call the effort human-level AI or HLAI; their first symposium was in
 2004 (Minsky et al., 2004). The effort will require very large knowledge bases; Hendler et al.
 (1995) discuss where these knowledge bases might come from.
 Arelated idea is the subfield of Artificial General Intelligence or AGI (Goertzel and
 Pennachin, 2007), which held its first conference and organized the Journal of Artificial Gen
eral Intelligence in 2008. AGI looks for a universal algorithm for learning and acting in
 any environment, and has its roots in the work of Ray Solomonoff (1964), one of the atten
dees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really
 Friendly AI is also a concern (Yudkowsky, 2008; Omohundro, 2008)
 The availability of very large data sets (2001–present)
 Throughout the 60-year history of computer science, the emphasis has been on the algorithm
 as the main subject of study. But some recent work in AI suggests that for many problems, it
 makes more sense to worry about the data and be less picky about what algorithm to apply.
 This is true because of the increasing availability of very large data sources: for example,
 trillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette,
 2006); or billions of base pairs of genomic sequences (Collins et al., 2003).
 One influential paper in this line was Yarowsky’s (1995) work on word-sense disam
biguation: given the use of the word “plant” in a sentence, does that refer to flora or factory?
 Previous approaches to the problem had relied on human-labeled examples combined with
 machine learning algorithms. Yarowsky showed that the task can be done, with accuracy
 above 96%, with no labeled examples at all. Instead, given a very large corpus of unanno
tated text and just the dictionary definitions of the two senses—“works, industrial plant” and
 “flora, plant life”—one can label examples in the corpus, and from there bootstrap to learn
 new patterns that help label new examples. Banko and Brill (2001) show that techniques
 like this perform even better as the amount of available text goes from a million words to a
 billion and that the increase in performance from using more data exceeds any difference in
 algorithm choice; a mediocre algorithm with 100 million words of unlabeled training data
 outperforms the best known algorithm with 1 million words.
 As another example, Hays and Efros (2007) discuss the problem of filling in holes in a
 photograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but
 now you need to fill in the masked area with something that matches the background. Hays
 and Efros defined an algorithm that searches through a collection of photos to find something
 that will match. They found the performance of their algorithm was poor when they used
 a collection of only ten thousand photos, but crossed a threshold into excellent performance
 when they grew the collection to two million photos.
 Work like this suggests that the “knowledge bottleneck” in AI—the problem of how to
 express all the knowledge that a system needs—may be solved in many applications by learn
ing methods rather than hand-coded knowledge engineering, provided the learning algorithms
 have enough data to go on (Halevy et al., 2009). Reporters have noticed the surge of new ap
plications and have written that “AI Winter” may be yielding to a new Spring (Havenstein,
 2005). As Kurzweil (2005) writes, “today, many thousands of AI applications are deeply
 embedded in the infrastructure of every industry.”
 What can AI do today? A concise answer is difficult because there are so many activities in
 so many subfields. Here we sample a few applications; others appear throughout the book.
 Robotic vehicles: A driverless robotic car named STANLEY sped through the rough
 terrain of the Mojave dessert at 22 mph, finishing the 132-mile course first to win the 2005
 DARPAGrand Challenge. STANLEY is a Volkswagen Touareg outfitted with cameras, radar,
 and laser rangefinders to sense the environment and onboard software to command the steer
ing, braking, and acceleration (Thrun, 2006). The following year CMU’s BOSS won the Ur
ban Challenge, safely driving in traffic through the streets of a closed Air Force base, obeying
 traffic rules and avoiding pedestrians and other vehicles.
 Speech recognition: A traveler calling United Airlines to book a flight can have the en
tire conversation guided by an automated speech recognition and dialog management system.
 Autonomous planning and scheduling: A hundred million miles from Earth, NASA’s
 Remote Agent program became the first on-board autonomous planning program to control
 the scheduling of operations for a spacecraft (Jonsson et al., 2000). REMOTE AGENT gen
erated plans from high-level goals specified from the ground and monitored the execution of
 those plans—detecting, diagnosing, and recovering from problems as they occurred. Succes
sor program MAPGEN (Al-Chang et al., 2004) plans the daily operations for NASA’s Mars
 Exploration Rovers, and MEXAR2 (Cesta et al., 2007) did mission planning—both logistics
 and science planning—for the European Space Agency’s Mars Express mission in 2008.
 Game playing: IBM’sDEEP BLUE became the first computer program to defeat the
 world champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in
 an exhibition match (Goodman and Keene, 1997). Kasparov said that he felt a “new kind of
 intelligence” across the board from him. Newsweek magazine described the match as “The
 brain’s last stand.” The value of IBM’s stock increased by $18 billion. Human champions
 studied Kasparov’s loss and were able to draw a few matches in subsequent years, but the
 most recent human-computer matches have been won convincingly by the computer.
 Spamfighting: Each day, learning algorithms classify over a billion messages as spam,
 saving the recipient from having to waste time deleting what, for many users, could comprise
 80% or 90% of all messages, if not classified away by algorithms. Because the spammers are
 continually updating their tactics, it is difficult for a static programmed approach to keep up,
 and learning algorithms work best (Sahami et al., 1998; Goodman and Heckerman, 2004).
 Logistics planning: During the Persian Gulf crisis of 1991, U.S. forces deployed a
 Dynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to do automated
 logistics planning and scheduling for transportation. This involved up to 50,000 vehicles,
 cargo, and people at a time, and had to account for starting points, destinations, routes, and
 conflict resolution among all parameters. The AI planning techniques generated in hours
 a plan that would have taken weeks with older methods. The Defense Advanced Research
 Project Agency (DARPA) stated that this single application more than paid back DARPA’s
 30-year investment in AI.
 Robotics: The iRobot Corporation has sold over two million Roomba robotic vacuum
 cleaners for home use. The company also deploys the more rugged PackBot to Iraq and
 Afghanistan, where it is used to handle hazardous materials, clear explosives, and identify
 the location of snipers.
 Machine Translation: A computer program automatically translates from Arabic to
 English, allowing an English speaker to see the headline “Ardogan Confirms That Turkey
 Would Not Accept Any Pressure, Urging Them to Recognize Cyprus.” The program uses a
 statistical model built from examples of Arabic-to-English translations and from examples of
 English text totaling two trillion words (Brants et al., 2007). None of the computer scientists
 on the team speak Arabic, but they do understand statistics and machine learning algorithms.
 These are just a few examples of artificial intelligence systems that exist today. Not
 magic or science fiction—but rather science, engineering, and mathematics, to which this
 book provides an introduction.
 Anagent is anything that can be viewed as perceiving its environment through sensors and
 acting upon that environment through actuators. This simple idea is illustrated in Figure 2.1.
 Ahumanagent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so
 on for actuators. A robotic agent might have cameras and infrared range finders for sensors
 and various motors for actuators. A software agent receives keystrokes, file contents, and
 network packets as sensory inputs and acts on the environment by displaying on the screen,
 writing files, and sending network packets.
 Weusetheterm percept torefer to the agent’s perceptual inputs at any given instant. An
 agent’s percept sequence is the complete history of everything the agent has ever perceived.
 In general, an agent’s choice of action at any given instant can depend on the entire percept
 sequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s
 choice of action for every possible percept sequence, we have said more or less everything there is to say about the agent. Mathematically speaking, we say that an agent’s behavior is
 described by the agent function that maps any given percept sequence to an action.
 We can imagine tabulating the agent function that describes any given agent; for most
 agents, this would be a very large table—infinite, in fact, unless we place a bound on the
 length of percept sequences we want to consider. Given an agent to experiment with, we can,
 in principle, construct this table by trying out all possible percept sequences and recording
 which actions the agent does in response.1 The table is, of course, an external characterization
 of the agent. Internally, the agent function for an artificial agent will be implemented by an
 agent program. It is important to keep these two ideas distinct. The agent function is an
 abstract mathematical description; the agent program is a concrete implementation, running
 within some physical system.
 To illustrate these ideas, we use a very simple example—the vacuum-cleaner world
 shown in Figure 2.2. This world is so simple that we can describe everything that happens;
 it’s also a made-up world, so wecan invent many variations. This particular world has just two
 locations: squares A and B. The vacuum agent perceives which square it is in and whether
 there is dirt in the square. It can choose to move left, move right, suck up the dirt, or do
 nothing. One very simple agent function is the following: if the current square is dirty, then
 suck; otherwise, move to the other square. A partial tabulation of this agent function is shown
 in Figure 2.3 and an agent program that implements it appears in Figure 2.8 on page 48.
 Looking at Figure 2.3, we see that various vacuum-world agents can be defined simply
 by filling in the right-hand column in various ways. The obvious question, then, is this: What
 is the right way to fill out the table? In other words, what makes an agent good or bad,
 intelligent or stupid? We answer these questions in the next section.
Before closing this section, we should emphasize that the notion of an agent is meant to
 be a tool for analyzing systems, not an absolute characterization that divides the world into
 agents and non-agents. One could view a hand-held calculator as an agent that chooses the
 action of displaying “4” when given the percept sequence “2 + 2 =,” but such an analysis
 would hardly aid our understanding of the calculator. In a sense, all areas of engineering can
 be seen as designing artifacts that interact with the world; AI operates at (what the authors
 consider to be) the most interesting end of the spectrum, where the artifacts have significant
 computational resources and the task environment requires nontrivial decision making.
 We need to be careful to distinguish between rationality and omniscience. An omniscient
 agent knows the actual outcome of its actions and can act accordingly; but omniscience is
 impossible in reality. Consider the following example: I am walking along the Champs
 Elys´ees one day and I see an old friend across the street. There is no traffic nearby and I’m
 not otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000
 feet, a cargo door falls off a passing airliner,2 and before I make it to the other side of the
 street I am flattened. Was I irrational to cross the street? It is unlikely that my obituary would
 read “Idiot attempts to cross street.”
 This example shows that rationality is not the same as perfection. Rationality max
imizes expected performance, while perfection maximizes actual performance. Retreating
 from a requirement of perfection is not just a question of being fair to agents. The point is
 that if we expect an agent to do what turns out to be the best action after the fact, it will be
 impossible to design an agent to fulfill this specification—unless we improve the performance
 of crystal balls or time machines. Our definition of rationality does not require omniscience, then, because the rational
 choice depends only on the percept sequence to date. We must also ensure that we haven’t
 inadvertently allowed the agent to engage in decidedly underintelligent activities. For exam
ple, if an agent does not look both ways before crossing a busy road, then its percept sequence
 will not tell it that there is a large truck approaching at high speed. Does our definition of
 rationality say that it’s now OK to cross the road? Far from it! First, it would not be rational
 to cross the road given this uninformative percept sequence: the risk of accident from cross
ing without looking is too great. Second, a rational agent should choose the “looking” action
 before stepping into the street, because looking helps maximize the expected performance.
 Doing actions in order to modify future percepts—sometimes called information gather
ing—is an important part of rationality and is covered in depth in Chapter 16. A second
 example of information gathering is provided by the exploration that must be undertaken by
 a vacuum-cleaning agent in an initially unknown environment.
 Our definition requires a rational agent not only to gather information but also to learn
 as much as possible from what it perceives. The agent’s initial configuration could reflect
 some prior knowledge of the environment, but as the agent gains experience this may be
 modified and augmented. There are extreme cases in which the environment is completely
 known apriori. In such cases, the agent need not perceive or learn; it simply acts correctly.
 Of course, such agents are fragile. Consider the lowly dung beetle. After digging its nest and
 laying its eggs, it fetches a ball of dung from a nearby heap to plug the entrance. If the ball of
 dung is removed from its grasp en route, the beetle continues its task and pantomimes plug
ging the nest with the nonexistent dung ball, never noticing that it is missing. Evolution has
 built an assumption into the beetle’s behavior, and when it is violated, unsuccessful behavior
 results. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go
 out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is
 well, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when
 the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches
 away while the sphex is doing the check, it will revert to the “drag” step of its plan and will
 continue the plan without modification, even after dozens of caterpillar-moving interventions.
 The sphex is unable to learn that its innate plan is failing, and thus will not change it.
 To the extent that an agent relies on the prior knowledge of its designer rather than
 on its own percepts, we say that the agent lacks autonomy. A rational agent should be
 autonomous—it should learn what it can to compensate for partial or incorrect prior knowl
edge. For example, a vacuum-cleaning agent that learns to foresee where and when additional
 dirt will appear will do better than one that does not. As a practical matter, one seldom re
quires complete autonomy from the start: when the agent has had little or no experience, it
 would have to act randomly unless the designer gave some assistance. So, just as evolution
 provides animals with enough built-in reflexes to survive long enough to learn for themselves,
 it would be reasonable to provide an artificial intelligent agent with some initial knowledge
 as well as an ability to learn. After sufficient experience of its environment, the behavior
 of a rational agent can become effectively independent of its prior knowledge. Hence, the
 incorporation of learning allows one to design a single rational agent that will succeed in a
 vast variety of environments.
 The idea that we learn by interacting with our environment is probably the first to occur
 to us when we think about the nature of learning. When an infant plays, waves its arms,
 or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection
 to its environment. Exercising this connection produces a wealth of information about
 cause and e↵ect, about the consequences of actions, and about what to do in order to
 achieve goals. Throughout our lives, such interactions are undoubtedly a major source
 of knowledge about our environment and ourselves. Whether we are learning to drive
 a car or to hold a conversation, we are acutely aware of how our environment responds
 to what we do, and we seek to influence what happens through our behavior. Learning
 from interaction is a foundational idea underlying nearly all theories of learning and
 intelligence.
Reinforcement learning is learning what to do—how to map situations to actions—so
 as to maximize a numerical reward signal. The learner is not told which actions to
 take, but instead must discover which actions yield the most reward by trying them. In
 the most interesting and challenging cases, actions may a↵ect not only the immediate
 reward but also the next situation and, through that, all subsequent rewards. These two
 characteristics—trial-and-error search and delayed reward—are the two most important
 distinguishing features of reinforcement learning.
Reinforcement learning, like many topics whose names end with “ing,” such as machine
 learning and mountaineering, is simultaneously a problem, a class of solution methods
 that work well on the problem, and the field that studies this problem and its solution
 methods. It is convenient to use a single name for all three things, but at the same time
 essential to keep the three conceptually separate. In particular, the distinction between
 problems and solution methods is very important in reinforcement learning; failing to
 make this distinction is the source of many confusions.
 We formalize the problem of reinforcement learning using ideas from dynamical sys
tems theory, specifically, as the optimal control of incompletely-known Markov decision
 processes. The details of this formalization must wait until Chapter 3, but the basic idea
 is simply to capture the most important aspects of the real problem facing a learning
 agent interacting over time with its environment to achieve a goal. A learning agent
 must be able to sense the state of its environment to some extent and must be able to
 take actions that a↵ect the state. The agent also must have a goal or goals relating to
 the state of the environment. Markov decision processes are intended to include just
 these three aspects—sensation, action, and goal—in their simplest possible forms without
 trivializing any of them. Any method that is well suited to solving such problems we
 consider to be a reinforcement learning method.
 Reinforcement learning is di↵erent from supervised learning, the kind of learning studied
 in most current research in the field of machine learning. Supervised learning is learning
 from a training set of labeled examples provided by a knowledgable external supervisor.
 Each example is a description of a situation together with a specification—the label—of
 the correct action the system should take in that situation, which is often to identify a
 category to which the situation belongs. The object of this kind of learning is for the
 system to extrapolate, or generalize, its responses so that it acts correctly in situations
 not present in the training set. This is an important kind of learning, but alone it is not
 adequate for learning from interaction. In interactive problems it is often impractical to
 obtain examples of desired behavior that are both correct and representative of all the
 situations in which the agent has to act. In uncharted territory—where one would expect
 learning to be most beneficial—an agent must be able to learn from its own experience.
 Reinforcement learning is also di↵erent from what machine learning researchers call
 unsupervised learning, which is typically about finding structure hidden in collections of
 unlabeled data. The terms supervised learning and unsupervised learning would seem
 to exhaustively classify machine learning paradigms, but they do not. Although one
 might be tempted to think of reinforcement learning as a kind of unsupervised learning
 because it does not rely on examples of correct behavior, reinforcement learning is trying
 to maximize a reward signal instead of trying to find hidden structure. Uncovering
 structure in an agent’s experience can certainly be useful in reinforcement learning, but by
 itself does not address the reinforcement learning problem of maximizing a reward signal.
 We therefore consider reinforcement learning to be a third machine learning paradigm,
 alongside supervised learning and unsupervised learning and perhaps other paradigms.
 One of the challenges that arise in reinforcement learning, and not in other kinds
 of learning, is the trade-o↵ between exploration and exploitation. To obtain a lot of
 reward, a reinforcement learning agent must prefer actions that it has tried in the past
 and found to be e↵ective in producing reward. But to discover such actions, it has to
 try actions that it has not selected before. The agent has to exploit what it has already
 experienced in order to obtain reward, but it also has to explore in order to make better
 action selections in the future. The dilemma is that neither exploration nor exploitation
 can be pursued exclusively without failing at the task. The agent must try a variety of
 actions and progressively favor those that appear to be best. On a stochastic task, each
 action must be tried many times to gain a reliable estimate of its expected reward. The
 exploration–exploitation dilemma has been intensively studied by mathematicians for
 many decades, yet remains unresolved. For now, we simply note that the entire issue of
 balancing exploration and exploitation does not even arise in supervised and unsupervised
 learning, at least in the purest forms of these paradigms.
 Another key feature of reinforcement learning is that it explicitly considers the whole
 problem of a goal-directed agent interacting with an uncertain environment. This is in
 contrast to many approaches that consider subproblems without addressing how they
 might fit into a larger picture. For example, we have mentioned that many machine
 learning researchers have studied supervised learning without specifying how such an
 ability would ultimately be useful. Other researchers have developed theories of planning
 with general goals, but without considering planning’s role in real-time decision making,
 or the question of where the predictive models necessary for planning would come from.
 Although these approaches have yielded many useful results, their focus on isolated
 subproblems is a significant limitation.
 Reinforcement learning takes the opposite tack, starting with a complete, interactive,
 goal-seeking agent. All reinforcement learning agents have explicit goals, can sense
 aspects of their environments, and can choose actions to influence their environments.
 Moreover, it is usually assumed from the beginning that the agent has to operate despite
 significant uncertainty about the environment it faces. When reinforcement learning
 involves planning, it has to address the interplay between planning and real-time action
 selection, as well as the question of how environment models are acquired and improved.
 When reinforcement learning involves supervised learning, it does so for specific reasons
 that determine which capabilities are critical and which are not. For learning research to
 make progress, important subproblems have to be isolated and studied, but they should
 be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if
 all the details of the complete agent cannot yet be filled in.
 By a complete, interactive, goal-seeking agent we do not always mean something like
 a complete organism or robot. These are clearly examples, but a complete, interactive,
 goal-seeking agent can also be a component of a larger behaving system. In this case, the
 agent directly interacts with the rest of the larger system and indirectly interacts with
 the larger system’s environment. A simple example is an agent that monitors the charge
 level of robot’s battery and sends commands to the robot’s control architecture. This
 agent’s environment is the rest of the robot together with the robot’s environment. It is important to look beyond the most obvious examples of agents and their environments
 to appreciate the generality of the reinforcement learning framework.
 One of the most exciting aspects of modern reinforcement learning is its substantive
 and fruitful interactions with other engineering and scientific disciplines. Reinforcement
 learning is part of a decades-long trend within artificial intelligence and machine learning
 toward greater integration with statistics, optimization, and other mathematical subjects.
 For example, the ability of some reinforcement learning methods to learn with parameter
ized approximators addresses the classical “curse of dimensionality” in operations research
 and control theory. More distinctively, reinforcement learning has also interacted strongly
 with psychology and neuroscience, with substantial benefits going both ways. Of all the
 forms of machine learning, reinforcement learning is the closest to the kind of learning
 that humans and other animals do, and many of the core algorithms of reinforcement
 learning were originally inspired by biological learning systems. Reinforcement learning
 has also given back, both through a psychological model of animal learning that better
 matches some of the empirical data, and through an influential model of parts of the
 brain’s reward system. The body of this book develops the ideas of reinforcement learning
 that pertain to engineering and artificial intelligence, with connections to psychology and
 neuroscience summarized in Chapters 14 and 15.
 Finally, reinforcement learning is also part of a larger trend in artificial intelligence
 back toward simple general principles. Since the late 1960s, many artificial intelligence re
searchers presumed that there are no general principles to be discovered, that intelligence is
 instead due to the possession of a vast number of special purpose tricks, procedures, and
 heuristics. It was sometimes said that if we could just get enough relevant facts into a
 machine, say one million, or one billion, then it would become intelligent. Methods based
 on general principles, such as search or learning, were characterized as “weak methods,”
 whereas those based on specific knowledge were called “strong methods.” This view is
 uncommon today. From our point of view, it was premature: too little e↵ort had been
 put into the search for general principles to conclude that there were none. Modern
 artificial intelligence now includes much research looking for general principles of learning,
 search, and decision making. It is not clear how far back the pendulum will swing, but
 reinforcement learning research is certainly part of the swing back toward simpler and
 fewer general principles of artificial intelligence.
 A good way to understand reinforcement learning is to consider some of the examples
 and possible applications that have guided its development.
 • Amaster chess player makes a move. The choice is informed both by planning—
 anticipating possible replies and counterreplies—and by immediate, intuitive judg
ments of the desirability of particular positions and moves.
 • An adaptive controller adjusts parameters of a petroleum refinery’s operation in
 real time. The controller optimizes the yield/cost/quality trade-o on the basis
 of specified marginal costs without sticking strictly to the set points originally
 suggested by engineers.
 • Agazelle calf struggles to its feet minutes after being born. Half an hour later it is
 running at 20 miles per hour.
 • Amobile robot decides whether it should enter a new room in search of more trash
 to collect or start trying to find its way back to its battery recharging station. It
 makes its decision based on the current charge level of its battery and how quickly
 and easily it has been able to find the recharger in the past.
 • Phil prepares his breakfast. Closely examined, even this apparently mundane
 activity reveals a complex web of conditional behavior and interlocking goal–subgoal
 relationships: walking to the cupboard, opening it, selecting a cereal box, then
 reaching for, grasping, and retrieving the box. Other complex, tuned, interactive
 sequences of behavior are required to obtain a bowl, spoon, and milk carton. Each
 step involves a series of eye movements to obtain information and to guide reaching
 and locomotion. Rapid judgments are continually made about how to carry the
 objects or whether it is better to ferry some of them to the dining table before
 obtaining others. Each step is guided by goals, such as grasping a spoon or getting
 to the refrigerator, and is in service of other goals, such as having the spoon to eat
 with once the cereal is prepared and ultimately obtaining nourishment. Whether
 he is aware of it or not, Phil is accessing information about the state of his body
 that determines his nutritional needs, level of hunger, and food preferences.
 These examples share features that are so basic that they are easy to overlook. All
 involve interaction between an active decision-making agent and its environment, within
 which the agent seeks to achieve a goal despite uncertainty about its environment. The
 agent’s actions are permitted to a↵ect the future state of the environment (e.g., the
 next chess position, the level of reservoirs of the refinery, the robot’s next location and
 the future charge level of its battery), thereby a↵ecting the actions and opportunities
 available to the agent at later times. Correct choice requires taking into account indirect,
 delayed consequences of actions, and thus may require foresight or planning.
 At the same time, in all of these examples the e↵ects of actions cannot be fully predicted;
 thus the agent must monitor its environment frequently and react appropriately. For
 example, Phil must watch the milk he pours into his cereal bowl to keep it from overflowing.
 All these examples involve goals that are explicit in the sense that the agent can judge
 progress toward its goal based on what it can sense directly. The chess player knows
 whether or not he wins, the refinery controller knows how much petroleum is being
 produced, the gazelle calf knows when it falls, the mobile robot knows when its batteries
 run down, and Phil knows whether or not he is enjoying his breakfast.
 In all of these examples the agent can use its experience to improve its performance
 over time. The chess player refines the intuition he uses to evaluate positions, thereby
 improving his play; the gazelle calf improves the e ciency with which it can run; Phil
 learns to streamline making his breakfast. The knowledge the agent brings to the task at
 the start—either from previous experience with related tasks or built into it by design or
evolution—influences what is useful or easy to learn, but interaction with the environment
 is essential for adjusting behavior to exploit specific features of the task.
Two set of spaces in Program:
Action Space - Discrete values of action. (Related to Action)
Observation Space - Box values of Environment. (Related to Env and agent)
Types of RL algorithms:
Model-free: [Q-learning, Policy Optimization]
	- Only uses current state for prediction of next action or reward.
Model-based: [Learn the Model, Given the Model]
	- Tries to make the prediction of future state of the model and
	  try to generate best possible action.
Model-based, as it sounds, has an agent trying to understand its environment and creating a model for it based on its interactions with this environment. In such a system, preferences take priority over the consequences of the actions i.e. the greedy agent will always try to perform an action that will get the maximum reward irrespective of what that action may cause.
On the other hand, model-free algorithms seek to learn the consequences of their actions through experience via algorithms such as Policy Gradient, Q-Learning, etc. In other words, such an algorithm will carry out an action multiple times and will adjust the policy (the strategy behind its actions) for optimal rewards, based on the outcomes.
Think of it this way, if the agent can predict the reward for some action before actually performing it thereby planning what it should do, the algorithm is model-based. While if it actually needs to carry out the action to see what happens and learn from it, it is model-free.
This results in different applications for these two classes, for e.g. a model-based approach may be the perfect fit for playing chess or for a robotic arm in the assembly line of a product, where the environment is static and getting the task done most efficiently is our main concern. 
However, in the case of real-world applications such as self-driving cars, a model-based approach might prompt the car to run over a pedestrian to reach its destination in less time (maximum reward), but a model-free approach would make the car wait till the road is clear (optimal way out).
Stable baseline only deals with Model-free.
Training Metrics:
- Evaluation Metrics: Ep_len_mean, Ep_rew_mean
- Time Metrics: FPS, iterations, time_elapsed, total_timesteps
- Loss Metrics: Entropy_loss, Policy_loss, Value_loss
- Other Metrics: Explained_variance, Learning_rate, n_updates.
For PPO, in stablebaselines-3, it have mlppolicy, cnnploicy and
multiinput policy. There is no lstmpolicy. Only stablebaselines-2 has 
lstmpolicy.
To get the deatils ablout class, type algorithm name with ??. Ex: PPO??
Core metrics to look at:
- Average Reward
- Average Episode length (How long agent is lasting in particular episode)
To improvise the model:
- Train longer
- Hyperparameter Tuning (Stablbaseline supports HT using optuna package)
- Try different models
Multi-armed Bandits
The most important feature distinguishing reinforcement learning from other types of
learning is that it uses training information that evaluates the actions taken rather
than instructs by giving correct actions. This is what creates the need for active
exploration, for an explicit search for good behavior. Purely evaluative feedback indicates
how good the action taken was, but not whether it was the best or the worst action
possible. Purely instructive feedback, on the other hand, indicates the correct action to
take, independently of the action actually taken. This kind of feedback is the basis of
supervised learning, which includes large parts of pattern classification, artificial neural
networks, and system identification. In their pure forms, these two kinds of feedback
are quite distinct: evaluative feedback depends entirely on the action taken, whereas
instructive feedback is independent of the action taken.
In this chapter we study the evaluative aspect of reinforcement learning in a simplified
setting, one that does not involve learning to act in more than one situation. This
nonassociative setting is the one in which most prior work involving evaluative feedback
has been done, and it avoids much of the complexity of the full reinforcement learning
problem. Studying this case enables us to see most clearly how evaluative feedback di↵ers
from, and yet can be combined with, instructive feedback.
The particular nonassociative, evaluative feedback problem that we explore is a simple
version of the k -armed bandit problem. We use this problem to introduce a number
of basic learning methods which we extend in later chapters to apply to the full rein-
forcement learning problem. At the end of this chapter, we take a step closer to the full
reinforcement learning problem by discussing what happens when the bandit problem
becomes associative, that is, when the best action depends on the situation.
2.1 A k-armed Bandit Problem
Consider the following learning problem. You are faced repeatedly with a choice among
k di↵erent options, or actions. After each choice you receive a numerical reward chosen
from a stationary probability distribution that depends on the action you selected. Your
26 Chapter 2: Multi-armed Bandits
objective is to maximize the expected total reward over some time period, for example,
over 1000 action selections, or time steps.
This is the original form of the k-armed bandit problem, so named by analogy to a slot
machine, or “one-armed bandit,” except that it has k levers instead of one. Each action
selection is like a play of one of the slot machine’s levers, and the rewards are the payo↵s
for hitting the jackpot. Through repeated action selections you are to maximize your
winnings by concentrating your actions on the best levers. Another analogy is that of
a doctor choosing between experimental treatments for a series of seriously ill patients.
Each action is the selection of a treatment, and each reward is the survival or well-being
of the patient. Today the term “bandit problem” is sometimes used for a generalization
of the problem described above, but in this book we use it to refer just to this simple
case.
If you maintain estimates of the action values, then at any time step there is at least
one action whose estimated value is greatest. We call these the greedy actions. When you
select one of these actions, we say that you are exploiting your current knowledge of the
values of the actions. If instead you select one of the nongreedy actions, then we say you
are exploring, because this enables you to improve your estimate of the nongreedy action’s
value. Exploitation is the right thing to do to maximize the expected reward on the one
step, but exploration may produce the greater total reward in the long run. For example,
suppose a greedy action’s value is known with certainty, while several other actions are
estimated to be nearly as good but with substantial uncertainty. The uncertainty is
such that at least one of these other actions probably is actually better than the greedy
action, but you don’t know which one. If you have many time steps ahead on which
to make action selections, then it may be better to explore the nongreedy actions and
discover which of them are better than the greedy action. Reward is lower in the short
run, during exploration, but higher in the long run because after you have discovered
the better actions, you can exploit them many times. Because it is not possible both to
explore and to exploit with any single action selection, one often refers to the “conflict”
between exploration and exploitation.
In any specific case, whether it is better to explore or exploit depends in a complex
way on the precise values of the estimates, uncertainties, and the number of remaining
steps. There are many sophisticated methods for balancing exploration and exploitation
for particular mathematical formulations of the k -armed bandit and related problems.
2.2. Action-value Methods 27
However, most of these methods make strong assumptions about stationarity and prior
knowledge that are either violated or impossible to verify in most applications and in
the full reinforcement learning problem that we consider in subsequent chapters. The
guarantees of optimality or bounded loss for these methods are of little comfort when the
assumptions of their theory do not apply.
In this book we do not worry about balancing exploration and exploitation in a
sophisticated way; we worry only about balancing them at all. In this chapter we present
several simple balancing methods for the k -armed bandit problem and show that they
work much better than methods that always exploit. The need to balance exploration
and exploitation is a distinctive challenge that arises in reinforcement learning; the
simplicity of our version of the k -armed bandit problem enables us to show this in a
particularly clear form.
2.2 Action-value Methods
We begin by looking more closely at methods for estimating the values of actions and
for using the estimates to make action selection decisions, which we collectively call
action-value methods. Recall that the true value of an action is the mean reward when
that action is selected.
28 Chapter 2: Multi-armed Bandits
from among all the actions with equal probability, independently of the action-value
estimates. We call methods using this near-greedy action selection rule "-greedy methods.
An advantage of these methods is that, in the limit as the number of steps increases,
every action will be sampled an infinite number of times, thus ensuring that all the Qt(a)
converge to their respective q⇤(a). This of course implies that the probability of selecting
the optimal action converges to greater than 1  ", that is, to near certainty. These are
just asymptotic guarantees, however, and say little about the practical e↵ectiveness of
the methods.
Multi-armed Bandits

shows that the greedy method found the optimal action in only approximately one-third
of the tasks. In the other two-thirds, its initial samples of the optimal action were
disappointing, and it never returned to it. The "-greedy methods eventually performed
better because they continued to explore and to improve their chances of recognizing
the optimal action. The " = 0.1 method explored more, and usually found the optimal
action earlier, but it never selected that action more than 91% of the time. The " = 0.01
method improved more slowly, but eventually would perform better than the " = 0.1
method on both performance measures shown in the figure. It is also possible to reduce "
over time to try to get the best of both high and low values.

The advantage of "-greedy over greedy methods depends on the task. For example,
suppose the reward variance had been larger, say 10 instead of 1. With noisier rewards
it takes more exploration to find the optimal action, and "-greedy methods should fare
even better relative to the greedy method. On the other hand, if the reward variances
were zero, then the greedy method would know the true value of each action after trying
it once. In this case the greedy method might actually perform best because it would
soon find the optimal action and then never explore. But even in the deterministic case
there is a large advantage to exploring if we weaken some of the other assumptions. For
example, suppose the bandit task were nonstationary, that is, the true values of the
actions changed over time. In this case exploration is needed even in the deterministic
case to make sure one of the nongreedy actions has not changed to become better than
the greedy one. As we shall see in the next few chapters, nonstationarity is the case
most commonly encountered in reinforcement learning. Even if the underlying task is
stationary and deterministic, the learner faces a set of banditlike decision tasks each of
which changes over time as learning proceeds and the agent’s decision-making policy
changes. Reinforcement learning requires a balance between exploration and exploitation.
Exercise 2.2: Bandit example Consider a k -armed bandit problem with k = 4 actions,
denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using
"-greedy action selection, sample-average action-value estimates, and initial estimates
of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1,
R1 =  1, A2 = 2, R2 = 1, A3 = 2, R3 =  2, A4 = 2, R4 = 2, A5 = 3, R5 = 0. On some
of these time steps the " case may have occurred, causing an action to be selected at
random. On which time steps did this definitely occur? On which time steps could this
possibly have occurred? ⇤
Exercise 2.3 In the comparison shown in Figure 2.2, which method will perform best in
the long run in terms of cumulative reward and probability of selecting the best action?
How much better will it be? Express your answer quantitatively. ⇤
2.4 Incremental Implementation
The action-value methods we have discussed so far all estimate action values as sample
averages of observed rewards. We now turn to the question of how these averages can be
computed in a computationally e cient manner, in particular, with constant memory
and constant per-time-step computation.
Fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production. This knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage. This makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.
Estimates of the size of adult vocabularies vary widely both within and across languages. For example, estimates of the vocabulary size of young adult speakers of American English range from 30,000 to 100,000 depending on the resources used to make the estimate and the definition of what it means to know a word. What is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with care givers and peers, usually well before the start of formal schooling. This active vocabulary is extremely limited compared to the size of the adult vocabulary (usually on the order of 2000 words for young speakers) and is quite stable, with very few additional words learned via casual conversation beyond this early stage. Obviously, this leaves a very large number of words to be acquired by other means.
A simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age. And indeed empirical estimates of vocabulary growth in late elementary through high school are consistent with this rate. How do children achieve this rate of vocabulary growth? Most of this growth is not happening through direct vocabulary instruction in school, which is not deployed at the rate that would be required to result in sufficient vocabulary growth.
The most likely explanation is that the bulk of this knowledge acquisition hap- pens as a by-product of reading, as part of the rich processing and reasoning that we perform when we read. Research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate. But the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!
Many of these facts have motivated approaches to word learning based on the distributional hypothesis, introduced in Chapter 6. This is the idea that something about what we’re loosely calling word meanings can be learned even without any grounding in the real world, solely based on the content of the texts we encounter over our lives. This knowledge is based on the complex association of words with the words they co-occur with (and with the words that those words occur with).
The crucial insight of the distributional hypothesis is that the knowledge that we acquire through this process can be brought to bear long after its initial acquisition.
Of course, adding grounding from vision or from real-world interaction can help build even more powerful models, but even text alone is remarkably useful.
In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remark- able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.
The standard architecture for building large language models is the transformer. We thus begin this chapter by introducing this architecture in detail. The transformer makes use of a novel mechanism called self-attention, which developed out of the idea of attention that was introduced for RNNs in Chapter 9. Self-attention can be thought of a way to build contextual representations of a word’s meaning that integrate information from surrounding words, helping the model learn how words relate to each other over large spans of text.
We’ll then see how to apply the transformer to language modeling, in a setting of- ten called causal or autoregressive language models, in which we iteratively predict words left-to-right from earlier words. These language models, like the feedforward and RNN language models we have already seen, are thus self-trained: given a large corpus of text, we iteratively teach the model to guess the next word in the text from the prior words. In addition to training, we’ll introduce algorithms for generating texts, including important methods like greedy decoding, beam search, and sam- pling. And we’ll talk about the components of popular large language models like the GPT family.
Finally, we’ll see the great power of language models: almost any NLP task can be modeled as word prediction, if we think about it in the right way. We’ll work through an example of using large language models to solve one NLP task of summarization (generating a short text that summarizes some larger document). The use of a large language model to generate text is one of the areas in which the impact of the last decade of neural algorithms for NLP has been the largest. Indeed, text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI.
We’ll save three more areas of large language models for the next three chapters; Chapter 11 will introduce the bidirectional transformer encoder and the method of masked language modeling, used for the popular BERT family of models. Chap- ter 12 will introduce the most powerful way to interact with large language models: prompting them to perform other NLP tasks by simply giving directions or instruc- tions in natural language to a transformer that is pretrained on language modeling. And Chapter 13 will introduce the use of the encoder-decoder architecture for trans- formers in the context of machine translation.
Transformers are made up of stacks of transformer blocks, each of which is a multilayer network that maps sequences of input vectors (x1, ..., xn) to sequences of output vectors (z1, ..., zn) of the same length. These blocks are made by combin- ing simple linear layers, feedforward networks, and self-attention layers, the key
innovation of transformers. Self-attention allows a network to directly extract and use information from arbitrarily large contexts. We’ll start by describing how self- attention works and then return to how it fits into larger transformer blocks. Finally, we’ll describe how to use the transformer block together with some input and output mechanisms as a language model, to predict upcoming words from prior words in the context.
The intuition of a transformer is that across a series of layers, we build up richer and richer contextualized representations of the meanings of input words or tokens (we will refer to the input as a sequence of words for convenience, although technically the input is first tokenized by an algorithm like BPE, so it is a series of tokens rather than words). At each layer of a transformer, to compute the representation of a word i we combine information from the representation of i at the previous layer with information from the representations of the neighboring words. The goal is to produce a contextualized representation for each word at each position. We can think of these representations as a contextualized version of the static vectors we saw in Chapter 6, which each represented the meaning of a word type. By contrast, our goal in transformers is to produce a contextualized version, something that represents what this word means in the particular context in which it occurs.
We thus need a mechanism that tells us how to weigh and combine the represen- tations of the different words from the context at the prior level in order to compute our representation at this layer. This mechanism must be able to look broadly in the context, since words have rich linguistic relationships with words that can be many sentences away. Even within the sentence, words have important linguistic relation- ships with contextual words. Consider these examples, each exhibiting linguistic relationships that we’ll discuss in more depth in later chapters:
(10.1) The keys to the cabinet are on the table.
(10.2) The chicken crossed the road because it wanted to get to the other side. (10.3) I walked along the pond, and noticed that one of the trees along the bank
had fallen into the water after the storm.
In (10.1), the phrase The keys is the subject of the sentence, and in English and many languages, must agree in grammatical number with the verb are; in this case both are plural. In English we can’t use a singular verb like is with a plural sub- ject like keys; we’ll discuss agreement more in Chapter 17. In (10.2), the pronoun it corefers to the chicken; it’s the chicken that wants to get to the other side. We’ll discuss coreference more in Chapter 26. In (10.3), the way we know that bank refers to the side of a pond or river and not a financial institution is from the context, in- cluding words like pond and water. We’ll discuss word senses more in Chapter 23. These helpful contextual words can be quite far way in the sentence or paragraph, so we need a mechanism that can look broadly in the context to help compute rep- resentations for words.
Self-attention is just such a mechanism: it allows us to look broadly in the con- text and tells us how to integrate the representation from words in that context from layer k − 1 to build the representation for words in layer k.
Here we want to compute a contextual representation for the word it, at layer 6 of the transformer, and we’d like that representation to draw on the represen- tations of all the prior words, from layer 5. The figure uses color to represent the attention distribution over the contextual words: the word animal has a high atten- tion weight, meaning that as we are computing the representation for it, we will draw most heavily on the representation for animal. This will be useful for the model to build a representation that has the correct meaning for it, which indeed is corefer- ent here with the word animal. (We say that a pronoun like it is coreferent with a noun like animal if they both refer to the same thing; we’ll return to coreference in Chapter 26.)
10.1.2	Causal or backward-looking self-attention
The concept of context can be used in two ways in self-attention. In causal, or backward looking self-attention, the context is any of the prior words. In general bidirectional self-attention, the context can include future words. In this chapter we focus on causal, backward looking self-attention; we’ll introduce bidirectional self-attention in Chapter 11.
Fig. 10.2 thus illustrates the flow of information in a single causal, or backward looking, self-attention layer. As with the overall transformer, a self-attention layer maps input sequences (x1, ..., xn) to output sequences of the same length (a1, ..., an). When processing each item in the input, the model has access to all of the inputs
up to and including the one under consideration, but no access to information about inputs beyond the current one. In addition, the computation performed for each item is independent of all the other computations. The first point ensures that we can use this approach to create language models and use them for autoregressive generation, and the second point means that we can easily parallelize both forward inference and training of such models.
10.1.3	Self-attention more formally
We’ve given the intuition of self-attention (as a way to compute representations of a word at a given layer by integrating information from words at the previous layer) and we’ve defined context as all the prior words in the input. Let’s now introduce the self-attention computation itself.
The core intuition of attention is the idea of comparing an item of interest to a collection of other items in a way that reveals their relevance in the current context. In the case of self-attention for language, the set of comparisons are to other words (or tokens) within a given sequence. The result of these comparisons is then used to compute an output sequence for the current input sequence. For example, returning to Fig. 10.2, the computation of a3 is based on a set of comparisons between the input x3 and its preceding elements x1 and x2, and to x3 itself.
How shall we compare words to other words? Since our representations for words are vectors, we’ll make use of our old friend the dot product that we used for computing word similarity in Chapter 6, and also played a role in attention in Chapter 9. Let’s refer to the result of this comparison between words i and j as a score (we’ll be updating this equation to add attention to the computation of this score):
The result of a dot product is a scalar value ranging from ∞ to ∞, the larger the value the more similar the vectors that are being compared. Continuing with our example, the first step in computing y3 would be to compute three scores: x3 x1, x3 x2 and x3 x3. Then to make effective use of these scores, we’ll normalize them with a softmax to create a vector of weights, αi j, that indicates the proportional relevance of each input to the input element i that is the current focus of attention.
Of course, the softmax weight will likely be highest for the current focus element i, since vecxi is very similar to itself, resulting in a high dot product. But other context words may also be similar to i, and the softmax will also assign some weight to those words.
Given the proportional scores in α, we generate an output value ai by summing the inputs seen so far, each weighted by its α value.
The steps embodied in Equations 10.4 through 10.7 represent the core of an attention-based approach: a set of comparisons to relevant items in some context, a normalization of those scores to provide a probability distribution, followed by a weighted sum using this distribution. The output a is the result of this straightfor- ward computation over the inputs.
This kind of simple attention can be useful, and indeed we saw in Chapter 9 how to use this simple idea of attention for LSTM-based encoder-decoder models for machine translation. But transformers allow us to create a more sophisticated way of representing how words can contribute to the representation of longer inputs. Consider the three different roles that each input embedding plays during the course of the attention process.
•As the current focus of attention when being compared to all of the other preceding inputs. We’ll refer to this role as a query.
•In its role as a preceding input being compared to the current focus of atten- tion. We’ll refer to this role as a key.
•And finally, as a value used to compute the output for the current focus of attention.
To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value
qi = xiWQ; ki = xiWK; vi = xiWV	(10.8)
The inputs x and outputs y of transformers, as well as the intermediate vectors after the various layers like the attention output vector a, all have the same dimensionality
1 d. We’ll have a dimension dk for the key and query vectors, and a separate dimension dv for the value vectors. In the original transformer work, d was 512, dk and dv were both 64. The shapes of the transform matrices are then WQ  Rd×dk , WK  Rd×dk , and WV  Rd×dv.
Given these projections, the score between a current focus of attention, xi, and
an element in the preceding context, x j, consists of a dot product between its query vector qi and the preceding element’s key vectors k j. This dot product has the right shape since both the query and the key are of dimensionality 1 dk. Let’s update our previous comparison calculation to reflect this, replacing Eq. 10.4 with Eq. 10.9:
Verson 2:	score(xi, x j) = qi · k j	(10.9)
The ensuing softmax calculation resulting in αi, j remains the same, but the output calculation for ai is now based on a weighted sum over the value vectors v.
ai =	αi jv j	(10.10)
j≤i
Again, the softmax weight αi j will likely be highest for the current focus element i, and so the value for yi will be most influenced by vi. But the model will also pay attention to other contextual words if they are similar to i, allowing their values to
also influence the final value of v j. Context words that are not similar to i will have their values downweighted and won’t contribute to the final value.
There is one final part of the self-attention model. The result of a dot product can be an arbitrarily large (positive or negative) value. Exponentiating large values can lead to numerical issues and to an effective loss of gradients during training. To avoid this, we scale down the result of the dot product, by dividing it by a factor related to the size of the embeddings. A typical approach is to divide by the square root of the dimensionality of the query and key vectors (dk), leading us to update our scoring function one more time, replacing Eq. 10.4 and Eq. 10.9 with Eq. 10.12. Here’s a final set of equations for computing self-attention for a single self-attention output vector ai from a single input vector xi, illustrated in Fig. 10.3 for the case of calculating the value of the third output a3 in a sequence.
qi = xiWQ; ki = xiWK; vi = xiWV	(10.11)
qi · k j
Final verson:	score(xi, x j) =  √dk	(10.12)
αi j = softmax(score(xi, x j)) ∀ j ≤ i	(10.13)
ai =	αi jv j	(10.14)
j≤i
10.1.4	Parallelizing self-attention using a single matrix X
This description of the self-attention process has been from the perspective of com- puting a single output at a single time step i. However, since each output, yi, is computed independently, this entire process can be parallelized, taking advantage of efficient matrix multiplication routines by packing the input embeddings of the N tokens of the input sequence into a single matrix X  RN×d. That is, each row of X is the embedding of one token of the input. Transformers for large language models can have an input length N = 1024, 2048, or 4096 tokens, so X has between 1K and 4K rows, each of the dimensionality of the embedding d.
We then multiply X by the key, query, and value matrices (all of dimensionality d  d) to produce matrices Q  RN×d, K  RN×d, and V  RN×d, containing all the key, query, and value vectors:
Q = XWQ; K = XWK; V = XWV	(10.15)
Given these matrices we can compute all the requisite query-key comparisons simul- taneously by multiplying Q and K| in a single matrix multiplication (the product is of shape N N; Fig. 10.4 shows a visualization). Taking this one step further, we can scale these scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N d: a vector embedding representation for each token in the input. We’ve reduced the entire self-attention step for an entire sequence of N tokens to the following computation: 
A = SelfAttention(Q, K, V)
Multihead Attention
Transformers actually compute a more complex kind of attention than the single self-attention calculation we’ve seen so far. This is because the different words in a sentence can relate to each other in many different ways simultaneously. For exam- ple, distinct syntactic, semantic, and discourse relationships can hold between verbs and their arguments in a sentence. It would be difficult for a single self-attention model to learn to capture all of the different kinds of parallel relations among its in- puts. Transformers address this issue with multihead self-attention layers. These layers  are sets of self-attention layers, called heads, that reside in parallel layers at the same depth in a model, each with its own set of parameters. By using these distinct sets of parameters, each head can learn different aspects of the relationships among inputs at the same level of abstraction.
To implement this notion, each head, i, in a self-attention layer is provided with
its own set of key, query and value matrices: WK, WQ and WV . These are used
to project the inputs into separate key, value, and query embeddings separately for each head, with the rest of the self-attention computation remaining unchanged.
In multi-head attention, as with self-attention, the model dimension d is still used for the input and output, the key and query embeddings have dimensionality dk, and the value embeddings are of dimensionality dv (again, in the original transformer paper dk = dv = 64, h = 8, and d = 512). Thus for each head i, we have weight
layers WQ ∈ Rd×dk , WK ∈ Rd×dk , and WV ∈ Rd×dv , and these get multiplied by
the inputs packed into X to produce Q RN×dk , K RN×dk , and V RN×dv . The output of each of the h heads is of shape N  dv, and so the output of the multi-head
layer with h heads consists of h matrices of shape N dv. To make use of these matrices in further processing, they are concatenated to produce a single output with dimensionality N hdv. Finally, we use yet another linear projection WO Rhdv×d , that reshape it to the original output dimension for each token. Multiplying the concatenated N  hdv matrix output by WO  Rhdv×d yields the self-attention output A of shape [N d], suitable to be passed through residual connections and layer norm.
Q = XWQ ; K = XWK ; V = XWV
headi = SelfAttention(Q, K, V)	(10.18)
A = MultiHeadAttention(X) = (head1 ⊕ head2... ⊕ headh)WO
 The Rise of Deep Learning
 Deep learning re-energized neural net
work research in the early 2000s by in
troducing a few elements which made 
it easy to train deeper networks. The 
emergence of GPUs and the availability 
of large datasets were key enablers of 
deep learning and they were greatly en
hanced by the development of open 
source, flexible software platforms 
with automatic differentiation such as 
Theano,16 Torch,25 Caffe,55 Tensor
Flow,1 and PyTorch.71 This made it easy 
to train complicated deep nets and to 
reuse the latest models and their build
ing blocks. But the composition of 
more layers is what allowed more com
plex non-linearities and achieved sur
prisingly good results in perception 
tasks, as summarized here.
 Why depth? Although the intuition 
that deeper neural networks could be 
more powerful pre-dated modern deep 
learning techniques,82 it was a series of 
advances in both architecture and 
training procedures,15,35,48 which ush
ered in the remarkable advances which 
are associated with the rise of deep 
learning. But why might deeper net
works generalize better for the kinds of 
input-output relationships we are in
terested in modeling? It is important 
to realize that it is not simply a ques
tion of having more parameters, since 
deep networks often generalize better 
than shallow networks with the same 
number of parameters.15 The practice 
confirms this. The most popular class 
of convolutional net architecture for 
computer vision is the ResNet family43 
of which the most common represen
tative, ResNet-50 has 50 layers. Other 
ingredients not mentioned in this arti
cle but which turned out to be very use
ful include image deformations, drop
out,51 and batch normalization.53
 We believe that deep networks excel 
because they exploit a particular form 
of compositionality in which features 
in one layer are combined in many dif
ferent ways to create more abstract fea
tures in the next layer.
 For tasks like perception, this kind 
of compositionality works very well and 
there is strong evidence that it is used 
by biological perceptual systems.83
 Unsupervised pre-training. When the 
number of labeled training examples is 
small compared with the complexity of 
the neural network required to perform 
the task, it makes sense to start by using 
some other source of information to cre
ate layers of feature detectors and then 
to fine-tune these feature detectors us
ing the limited supply of labels. In trans
fer learning, the source of information is 
another supervised learning task that 
has plentiful labels. But it is also possi
ble to create layers of feature detectors 
without using any labels at all by stack
ing auto-encoders.15,50,59
 First, we learn a layer of feature de
tectors whose activities allow us to re
construct the input. Then we learn a 
second layer of feature detectors whose 
activities allow us to reconstruct the ac
tivities of the first layer of feature detec
tors. After learning several hidden lay
ers in this way, we then try to predict 
the label from the activities in the last 
hidden layer and we backpropagate the 
errors through all of the layers in order 
to fine-tune the feature detectors that 
were initially discovered without using 
the precious information in the labels. 
The pre-training may well extract all 
sorts of structure that is irrelevant to 
the final classification but, in the re
gime where computation is cheap and 
labeled data is expensive, this is fine so 
long as the pre-training transforms the 
input into a representation that makes 
classification easier.
 In addition to improving generaliza
tion, unsupervised pre-training initial
izes the weights in such a way that it is 
easy to fine-tune a deep neural network 
with backpropagation. The effect of 
pre-training on optimization was his
torically important for overcoming the 
accepted wisdom that deep nets were 
hard to train, but it is much less rele
vant now that people use rectified lin
ear units (see next section) and residu
al connections.43 However, the effect of 
pre-training on generalization has 
proved to be very important. It makes it 
possible to train very large models by 
leveraging large quantities of unla
beled data, for example, in natural lan
guage processing, for which huge cor
pora are available.26,32 The general 
principle of pre-training and fine-tun
ing has turned out to be an important 
tool in the deep learning toolbox, for 
example, when it comes to transfer 
learning or even as an ingredient of 
modern meta-learning.33
 The mysterious success of rectified 
linear units. The early successes of 
deep networks involved unsupervised 
pre-training of layers of units that used 
the logistic sigmoid nonlinearity or the 
closely related hyperbolic tangent. Rec
tified linear units had long been hy
pothesized in neuroscience29 and al
ready used in some variants of RBMs70 
and convolutional neural networks.54 It 
was an unexpected and pleasant sur
prise to discover35 that rectifying non
linearities (now called ReLUs, with 
many modern variants) made it easy to 
train deep networks by backprop and 
stochastic gradient descent, without 
the need for layerwise pre-training. 
This was one of the technical advances 
that enabled deep learning to outper
form previous methods for object rec
ognition,60 as outlined here.
 Breakthroughs in speech and object 
recognition. An acoustic model con
verts a representation of the sound wave 
into a probability distribution over frag
ments of phonemes. Heroic efforts by 
Robinson72 using transputers and by 
Morgan et al.69 using DSP chips had al
ready shown that, with sufficient pro
cessing power, neural networks were 
competitive with the state of the art for 
acoustic modeling. In 2009, two gradu
ate students68 using Nvidia GPUs 
showed that pre-trained deep neural 
nets could slightly outperform the SOTA 
on the TIMIT dataset. This result reig
nited the interest of several leading 
speech groups in neural networks. In 
2010, essentially the same deep network 
was shown to beat the SOTA for large vo
cabulary speech recognition without re
quiring speaker-dependent training28,46 
and by 2012, Google had engineered a 
production version that significantly 
improved voice search on Android. This 
was an early demonstration of the dis
ruptive power of deep learning.
 At about the same time, deep learn
ing scored a dramatic victory in the 
2012 ImageNet competition, almost 
halving the error rate for recognizing a 
thousand different classes of object in 
natural images.60 The keys to this vic
tory were the major effort by Fei-Fei Li 
and her collaborators in collecting 
more than a million labeled images31 
for the training set and the very effi
cient use of multiple GPUs by Alex 
Krizhevsky. Current hardware, includ
ing GPUs, encourages the use of large 
mini-batches in order to amortize the 
cost of fetching a weight from memory across many uses of that weight. Pure 
online stochastic gradient descent 
which uses each weight once converges 
faster and future hardware may just 
use weights in place rather than fetch
ing them from memory. 
The deep convolutional neural net 
contained a few novelties such as the 
use of ReLUs to make learning faster 
and the use of dropout to prevent over
fitting, but it was basically just a feed
forward convolutional neural net of the 
kind that Yann LeCun and his collabo
rators had been developing for many 
years.64,65 The response of the computer 
vision community to this breakthrough 
was admirable. Given this incontro
vertible evidence of the superiority of 
convolutional neural nets, the commu
nity rapidly abandoned previous hand
engineered approaches and switched 
to deep learning.
 Recent Advances
 Here we selectively touch on some of 
the more recent advances in deep 
learning, clearly leaving out many im
portant subjects, such as deep rein
forcement learning, graph neural net
works and meta-learning.
 Unsupervised and self-supervised 
learning. Supervised learning, while 
successful in a wide variety of tasks, 
typically requires a large amount of 
human-labeled data. Similarly, when 
reinforcement learning is based only 
on rewards, it requires a very large 
number of interactions. These learn
ing methods tend to produce task-spe
cific, specialized systems that are often 
brittle outside of the narrow domain 
they have been trained on. Reducing 
the number of human-labeled samples 
or interactions with the world that are 
required to learn a task and increasing 
the out-of-domain robustness is of cru
cial importance for applications such 
as low-resource language translation, 
medical image analysis, autonomous 
driving, and content filtering.
 Humans and animals seem to be 
able to learn massive amounts of back
ground knowledge about the world, 
largely by observation, in a task-inde
pendent manner. This knowledge un
derpins common sense and allows hu
mans to learn complex tasks, such as 
driving, with just a few hours of prac
tice. A key question for the future of AI 
is how do humans learn so much from 
observation alone?
In supervised learning, a label for 
one of N categories conveys, on aver
age, at most log2(N) bits of information 
about the world. In model-free rein
forcement learning, a reward similarly 
conveys only a few bits of information. 
In contrast, audio, images and video 
are high-bandwidth modalities that 
implicitly convey large amounts of in
formation about the structure of the 
world. This motivates a form of predic
tion or reconstruction called self-su
pervised learning which is training to 
“fill in the blanks” by predicting 
masked or corrupted portions of the 
data. Self-supervised learning has been 
very successful for training transform
ers to extract vectors that capture the 
context-dependent meaning of a word 
or word fragment and these vectors 
work very well for downstream tasks.
 For text, the transformer is trained 
to predict missing words from a dis
crete set of possibilities. But in high
dimensional continuous domains 
such as video, the set of plausible con
tinuations of a particular video seg
ment is large and complex and repre
senting the distribution of plausible 
continuations properly is essentially 
an unsolved problem.
 Contrastive learning. One way to ap
proach this problem is through latent 
variable models that assign an energy 
(that is, a badness) to examples of a 
video and a possible continuation.a
 Given an input video X and a pro
posed continuation Y , we want a mod
el to indicate whether Y is compatible 
with X by using an energy function 
E(X, Y) which takes low values when 
X and Y are compatible, and higher 
values otherwise.
 E(X, Y) can be computed by a deep 
neural net which, for a given X, is 
trained in a contrastive way to give a 
low energy to values Y that are compat
ible with X (such as examples of (X, Y) 
pairs from a training set), and high en
ergy to other values of Y that are incom
patible with X. For a given X, inference 
consists in finding one Yˇ that minimizes 
E(X, Y) or perhaps sampling from the Y s 
that have low values of E(X, Y). This en
ergy-based approach to representing 
the way Y depends on X makes it possi
ble to model a diverse, multi-modal set 
of plausible continuations.
 The key difficulty with contrastive 
learning is to pick good “negative” 
samples: suitable points Y whose ener
gy will be pushed up. When the set of 
possible negative examples is not too 
large, we can just consider them all. 
This is what a softmax does, so in this 
case contrastive learning reduces to 
standard supervised or self- supervised 
learning over a finite discrete set of 
symbols. But in a real-valued high-di
mensional space, there are far too 
many ways a vector Yˆ could be different 
from Y and to improve the model we 
need to focus on those Ys that should 
have high energy but currently have low 
energy. Early methods to pick negative 
samples were based on Monte-Carlo 
methods, such as contrastive divergence 
for restricted Boltzmann machines48 and 
noise-contrastive estimation.41
 Generative Adversarial Networks 
(GANs)36 train a generative neural net to 
produce contrastive samples by apply
a As Gibbs pointed out, if energies are defined 
so that they add for independent systems, they 
must correspond to negative log probabilities 
in any probabilistic interpretationIn supervised learning, a label for 
one of N categories conveys, on aver
age, at most log2(N) bits of information 
about the world. In model-free rein
forcement learning, a reward similarly 
conveys only a few bits of information. 
In contrast, audio, images and video 
are high-bandwidth modalities that 
implicitly convey large amounts of in
formation about the structure of the 
world. This motivates a form of predic
tion or reconstruction called self-su
pervised learning which is training to 
“fill in the blanks” by predicting 
masked or corrupted portions of the 
data. Self-supervised learning has been 
very successful for training transform
ers to extract vectors that capture the 
context-dependent meaning of a word 
or word fragment and these vectors 
work very well for downstream tasks.
 For text, the transformer is trained 
to predict missing words from a dis
crete set of possibilities. But in high
dimensional continuous domains 
such as video, the set of plausible con
tinuations of a particular video seg
ment is large and complex and repre
senting the distribution of plausible 
continuations properly is essentially 
an unsolved problem.
 Contrastive learning. One way to ap
proach this problem is through latent 
variable models that assign an energy 
(that is, a badness) to examples of a 
video and a possible continuation.a
 Given an input video X and a pro
posed continuation Y , we want a mod
el to indicate whether Y is compatible 
with X by using an energy function 
E(X, Y) which takes low values when 
X and Y are compatible, and higher 
values otherwise.
 E(X, Y) can be computed by a deep 
neural net which, for a given X, is 
trained in a contrastive way to give a 
low energy to values Y that are compat
ible with X (such as examples of (X, Y) 
pairs from a training set), and high en
ergy to other values of Y that are incom
patible with X. For a given X, inference 
consists in finding one Yˇ that minimizes 
E(X, Y) or perhaps sampling from the Y s 
that have low values of E(X, Y). This en
ergy-based approach to representing 
the way Y depends on X makes it possi
ble to model a diverse, multi-modal set 
of plausible continuations.
 The key difficulty with contrastive 
learning is to pick good “negative” 
samples: suitable points Y whose ener
gy will be pushed up. When the set of 
possible negative examples is not too 
large, we can just consider them all. 
This is what a softmax does, so in this 
case contrastive learning reduces to 
standard supervised or self- supervised 
learning over a finite discrete set of 
symbols. But in a real-valued high-di
mensional space, there are far too 
many ways a vector Yˆ could be different 
from Y and to improve the model we 
need to focus on those Ys that should 
have high energy but currently have low 
energy. Early methods to pick negative 
samples were based on Monte-Carlo 
methods, such as contrastive divergence 
for restricted Boltzmann machines48 and 
noise-contrastive estimation.41
 Generative Adversarial Networks 
(GANs)36 train a generative neural net to 
produce contrastive samples by apply ing a neural network to latent samples 
from a known distribution (for exam
ple, a Gaussian). The generator trains 
itself to produce outputs Yˆ  to which the 
model gives low energy E(Yˆ). The gen
erator can do so using backpropagation 
to get the gradient of E(Yˆ) with respect 
to Yˆ. The generator and the model are 
trained simultaneously, with the model 
attempting to give low energy to train
ing samples, and high energy to gener
ated contrastive samples.
 GANs are somewhat tricky to opti
mize, but adversarial training ideas 
have proved extremely fertile, produc
ing impressive results in image synthe
sis, and opening up many new applica
tions in content creation and domain 
adaptation34 as well as domain or style 
transfer.87
 Making representations agree using 
contrastive learning. Contrastive learn
ing provides a way to discover good fea
ture vectors without having to recon
struct or generate pixels. The idea is to 
learn a feed-forward neural network that 
produces very similar output vectors 
when given two different crops of the 
same image10 or two different views of 
the same object17 but dissimilar output 
vectors for crops from different images 
or views of different objects. The squared 
distance between the two output vectors 
can be treated as an energy, which is 
pushed down for compatible pairs and 
pushed up for incompatible pairs.24,80
 A series of recent papers that use 
convolutional nets for extracting repre
sentations that agree have produced 
promising results in visual feature 
learning. The positive pairs are com
posed of different versions of the same 
image that are distorted through crop
ping, scaling, rotation, color shift, blur
ring, and so on. The negative pairs are 
similarly distorted versions of different 
images which may be cleverly picked 
from the dataset through a process 
called hard negative mining or may 
simply be all of the distorted versions of 
other images in a minibatch. The hid
den activity vector of one of the higher
level layers of the network is subse
quently used as input to a linear 
classifier trained in a supervised man
ner. This Siamese net approach has 
yielded excellent results on standard 
image recognition bench
marks.6,21,22,43,67 Very recently, two Sia
mese net approaches have managed to 
eschew the need for contrastive sam
ples. The first one, dubbed SwAV, quan
tizes the output of one network to train 
the other network,20 the second one, 
dubbed BYOL, smoothes the weight 
trajectory of one of the two networks, 
which is apparently enough to prevent a 
collapse.40
 Variational auto-encoders. A popu
lar recent self-supervised learning 
method is the Variational Auto-Encoder 
(VAE).58 This consists of an encoder 
network that maps the image into a la
tent code space and a decoder network 
that generates an image from a latent 
code. The VAE limits the information 
capacity of the latent code by adding 
Gaussian noise to the output of the 
encoder before it is passed to the de
coder. This is akin to packing small 
noisy spheres into a larger sphere of 
minimum radius. The information ca
pacity is limited by how many noisy 
spheres fit inside the containing 
sphere. The noisy spheres repel each 
other because a good reconstruction 
error requires a small overlap between 
codes that correspond to different 
samples. Mathematically, the system 
minimizes a free energy obtained 
through marginalization of the latent 
code over the noise distribution. How
ever, minimizing this free energy with 
respect to the parameters is intracta
ble, and one has to rely on variational 
approximation methods from statisti
cal physics that minimize an upper 
bound of the free energy.
 The Future of Deep Learning
 The performance of deep learning sys
tems can often be dramatically im
proved by simply scaling them up. 
With a lot more data and a lot more 
computation, they generally work a lot 
better. The language model GPT-318 
with 175 billion parameters (which is 
still tiny compared with the number of 
synapses in the human brain) gener
ates noticeably better text than GPT-2 
with only 1.5 billion parameters. The 
chatbots Meena2 and BlenderBot73 also 
keep improving as they get bigger. 
Enormous effort is now going into scal
ing up and it will improve existing sys
tems a lot, but there are fundamental 
deficiencies of current deep learning 
that cannot be overcome by scaling 
alone, as discussed here.
 Comparing human learning abili
ties with current AI suggests several di
rections for improvement: 
1. Supervised learning requires too 
much labeled data and model-free rein
forcement learning requires far too 
many trials. Humans seem to be able to 
generalize well with far less experience.
 2. Current systems are not as robust 
to changes in distribution as humans, 
who can quickly adapt to such changes 
with very few examples.
 3. Current deep learning is most 
successful at perception tasks and gen
erally what are called system 1 tasks. 
Using deep learning for system 2 tasks 
that require a deliberate sequence of 
steps is an exciting area that is still in 
its infancy.
 What needs to be improved. From 
the early days, theoreticians of ma
chine learning have focused on the iid 
assumption, which states that the test 
cases are expected to come from the 
same distribution as the training ex
amples. Unfortunately, this is not a re
alistic assumption in the real world: 
just consider the non-stationarities 
due to actions of various agents chang
ing the world, or the gradually expand
ing mental horizon of a learning agent 
which always has more to learn and 
discover. As a practical consequence, 
the performance of today’s best AI sys
tems tends to take a hit when they go 
from the lab to the field.
 Our desire to achieve greater robust
ness when confronted with changes in 
distribution (called out-of-distribution 
generalization) is a special case of the 
more general objective of reducing 
sample complexity (the number of ex
amples needed to generalize well) when 
faced with a new task—as in transfer 
learning and lifelong learning or 
simply with a change in distribution or 
in the relationship between states of 
the world and rewards. Current super
vised learning systems require many 
more examples than humans (when 
having to learn a new task) and the situ
ation is even worse for model-free rein
forcement learning23 since each re
warded trial provides less information 
about the task than each labeled exam
ple. It has already been noted that 
humans can generalize in a way that is 
different and more powerful than ordi
nary iid generalization: we can correctly 
interpret novel combinations of exist
ing concepts, even if those combinations are extremely unlikely under our 
training distribution, so long as they 
respect high-level syntactic and se
mantic patterns we have already 
learned. Recent studies help us clarify 
how different neural net architectures 
fare in terms of this systematic general
ization ability. How can we design fu
ture machine learning systems with 
these abilities to generalize better or 
adapt faster out-of-distribution?
As Gibbs pointed out, if energies are defined 
so that they add for independent systems, they 
must correspond to negative log probabilities 
in any probabilistic interpretation.
 From homogeneous layers to groups 
of neurons that represent entities. Evi
dence from neuroscience suggests that 
groups of nearby neurons (forming what 
is called a hyper-column) are tightly con
nected and might represent a kind of 
higher-level vector-valued unit able to 
send not just a scalar quantity but rather 
a set of coordinated values. This idea is 
at the heart of the capsules architec
tures,47,59 and it is also inherent in the 
use of soft-attention mechanisms, 
where each element in the set is associ
ated with a vector, from which one can 
read a key vector and a value vector (and 
sometimes also a query vector). One way 
to think about these vector-level units is 
as representing the detection of an ob
ject along with its attributes (like pose 
information, in capsules). Recent pa
pers in computer vision are exploring 
extensions of convolutional neural net
works in which the top level of the hier
archy represents a set of candidate ob
jects detected in the input image, and 
operations on these candidates is per
formed with transformer-like architec
tures.19,84,86 Neural networks that assign 
intrinsic frames of reference to objects 
and their parts and recognize objects by 
using the geometric relationships be
tween parts should be far less vulnerable 
to directed adversarial attacks,79 which 
rely on the large difference between the 
information used by people and that 
used by neural nets to recognize objects.
 Multiple time scales of adaption. 
Most neural nets only have two times
cales: the weights adapt slowly over 
many examples and the activities adapt 
rapidly changing with each new input. 
Adding an overlay of rapidly adapting 
and rapidly, decaying “fast weights”49 
introduces interesting new computa
tional abilities. In particular, it creates 
a high-capacity, short-term memory,4 
which allows a neural net to perform 
true recursion in which the same neu
rons can be reused in a recursive call 
because their activity vector in the 
higher-level call can be reconstructed 
later using the information in the fast 
weights. Multiple time scales of adap
tion also arise in learning to learn, or 
meta-learning.12,33,75
 Higher-level cognition. When think
ing about a new challenge, such as driv
ing in a city with unusual traffic rules, 
or even imagining driving a vehicle on 
the moon, we can take advantage of 
pieces of knowledge and generic skills 
we have already mastered and recom
bine them dynamically in new ways. 
This form of systematic generalization 
allows humans to generalize fairly well 
in contexts that are very unlikely under 
their training distribution. We can 
then further improve with practice, 
f
 ine-tuning and compiling these new 
skills so they do not need conscious at
tention anymore. How could we endow 
neural networks with the ability to 
adapt quickly to new settings by mostly 
reusing already known pieces of knowl
edge, thus avoiding interference with 
known skills? Initial steps in that direc
tion include Transformers32 and Recur
rent Independent Mechanisms.38
 It seems that our implicit (system 1) 
processing abilities allow us to guess 
potentially good or dangerous futures, 
when planning or reasoning. This rais
es the question of how system 1 net
works could guide search and plan
ning at the higher (system 2) level, 
maybe in the spirit of the value func
tions which guide Monte-Carlo tree 
search for AlphaGo.77
 Machine learning research relies on 
inductive biases or priors in order to en
courage learning in directions which 
are compatible with some assumptions 
about the world. The nature of system 2 
processing and cognitive neuroscience 
theories for them5,30 suggests several 
such inductive biases and architec
tures,11,45 which may be exploited to de
sign novel deep learning systems. How 
do we design deep learning architec
tures and training frameworks which 
incorporate such inductive biases?
 The ability of young children to per
form causal discovery37 suggests this 
may be a basic property of the human 
brain, and recent work suggests that 
optimizing out-of-distribution gener
alization under interventional changes 
can be used to train neural networks to 
discover causal dependencies or causal 
variables.3,13,57,66 How should we struc
ture and train neural nets so they can 
capture these underlying causal prop
erties of the world?
 How are the directions suggested by 
these open questions related to the 
symbolic AI research program from the 
20th century? Clearly, this symbolic AI 
program aimed at achieving system 2 
abilities, such as reasoning, being able 
to factorize knowledge into pieces 
which can easily recombined in a se
quence of computational steps, and 
being able to manipulate abstract vari
ables, types, and instances. We would 
like to design neural networks which 
can do all these things while working 
with real-valued vectors so as to pre
serve the strengths of deep learning 
which include efficient large-scale 
learning using differentiable computa
tion and gradient-based adaptation, 
grounding of high-level concepts in 
low-level perception and action, han
dling uncertain data, and using distrib
uted representations. 
AS ANNE GREEN WALKED to the gallows in the castle yard of Oxford, England, 
in 1650, she must have bee n fee ling scared, angry, and fr ustrated. She was about to be 
executed for a crime she had not committ ed: murdering her stillborn child. 
Many thoughts raced through her head, but “I am about to play a role in 
the founding of clinical neurology and neuroanatomy” although accurate, 
certainly was not one of them. She proclaimed her innocence to the crowd, 
a psalm was read, and she was hanged. She hung there for a full half hour 
before she was taken down, pronounced dead, and placed in a coffi  n provided 
by Drs. Th omas  Willis and William Petty  . Th is was when Anne Gree n’s luck 
began to improve.  Willis and Petty  were physicians and had permission fr om 
King Charles I to dissect, for medical research, the bodies of any criminals 
killed within 21 miles of  Oxford. So, instead of being buried, Anne’s body was 
carried to their offi  ce. 
An autopsy, however, was not what took place. As if in a scene fr om 
Edgar Allan Poe, the coffi  n began to emit a grumbling sound. Anne was alive! 
The doctors poured spirits in her mouth and rubbed a feather on her neck to make her 
cough. Th ey rubbed her hands and fee t for several minutes, bled fi ve ounces  of her blood, 
swabbed her neck wounds with turpentine, and cared for her through the night. Th e next 
morning, able to drink fl uids and fee ling more chipper, Anne asked for a bee r. Five days 
later, she was out of bed and eating normally (Molnar, 2004; Zimmer, 2004). 
 Aft er her ordeal, the authorities wanted to hang Anne again. But Willis and Petty
 fought in her defense, arguing that her baby had bee n stillborn and its death was not her 
fault. Th ey declared that divine providence had stepped in and provided her miraculous 
escape fr om death, thus proving her innocence. Th eir arguments prevailed. Anne was set 
free  and went on to marry and have three  more children. 
 This miraculous experience was well publicized in England (Figure 1.1). Th omas  Willis 
(Figure 1.2) owed much to Anne Gree n and the fame brought to him by the events of 
her resurrection. With it came money he desperately nee ded and the prestige to publish 
his work and disseminate his ideas, and he had some good ones. An inquisitive neurolo
gist, he actually coined the term  neurology  and became one of the best-known doctors 
of his time. He was the fi rst anatomist to link abnormal human behaviors to changes 
in brain structure. He drew these conclusions aft er treating patients throughout their lives and autopsying them aft er their deaths.  Willis was 
among the fi rst to link specifi c brain damage to specifi c 
behavioral defi cits, and to theorize how the brain trans
fers information in what would later be called   neuronal 
conduction. 
With his colleague and fr iend Christopher Wren (the 
architect who designed St. Paul’s Cathedral in London), 
Willis created drawings of the human brain that re
mained the most accurate representations for 200 years. He also coined names for a myriad of brain 
regions. In 
short, Willis set in motion the ideas and knowledge base 
that took hundreds of years to develop into what we 
know today as the fi eld of  cognitive neuroscience . 
In this chapter, we discuss some of the scientists and 
physicians who have made important contributions to 
this fi eld. You will discover the origins of cognitive neu
roscience and how it has developed into what it is today: 
a discipline geared toward understanding how the brain 
works, how brain structure and function aff ect behavior, 
and ultimately how the brain enables the mind. 
A Historical Perspective 
 Th e scientifi c fi eld of  cognitive neuroscience  received 
its name in the late 1970s in the back seat of a New York 
City  taxi. One of us (M.S.G.) was riding with the great 
cognitive psychologist George A. Miller on the way to a 
dinner mee ting at the Algonquin Hotel. Th e dinner was 
being held for scientists fr om Rockefeller and Cornell 
universities, who were joining forces to study how the 
brain enables the mind—a subject in nee d of a name. Out 
of that taxi ride came the term  cognitive neuroscience — 
from  cognition , or the process of knowing (i.e., what 
arises fr om awareness, per
ception, and  reasoning), 
and   neuroscience  (the study 
of how the nervous system is organized and functions). Th is see med the 
perfect term to describe the 
question of understanding 
how the functions of the 
physical brain can yield the 
thoughts and ideas of an 
intangible mind. And so the 
term took hold in the scientific  community. 
 When considering the 
miraculous properties of 
brain function, bear in mind that Mother Nature built 
our brains through the process of evolution; they were not 
designed by a team of rational enginee rs. While life fi rst 
appeared on our 4.5-billion-year-old Earth  approximately 
3.8 billion years ago, human brains, in their present 
form, have bee n around for only about 100,000 years, 
a mere drop in the bucket. Th e primate brain appeared 
betw ee n 34 million and 23 million years ago, during the 
Oligocene epoch. It evolved into the progressively larger 
brains of the great apes in the Miocene epoch betw ee n 
roughly 23 million and 7 million years ago. 
The Brain Story 
Imagine that you are given a problem to solve. A hunk 
of biological tissue is known to think, remember, att end, 
solve problems, tell jokes, want sex, join clubs, write novels, exhibit bias, fee l guilty , and do a zillion other things. 
You are supposed to fi gure out how it works. You might 
start by looking at the big picture and asking yourself a 
couple of questions. “Hmmm, does the blob work as a 
unit with each part contributing to a whole? Or, is the 
blob full of individual processing parts, each carrying out 
specifi c functions, so the result is something that looks 
like it is acting as a whole unit?” From a distance the city  
of New York (another ty pe of blob) appears as an inte
grated whole, but it is actually composed of  millions of 
individual processors—that is, people. Perhaps people, in 
turn, are made of smaller, more specialized units. 
 This central issue—whether the mind is enabled by the 
whole brain working in concert or by specialized parts of 
the brain working at least partly independently— is what 
fuels much of modern research in cognitive neuroscience. 
 As we will see , the dominant 
view has changed back and 
forth over the years, and it 
continues to change today. 
 Th omas Willis foreshad
owed cognitive neuroscience 
with the notion that isolated 
brain damage ( biology) could 
aff ect behavior (psychology), 
but his insights slipped fr om 
view. It took another century 
for Willis’s ideas to resurface. 
They were expanded upon by 
a young Austrian physician 
and neuroanatomist, Franz 
Joseph Gall ( Figure 1.4).  After studying numerous patients, 
Gall became convinced that the brain was the organ of the 
mind and that innate faculties were localized in specifi c 
regions of the cerebral cortex. He thought that the brain 
was  organized around some 35 or more specific functions, 
ranging fr om cognitive basics such as language and color 
perception to more ephemeral capacities such as aff ection 
and a moral sense, and each was supported by specifi c brain 
regions. These ideas were well received, and Gall took his 
theory on the road, lecturing throughout Europe. 
Building on his theories, Gall and his disciple Johann 
Spurzheim hypothesized that if a person used one of the 
faculties with greater frequency than the others, the part 
of the brain representing that function would grow (Gall 
& Spurzheim, 1810–1819). This increase in local brain size would cause a bump in the overlying skull. Logical
ly, then, Gall and his colleagues believed that a careful 
analysis of the skull could go a long way in describing the 
personality  of the person inside the skull. Gall called this 
technique  anatomical personology  (Figure 1.5). Th e idea 
that character could be divined through palpating the 
skull was dubbed  phrenology  by Spurzheim and, as you 
may well imagine, soon fell into the hands of charlatans. 
Some employers even required job applicants to have 
their skulls “read” before they were hired. 
Gall, apparently, was not politically astute. When 
asked to read the skull of Napoleon Bonaparte, Gall 
did not ascribe to his skull the noble characteristics 
that the future emperor was quite sure he possessed. 
When Gall later applied to the Academy of Science of 
Paris,  Napoleon decided that phrenology nee ded closer 
scrutiny and ordered the Academy to obtain some sci
entifi c evidence of its validity . Although Gall was a 
physician and neuroanatomist, he was not a scientist. 
He observed  correlations and sought only to confi rm, 
not disprove, them. Th e Academy asked physiologist 
Marie-Jean-Pierre  Flourens (Figure 1.6) to see  if he could 
come up with any concrete fi ndings that could back up 
this theory. 
Flourens set to work. He destroyed parts of the brains 
of pigeons and rabbits and observed what happened. He 
was the fi rst to show that indee d certain parts of the brain 
were responsible for certain functions. For instance, 
when he removed the cerebral hemispheres, the animal 
no longer had perception, motor ability , and judgment.
Without the cerebellum, the animals became uncoordi
nated and lost their equilibrium. He could not, however, 
find any areas for advanced abilities such as memory or 
cognition and concluded that these were more diff usely 
scatt ered throughout the brain. Flourens developed the 
notion that the whole brain participated in behavior, a 
view later known as the  aggregate field theory . In 1824, 
Flourens wrote, “All sensations, all perceptions, and all 
volitions occupy the same seat in these (cerebral) organs. 
The faculty  of sensation, percept and volition is then 
essentially one faculty .” The theory of localized brain 
functions, known as localizationism, fell out of favor. 
 That state of aff airs didn’t last for too long, however. 
New evidence obtained through clinical observations 
and autopsies started trickling in fr om across  Europe, 
and it helped to swing the pendulum slowly back to the 
localizationist view. In 1836 a neurologist fr om Montpel
lier, Marc Dax, provided one of the fi rst bits of  evidence. 
He sent a report to the Academy of Sciences about three  
patients, noting that each 
had spee ch disturbances 
and similar left -hemisphere 
lesions found at autopsy. At 
the time, a report fr om the 
provinces got short shrift  
in Paris, and it would be 
another 30 years before 
anyone took much notice 
of this observation that 
spee ch could be disrupted 
by a lesion to one hemi
sphere only. 
Although Jackson was also the fi rst to observe that 
lesions on the right side of the brain aff ect visuospatial 
processes more than do lesions on the left  side, he did not 
maintain that specifi c parts of the right side of the brain 
were solely committ ed to this important human cognitive 
function. Being an observant clinical neurologist, Jackson 
noticed that it was rare for a patient to lose a function 
completely. For example, most people who lost their 
 capacity  to speak following a cerebral stroke could still 
say some words. Patients unable to direct their hands vol
untarily to specifi c places on their bodies could still easily 
scratch those places if they itched. When Jackson made 
these observations, he concluded that many regions of 
the brain contributed to a given behavior. 
Meanwhile, the well-known and respected Parisian 
physician Paul Broca (Figure 1.8a) published, in 1861, 
the results of his autopsy on a patient who had bee n nick
named Tan—perhaps the most famous neurological case 
in history. Tan had developed aphasia: He could under
stand language, but “tan” was the only word he could 
 utt er. Broca found that Tan (his real name was  Leborgne) 
had a syphilitic lesion in his left  hemisphere in the inferior 
frontal lobe. This region of the brain has come to be called 
Broca’s area. The impact of this fi nding was huge. Here 
was a specific aspect of language that was impaired by a 
specifi c lesion. Soon Broca had a series of such patients. 
This theme was picked up by the German neurologist Carl 
Wernicke. In 1876, Wernicke reported on a stroke victim 
who (unlike Broca’s patient) could talk quite fr ee ly but 
made litt le sense when he spoke.  Wernicke’s patient also 
could not understand spoken or writt en language. He had 
a lesion in a more posterior region of the left  hemisphere, 
an area in and around where the temporal and  parietal 
lobes mee t, which is now referred to as Wernicke’s area.
 Today, diff erences in how the brain responds to focal 
disease are well known (H. Damasio et al., 2004; R. J. 
Wise, 2003), but a litt le over 100 years ago Broca’s and 
Wernicke’s discoveries were earth-shatt ering. (Note that 
people had largely forgott en Willis’s observations that iso
lated brain damage could aff ect behavior. Th roughout the 
history of brain science, an unfortunate and oft  repeated 
trend is that we fail to consider crucial observations made 
by our predecessors.) With the discoveries of Broca and 
Wernicke, att ention was again paid to this startling point: 
Focal brain damage causes specifi c  behavioral defi cits.  
As is so oft en the case, the study of humans leads to 
questions for those who work on animal models. Shortly 
aft er Broca’s discovery, the German physiologists  Gustav 
Fritsch and Eduard Hitzig electrically stimulated discrete 
parts of a dog brain and observed that this stimulation 
produced characteristic movements in the dog. Th is dis
covery led neuroanatomists to more closely analyze the 
cerebral cortex and its cellular organization; they  wanted 
support for their ideas about the importance of local. 
Following this logic, German neuroanatomists began 
to analyze the brain by using microscopic methods to 
view the cell ty pes in diff erent brain regions. Perhaps the 
most famous of the group was Korbinian Brodmann, who 
analyzed the cellular organization of the cortex and char
acterized 52 distinct regions (Figure 1.9). He published 
his cortical maps in 1909. Brodmann used tissue stains, 
such as the one developed by Franz Nissl, that permitt ed 
him to visualize the diff erent cell ty pes in diff erent brain 
regions. How cells diff er betw een brain regions is called 
cytoarchitectonics, or cellular architecture.
The Psychological Story 
Physicians were the early pionee rs studying how the brain 
worked. In 1869 a Dutch ophthalmologist,  Franciscus 
Donders, was the fi rst to propose the now- common 
method of using diff erences in reaction times to infer dif
ferences in cognitive processing. He suggested that the 
diff erence in the amount of time it took to react to a light 
and the amount of time nee ded to react to a particular 
color of light was the amount of time required for the 
process of identify ing a color. Psychologists began to use  
this approach, claiming that they could study the mind 
by measuring behavior, and experimental  psychology 
was born. 
Before the start of experimental psychological sci
ence the mind had bee n the province of philosophers, 
who wondered about the nature of knowledge and how 
we come to know things. Th e philosophers had tw o 
main positions:  rationalism  and  empiricism . Rational
ism grew out of the Enlightenment period and held that 
all knowledge could be gained through the use of rea
son alone: Truth was intellectual, not sensory. Th rough 
thinking, then, rationalists would determine true beliefs 
and would reject beliefs that, although perhaps com
forting, were unsupportable and even superstitious. 
Among intellectuals and scientists, rationalism replaced 
religion and became the only way to think about the 
world. In particular, this view, in one form or another, was supported by René Descartes, Baruch Spinoza, and 
 Gott fr ied Leibniz. 
Although rationalism is fr equently equated with logi
cal thinking, the tw o are not identical. Rationalism con
siders such issues as the meaning of life, whereas logic 
does not. Logic simply relies on inductive reasoning, 
statistics, probabilities, and the like. It does not con
cern itself with personal mental states like happiness, 
self- interest, and public good. Each person weighs these 
 issues diff erently, and as a consequence, a rational deci
sion is more problematic than a simple logical decision. 
Empiricism, on the other hand, is the idea that all 
knowledge comes fr om sensory experience, that the 
brain began life as a blank slate. Direct sensory experi
ence produces simple ideas and concepts. When simple 
ideas  interact and become  as sociated  with one another, 
complex ideas and concepts are created in an individual’s 
knowledge system. The British philosophers—from T
 h omas Hobbes in the 
17th century, through John 
Locke and  David Hume, to 
John Stuart Mill in the 19th 
 century—all 
emphasized 
the role of experience. It 
is no surprise, then, that a 
major school of experimen
tal psychology arose fr om 
this associationist view. 
Psychological association
ists believed that the aggre
gate of a person’s experi
ence determined the course 
of mental  development. 
One of the fi rst scientists to study  associationism  was 
Hermann Ebbinghaus, who, in the late 1800s, decided 
that complex processes like memory could be measured 
and analyzed. He took his lead fr om the great psycho
physicists Gustav Fechner and Ernst Heinrich Weber, 
who were hard at work relating the physical properties 
of things such as light and sound to the psychological ex
periences that they produce in the observer. Th ese mea
surements were rigorous and reproducible. Ebbinghaus 
was one of the fi rst to understand that mental processes 
that are more internal, such as memory, also could be 
 measured (see  Chapter 9). 
Even more infl uential to the shaping of the associa
tionist view was the classic 1911 monograph  Animal Intel
ligence: An Experimental Study of the Associative Processes 
in  Animals , by Edward Th orndike ( Figure 1.15). In this 
volume,  Th orndike articulated his law of eff ect, which was 
the first general statement about the nature of associa
tions. Th orndike simply observed that a  response that was 
followed by a  reward would be stamped into the organism 
as a habitual  response. If no reward followed a response, 
the response would disappear. Th us, rewards provided a 
mechanism for establishing a more adaptive response. 
Associationism came to be dominated by American 
behavioral psychologist John B. Watson (Figure 1.16), 
who proposed that psychology could be objective only 
if it were based on observable behavior. He rejected 
Ebbinghaus’s methods and declared that all talk of men
tal processes, which cannot be publicly observed, should 
be avoided. Associationism became committ ed to an idea 
widely popularized by Watson that he could turn any 
baby into anything. Learning was the key, he proclaimed, 
and everybody had the same neural equipment on which 
learning could build. Appealing to the American sense of 
equality , American psychology was giddy with this idea 
of the brain as a  blank slate  upon which to build through 
learning and experience, and every prominent psychol
ogy department in the country was run by people who 
held this view. 
Behaviorist–associationist psychology went on de
spite the already well-established position—fi rst articu
lated by Descartes, Leibniz, Kant, and others—that 
complexity  is built into the human organism. Sensory 
information is merely data on which pree xisting mental 
structures act. Th is idea, which dominates psychology 
today, was blithely asserted in that golden age, and later 
forgotten or ignored. 
Although American psychologists were focused on 
behaviorism, the psychologists in Britain and Canada were 
not. Montreal became a hot spot for new ideas on how 
biology shapes cognition and behavior.
 ONE DAY IN 1963, neuroscientist Jose Delgado coolly stood in a bullring in  Cordoba, Spain, 
facing a charging bull. He did not sport the Spanish matador’s ty pical gear of  toreador pants, 
jacket, and sword, however. No theoretical scientist he,  Delgado stepped into 
the ring in slacks and a pullover sweater while holding a small device in his hand 
(and a cape, for good eff ect). He was about to see  if it worked. As the bull came 
charging toward him, Delgado stood his ground, trigger fi nger itchy on the de
vice’s butt on. And then he calmly pushed it. Th e bull slammed on the brakes and 
skidded to a stop, standing a few fee t  before the scientist (Figure 2.1). Th e bull 
placidly looked at the smiling Delgado. See mingly, this was no ordinary bull; but 
yet it was. One odd thing about this bull, however, gave Delgado his confi dence: 
An electric stimulator had bee n surgically implanted in its caudate nucleus. Th e 
device in Delgado’s hand was a transmitt er he had built to activate the stimulator. 
By stimulating the bull’s caudate nucleus, Delgado had turned off  its aggression. 
Years before, Delgado had bee n horrifi ed by the increasingly popular  fr ontal 
lobotomy surgical procedure that destroyed brain tissue and function. He was 
interested in fi nding a more conservative approach to treating mental disorders 
through electrical stimulation. Using his knowledge of the electrical nature of neu
rons, neuroanatomy, and brain function, he designed his devices, the fi rst neural 
implants ever to be used. Excee dingly controversial at the time, his devices were 
the forerunners of the now common intracranial devices used for stimulating the 
brain to treat disorders like Parkinson’s disease, chronic pain, and other maladies. 
Delgado understood that our nervous system uses electrochemical energy 
for communication and that nerves can be thought of as glorifi ed electrical 
cables running to and fr om our brains. He also understood that inside our brains, neurons 
form an intricate  wiring patt ern: An electrical signal initiated at one location could travel 
to another location to trigger a muscle to contract or initiate a behavior, such as aggres
sion, to arise or cease. Delgado was banking on the hope that he had figured out the correct circuit involved in aggressive behavior. Delgado’s device was built with the knowledge 
that neurons use electrochemical signals to communicate. This knowledge is the founda
tion on which all theories of neuronal signaling are built. Thus, for us, it is  important to 
understand the  basic physiology of neurons and the  anatomy of the nervous system, which 
is what this chapter discusses.
The Structure of Neurons 
 The nervous system is composed of tw o main classes 
of cells: neurons and glial cells.  Neurons  are the basic 
signaling units that transmit information throughout the 
nervous system. As Ramón y Cajal and others of his time 
deduced, neurons take in information, make a “ decision” 
about it following some relatively simple rules, and then, 
by changes in their activity  levels, pass it along to other 
neurons. Neurons vary in their form, location, and inter
connectivity  within the nervous system (Figure 2.2), and 
these variations are closely related to their functions. 
Glial cells are nonneural cells that serve various func
tions in the nervous system, some of which are only now 
being elucidated. These include providing structural 
support and electrical insulation to neurons, and modulating neuronal activity. We begin with a look at neuronal 
structure and function, and then we return to glial cells. 
 The standard cellular components found in almost alleu
karyotic cells are found in neurons as well. A cell membrane 
encases the cell body (in neurons, it is sometimes called the soma ; Gree k for “body”), which contains the metabolic 
machinery that maintains the neuron: a nucleus, endoplas
mic reticulum, a cytoskeleton, mitochondria, Golgi appara
tus, and other common intracellular organelles (Figure 2.3). 
These structures are suspended in cytoplasm, the salty  
 intracellular fl uid that is made up of a combination of ions, 
predominantly ions of potassium, sodium, chloride, and calcium, as well as molecules such as proteins. Th e neuron, 
like any other cell, sits in a bath of salty extracellular fl uid, 
which is also made up of a mixture of the same types of ions. 
Axon terminals
 Neurons, unlike other cells, possess unique cytological 
features and physiological properties that enable them to 
transmit and process information  rapidly. The two predomi
nant cellular components unique to neurons are the dendrites 
and axon. Dendrites are branching extensions of the neuron 
that receive inputs from other neurons. They take many varied and complex forms,  depending on the type and location 
of the neuron. The arborizations may look like the branches 
and twigs of an old oak tree, as seen in the complex dendritic structures of the cerebellar Purkinje cells, or they may be much simpler, such as the  dendrites in spinal motor neurons. Many dendrites also have 
specialized processes called  spines , litt le knobs att ached 
by small necks to the surface of the dendrites, where the 
dendrites receive inputs from other neurons. 
 The axon is a single process that extends fr om the
cell body. This structure represents the output side of 
the neuron. Electrical signals travel along the length 
of the axon to its end, the axon terminals, where the neuron transmits the signal to other neurons or other 
cell ty pes.  Transmission occurs at the  synapse,  a spe
cialized structure where tw o neurons come into close 
contact so that chemical or electrical signals can be 
passed fr om one cell to the next. Some axons branch 
to form  axon collaterals  that can transmit signals 
to more than one cell. Many axons are 
wrapped in layers of a fatty substance called  myelin. 
Along the length of the axons, there are evenly spaced 
gaps in the myelin. These gaps are commonly  referred 
to as the nodes of Ranvier, named after the French histologist and anatomist Louis-Antoine 
Ranvier, who first described them. Later, when we look 
at how signals move down an axon, we will  explore the 
role of myelin and the nodes of  Ranvier in accelerating 
signal transmission.
 Neuronal Signaling 
Neurons receive, evaluate, and transmit information. Th is 
process is referred to as  neuronal signaling .  Information is 
transferred across synapses fr om one neuron to the next, 
or fr om a neuron to a non-neuronal cell such as those in 
muscles or glands. It is also conveyed within a neuron, 
being received at synapses on dendrites, conducted with
in the neuron, transmitt ed down the axon, and passed 
along at synapses on the axon terminals. Th ese tw o ty pes 
of transport, within and betw een neurons, are ty pically 
handled in diff erent ways. Within a neuron, transferring 
information involves changes in the electrical state of the 
neuron as electrical currents fl ow through the volume 
of the  neuron. Betw een neurons, information transfer 
occurs at synapses, ty pically mediated by chemical signal
ing molecules (neurotransmitt ers) but, in some cases, also 
by electrical signals. Regarding information fl ow, neurons 
are referred to as either presynaptic or postsynaptic in 
relation to any particular synapse.  Most neurons are both 
presynaptic and postsynaptic : Th ey are   presynaptic  when 
their axon makes a connection onto other neurons, and 
postsynaptic  when other  neurons make a connection 
onto their dendrites. 
The Membrane Potential 
 The process of signaling has several stages. Let’s return to 
Delgado’s bull, because his neurons process information 
in the same way ours do. Th e bull may have been snorting 
about in the dirt, his head down, when suddenly a sound 
wave—produced by Delgado entering the ring—courses 
down his auditory canal and hits his tympanic membrane 
(eardrum). The resultant stimulation of the auditory 
receptor cells (auditory hair cells) generates neural signals that are transmitted via the auditory pathways to the 
brain. At each stage of this ascending auditory pathway, 
neurons receive inputs on their dendrites that typically 
cause them to generate signals that are transmitt ed to the 
next neuron in the pathway. 
How does the neuron generate these signals, and what 
are these signals? To answer these questions, we have to 
understand several things about neurons. First, energy 
is needed to generate the signals; second, this energy is 
in the form of an electrical potential across the neuronal membrane. This electrical potential is defi ned as the 
diff erence in the voltage across the neuronal membrane, 
or put simply, the voltage inside the neuron versus out
side the neuron. Third, these tw o voltages depend on the 
concentrations of potassium, sodium, and chloride ions 
as well as on charged protein molecules both inside and 
outside of the cell. Fourth, when a neuron is not actively 
signaling—what we call its resting state—the inside of a 
neuron is more negatively charged than the outside. Th e 
voltage diff erence across the neuronal membrane in the  
resting state is typically  − 70 millivolts (mV) inside, which 
is known as the  resting potential or resting membrane 
potential. This electrical potential difference means that 
the neuron has at its disposal a kind of battery; and like 
a battery, the stored energy can be used to do work— 
signaling work. 
How does the neuron generate and maintain this 
resting potential, and how does it use it for signaling? To 
answer these questions about function, we first need to 
examine the structures in the neuron that are involved 
in signaling. The bulk of the  neuronal membrane is a 
bilayer of fatty lipid molecules that separates the cyto
plasm from the extracellular milieu. Because the mem
brane is composed of lipids, it does not dissolve in the watery  environments found inside and outside of the 
neuron. Th e lipid membrane blocks the fl ow of water
soluble substances betw een the inside and the outside 
of the neuron. It also prevents ions (molecules or atoms 
that have either a positive or negative electrical charge), 
proteins, and other water-soluble molecules fr om moving 
across it. To understand neuronal signaling, we must fo
cus on ions. Th is point is important: Th e lipid membrane 
maintains the separation of intracellular and  extracellular 
ions and electrical charge that ultimately permits neuronal communication. 
The neuronal membrane, though, is not merely a lipid 
bilayer. The membrane is peppered with transmembrane 
proteins that serve as conduits for ions to move across 
the neuronal membrane. There are two 
main types of these proteins: ion channels and ion pumps. 
Ion channels , as we shall see , are proteins with a pore 
through their centers, and they allow certain ions to fl ow 
down their concentration gradients. Ion pumps use energy to actively transport ions across the membrane against 
their concentration gradients, that is, from regions of low 
concentration to regions of higher concentration.
 Ion Channels    Th e transmembrane passageways cre ated 
by ion channels are formed fr om the  three -dimensional 
structure of these proteins. Th ese hydrophilic channels 
selectively permit one ty pe of ion to pass through the 
membrane. Th e ion channels of concern to us—the ones 
found in neurons—are selective for either sodium, potas
sium, calcium, or chloride ions (Na  +  , K  +  , Ca 2 +  , and Cl  −  , 
respectively; Figure 2.8, inset). Th e extent to which a 
particular ion can cross the membrane through a given 
ion channel is referred to as its  permeability  . Th is char
acteristic of ion channels gives the neuronal membrane 
the att ribute of  selective permeability  . (Selective perme
ability  is actually a property  of all cells in the body; as part 
of cellular homeostasis, it enables cells to maintain inter
nal chemical stability .) Th e neuronal membrane is more 
permeable to K  +   than to Na  +   (or other) ions, a property  
that contributes to the resting membrane potential, as we 
shall learn shortly. Th e membrane permeability  to K  +   is 
larger because there are many more K  +  -selective chan
nels than any other ty pe of ion channel. 
Unlike most cells in the body, neurons are excitable, 
meaning that they can change the permeability  of their 
membranes. Th is is brought about by ion channels that 
are capable of changing their permeability  for a particu
lar ion. Such proteins are called  gated ion channels . Th ey 
open or close based on changes in nearby transmembrane 
voltage, or as a response to chemical or physical stim
uli. In contrast, ion channels that are unregulated, and 
hence always allow the associated ion to pass through, 
are known as  nongated ion channels . 
Ion Pumps Under normal conditions, there are con
centration gradients of diff erent ions across the neuro
nal membrane. Specifi cally, Na  +   and Cl  −   concentrations 
are greater outside of the cell, and K  +   concentrations are 
greater inside the cell. Given that the neuronal membrane 
contains ion channels that permit the diff erent ions in
side and outside of the cell to fl ow across the  neuronal 
membrane, how does the neuron maintain  diff erent 
concentrations of ions inside compared with outside of the 
cell? Put another way, why don’t K  +   ions fl ow out of the 
neuron—down their concentration gradient— until the K  + 
ion concentrations inside and outside the cell are equal? 
We can ask the same questions for all other ions. To com
bat this drive toward equilibrium, neurons use   activetrans
port proteins, known as  ion pumps . In particular, neurons 
use a Na  +  /K  +   pump that pumps Na  +   ions out of the cell 
and K  +   ions into the cell. Because this process is transport
ing ions up their concentration  gradients, the mechanism 
requires energy. Each pump is an enzyme that hydrolyzes 
adenosine triphosphate (ATP). For each molecule of ATP 
that is hydrolyzed, the resulting energy is used to move 
three  Na  +   ions out of the cell and two K  +   ions into the cell.
Th e concentration gradients 
create forces—the forces of the  unequal distribution of 
ions. Th e force of the Na  +   concentration gradient wants to 
push Na  +   fr om an area of high  concentration to one of low 
concentration (fr om outside to inside), while the K  +   con
centration gradient acts to push K  +   fr om an area of high 
concentration to an area of low concentration (fr om inside 
to outside)—the very thing the pump is working against. 
Since there are both positively and negatively charged ions 
inside and outside the cell, why is there a diff erence in volt
age inside versus outside the neuron? 
 Th e inside and outside voltages are diff erent because 
the membrane is more permeable to K  +   than to Na  +  . Th e 
force of the K  +   concentration gradient pushes some K  +   out 
of the cell, leaving the inside of the neuron slightly more 
negative than the outside. Th is creates another force, an 
electrical gradient , because each K  +   ion carries one unit 
of positive charge out of the neuron as it moves across the 
membrane. Th ese tw o gradients (electrical and ionic con
centration) are in opposition to one another with respect 
to K  +   (Figure 2.10). As negative charge builds up along 
the inside of the membrane (and an equivalent positive 
charge forms along the extracellular side), the  positively 
charged K  +   ions outside of the cell are drawn electrically 
back into the neuron through the same ion channels that 
are allowing K  +   ions to leave the cell by diff usion. Eventu
ally, the force of the concentration gradient pushing K  +   out 
through the K  +   channels is equal to the force of the electri
cal gradient driving K  +   in. When that happens, the oppos
ing forces are said to reach   electrochemical equilibrium . Th e 
diff erence in charge thus produced across the membrane is 
the resting membrane potential, that  − 70 mV diff erence. 
The value for the  resting membrane potential of any cell 
can be calculated by using knowledge fr om electrochemis
try, provided that the concentrations of the ions inside and 
outside the neuron are known.
The Action Potential 
We now understand the basis of the energy source that 
neurons can use for signaling. Next we want to learn how 
this energy can be used to transmit information within a 
neuron, fr om its dendrites that receive inputs fr om other 
neurons, to its axon terminals where it makes synapses 
on the next neurons in the chain. Th e process begins 
when excitatory postsynaptic  potentials  (EPSPs) at synaps
es on the  neuron’s dendrites cause ionic currents to fl ow 
in the volume of the cell body. If these currents are strong 
enough to reach the axon terminals, then the processes 
of neuronal signaling could be completed. Unfortunately, 
in the vast majority  of cases, this distance is too great for 
the EPSP to have any eff ect. Why is this the case?
 transmit the signal to another cell 
(your toes would be in trouble, 
for example, because they are 
1 meter fr om the spinal cord and 
close to 2 meters fr om the brain). 
How does the neuron solve this 
problem of decremental conduc
tion and the nee d to conduct over 
long distances? 
Neurons evolved a clever 
mechanism to regenerate and 
pass along the signal initiated in 
the synapse. It works something 
like 19th-century fi refi ghters in 
a  
bucket  brigade, who handed 
buckets of water fr om one person 
to the next along a distance fr om 
the source of water to where it was 
nee ded at the fi re. Th is regenera
tive process is an active membrane 
mechanism known as the   action 
potential . An action potential is 
a rapid depolarization and repo
larization of a small region of the 
membrane caused by the opening 
and closing of ion channels. 
An action potential is an entirely diff erent animal 
f
 r om the EPSP. Unlike a postsynaptic potential, it doesn’t 
decrement aft er only 1 millimeter. Action potentials can 
travel for meters with no loss in signal strength, because 
they continuously regenerate the signal. Th is is one rea
son there can be giraff es and blue whales. It is, however, 
metabolically expensive, and it contributes to the inordi
nate amount of the body’s energy used by the brain. 
 Th e small electrical current produced by the EPSP is 
passively conducted through the cytoplasm of the den
drite, cell body, and axon. Passive current conduction is 
called  electrotonic conduction  or  decremental   conduction. 
Decremental, because it diminishes with  distance fr om its 
origin—the synapse, in this case. Th e maximum distance 
a passive current will fl ow is only about 1 millimeter. In 
most cases, a millimeter is too short to be eff ective for con
ducting electrical signals, but in a structure like the retina, 
a millimeter is enough to permit  neuron-to-neuron com
munication. Most of the time, however, the reduction in 
signal intensity  makes it unlikely that a single EPSP will 
be enough to trigger the fi ring of its own cell, much less 
 Th e action potential is able to regenerate itself due 
to the presence of  voltage-gated ion channels  located 
in the neuronal membrane (Figure 2.11a, inset). Th ese 
are found at the  spike-triggering zone  in the  axon 
hillock  and along the axon. In myelinated axons, these 
voltage-gated ion channels are confi ned to the axon hill
ock and the nodes of Ranvier (Figure 2.11a). As its name 
denotes, the spike-triggering zone initiates the  action 
potential. (Th e term  spike  is shorthand for an action 
potential, because when viewed as a recording displayed 
on an oscilloscope scree n, the action potential looks like 
a litt le spike in the recorded signal.) How does the  spike- 
triggering zone initiate an action potential? 
 Th e passive electrical currents that are generated fol
lowing EPSPs on multiple distant dendrites sum together 
at the axon hillock. Th is  current fl ows across the neuro
nal membrane in the spike- triggering zone, depolarizing 
the membrane. If the depolarization is strong enough, 
meaning the  membrane moves fr om its resting potential 
of about  − 70 mV to a less negative value of approximately − 55 mV, an  action  potential is triggered. We refer to this 
depolarized membrane potential value as the  threshold
 for initiating an action potential. Figure 2.11b  illustrates 
an idealized action potential. Th e numbered boxes in the 
f
 i gure correspond to the numbered events in the next 
paragraph. Each event alters a small region of the mem
brane’s permeability  for Na  +   and K  +   due to the opening 
and closing of voltage-gated ion channels. 
When the threshold (Figure 2.11 b, label 1) is reached, 
voltage-gated Na  +   channels open and Na  +   fl ows rapidly 
into the neuron. Th is infl ux of positive ions further depo
larizes the neuron, opening additional voltage-gated Na  + 
channels; thus, the neuron becomes more depolarized 
(2), continuing the cycle by causing even more Na  +   chan
nels to open. Th is process is called the Hodgkin–Huxley 
cycle. Th is rapid, self-reinforcing cycle, lasting only about 
1 millisecond, generates the large depolarization that is 
the fi rst portion of the action potential. Next, the voltage
gated K  +   channels open, allowing K  +   to fl ow out of the 
neuron down its concentration gradient. This outw ard 
flow of positive ions begins to shift  the membrane poten
tial back toward its resting potential (3). The opening of 
the K  +   channels outlasts the closing of the Na  +   channels, 
causing a  second repolarizing phase of the action poten
tial; and this drives the membrane potential toward the 
equilibrium potential of K  +  , which is even more negative 
than the resting potential. The equilibrium potential is 
the particular voltage at which there is no net fl ux of ions. 
As a result, (4) the membrane is temporarily hyperpolar
ized, meaning that the membrane potential is even farther 
from the threshold required for triggering an action poten
tial (e.g., around  − 80 mV). Hyperpolarization causes the 
K  +   channels to close, resulting in (5) the membrane po
tential gradually returning to its resting state. During this 
transient  hyperpolarization state, the  voltage-gated Na  + 
channels are unable to open, and another action potential 
cannot be generated. This is known as the absolute refrac
tory period. It is followed by the relative refractory period , 
during which the neuron can generate action potentials, 
but only with larger-than-normal depolarizing currents. 
The refractory period lasts only a couple of millisec
onds and has tw o consequences. One is that the neuron’s 
spee d for generating action potentials is limited to about 
200 action potentials per second. The other is that the 
passive current that flows from the action potential can
not reopen the ion-gated channels that generated it. The 
passive current, however, does flow down the axon with 
enough strength to depolarize the membrane a bit farther 
on, opening voltage-gated channels in this next portion of 
the membrane. The result is that the action potential is 
propagated down the axon in one direction only from 
the axon hillock toward the axon terminal. 
 So that is the story of the self-regenerating action 
potential as it propagates itself down an axon (sometimes 
traveling several meters). But traveling far is not the end 
of the story. Action potentials must also  travel quickly if 
a person wants to run, or a bull wants to charge, or a very 
large animal (think blue whale) simply wants to react in 
a reasonable amount of time. Accelerated transmission of 
the action potential is accomplished in  myelinated axons. 
The thick lipid sheath of myelin (Figure 2.11a)  surrounding 
the membrane of myelinated axons makes the axon super
resistant to voltage loss. Th e high electrical resistance al
lows passive currents generated by the action potential to 
be shunted farther down the axon. Th e result is that action 
potentials do not have to be generated as oft en, and they 
can be spread out along the axon at wider intervals. Indee d, 
action potentials in myelinated axons nee d occur only at the 
nodes of Ranvier, where myelination is interrupted. Th is 
creates the appearance that the action potential is jumping 
down the axon at great spee d, fr om one node of Ranvier 
to the next. We call this  saltatory conduction . (Saltatory 
conduction is derived fr om the Latin word saltare, to jump 
or leap.) Th e importance of myelin for effi  cient neuronal 
conduction is notable when it is lost, which is what happens 
when a person is affl  icted with multiple sclerosis (MS). 
 Th ere is one interesting tidbit left  concerning action 
potentials. Action potentials are always the same amplitude; 
therefore, they are said to be  all or none   phenomena. Since 
one action potential is the same amplitude as any  other, 
the strength of the action potential does not communicate 
anything about the strength of the stimulus. Th e  intensity  
of a stimulus (e.g., a sensory signal) is  communicated by 
the  rate of fi ring  of the action potentials: More intense 
stimuli elicit higher action potential fi ring rates. 
So, we see  how the neuron has solved the problem 
of long-distance communication as well as communi
cation spee d. When the action potential reaches the 
axon terminal, the signal is now strong enough to cause 
depolarization of the presynaptic membrane and to trig
ger neurotransmitt er release. Th e signal is ready to be 
transferred to the next neuron across the  synaptic cleft, 
the gap betw een neurons at the synapse. 
Neurotransmitters Th e process just described brings us 
to a hot topic of the popular press: 
neurotransmitters . While you may 
have heard of a few of them, more 
than 100 neurotransmitt ers have 
been identified. What makes a molecule a neurotransmitter?
It is synthesized by and localized 
within the presynaptic neuron, 
and stored in the presynaptic 
terminal before release. 
It is released by the presynaptic 
neuron when  action  potentials 
depolarize the terminal. 
■ Th e postsynaptic neuron 
contains receptors specific for 
the neurotransmitter. 
■ When artifi cially applied 
to a postsynaptic cell, the 
 neurotransmitt er elicits the same 
response that stimulating the 
presynaptic neuron would. 
Neural Coding of Movement 
Neurophysiologists have long puzzled over how best to 
describe cellular activity  in the motor structures of the 
CNS. Stimulation of the primary motor cortex, either 
during neurosurgery or via TMS, can produce discrete 
movements about single joints, providing a picture of the 
somatotopic organization of the motor cortex. Th is meth
od, however, does not provide insight into the activity  of 
single neurons, nor can it be used to study how and when 
cells become active during volitional movement. To ad
dress these issues, we have to record the activity  of  single 
cells and ask what parameters of movement are coded by 
such cellular activity . For example, is cellular activity  cor
related with parameters of muscle activity  such as force, 
or with more abstract entities such as movement direc
tion or desired fi nal location? 
In a classic series of experiments, Apostolos Geor
gopoulos (1995) and his colleagues studied this question 
by recording fr om cells in various motor areas of rhesus 
monkeys. Th e monkeys were trained with the apparatus 
shown in Figure 8.12 on what has come to be called the 
center-out tas k. Th e animal initiates the trial by moving 
the lever to the center of the table. Aft er a brief hold pe
riod, a light illuminates one of eight surrounding target 
positions, and the animal moves the lever to this position 
to obtain a food reward. Th is movement is similar to a 
reaching action and usually involves rotating tw o joints, 
the shoulder and the elbow. 
 Th e results of these studies convincingly demonstrate 
that the activity  of the cells in the primary motor cortex 
correlates much bett er with  movement direction  than with 
 Th e results of these studies convincingly demonstrate 
that the activity  of the cells in the primary motor cortex 
correlates much bett er with  movement direction  than with 
target location. Figure 8.12a shows a neuron’s  activity  
when movements were initiated fr om a center location 
to eight radial locations. Th is cell was most strongly 
activated (red arrows in Figure 8.12a) when the move
ment was toward the animal. Figure 8.12b shows results 
f
 r om the same cell when movements were initiated at ra
dial locations and always ended at the center position. In 
this condition, the cell was most active (Figure 8.12b, red 
arrows) for movements initiated fr om the most distant 
position; movement was again toward the animal. Many 
cells in motor areas show directional tuning, or exhibit 
what is referred to as a  preferred direction . Th is tun
ing is relatively broad. For example, the cell shown in 
Figure 8.12 shows a signifi cant increase in activity  for 
movements in four of the eight directions. An experi
menter would be hard-pressed to predict the direction of 
an ongoing movement if he were observing only the activity  of this  individual cell. 
We can assume, however, that activity  is distrib
uted across many cells, each with their unique preferred 
direction. To provide a more global representation, 
Georgopoulos and his colleagues introduced the concept 
of the  population vector  (Figure 8.13). Th e idea is quite 
simple: Each neuron can be considered to be contribut
ing a “vote” to the overall activity  level. Th e strength of 
the vote will correspond to how closely the movement 
matches the cell’s preferred direction: If the match is 
close, the cell will fi re strongly; if the match is poor, the 
cell will fi re weakly or even be inhibited. Th us, the activ
ity  of each neuron can be described as a vector, oriented 
to the cell’s preferred direction with a strength equal to 
its fi ring rate. Th e population vector is the sum of all the 
individual vectors. 
 The population vector has proved to be a powerful 
tool in motor neurophysiology. With relatively small 
numbers of neurons (e.g., 30–50), the population 
vector provides an excellent predictor of movement 
 direction. Th e population vector is not limited to simple 
2-D movements; it also has proven eff ective at repre
senting movements in 3-D space. Interestingly, neural 
activity  in many motor areas appears to be correlated 
with movement direction. 
It is important to kee p in mind that the physiological 
method is inherently correlational. Directional tuning is 
prevalent in motor areas, but this does not mean that di
rection is the key variable represented in the brain. Note 
that the experiment outlined in Figure 8.12 contains 
a critical confound. We can describe the data in terms 
of movement direction, interpreting the results to show 
that the cell is active for movements toward the animal. 
To move in this direction, the animal activates the biceps 
muscle to produce fl exion about the elbow. From these 
data, we do not know if the cell is coding direction, or the level of biceps activation when the  elbow is being fl exed, 
or some other parameter correlated with these variables. 
Subsequent experiments have addressed this problem. 
The results are, as so oft en happens when looking at the 
brain, complex. Within any given area, a mixture of rep
resentations is found. Th e activity  of some cells is best 
correlated with external movement direction, and the 
 activity  of other cells with parameters more closely linked 
to muscular activation patt erns ( Kakei et al., 1999).
Alternative Perspectives on Neural 
Representation of Movement 
 The population vector is dynamic and can be calculated 
continuously over time. Indee d, aft er defi ning the pre
ferred direction of a set of neurons, we can calculate the 
population vector fr om the activation of that set of neurons 
even before the animal starts to move. To do this, and pro
vide a way to dissociate planning- and movement- related  activity , experimenters fr equently impose a  delay period. 
The animal is fi rst given a cue indicating the direction of 
a forthcoming movement and then required to wait for a 
“go” signal before initiating the movement (Figure 8.14). 
This procedure reveals that the population vector shifts in 
the direction of the upcoming movement well  before the 
movement is  produced , suggesting that at least some of the 
cells are involved in planning the  movement and not simply 
recruited once the movement is being executed. In fact, 
by looking at the population vector, which was recorded 
more than 300 ms  before  the movement, the direction of 
the forthcoming movement can be precisely predicted. 
This result may not sound like that big of a deal to you, 
but it put motor researchers into a fr enzy—although not 
until about 10 years aft er Georgopolous’s initial studies on 
the population vector. With hindsight, can you see  why? 
As a hint, consider how this fi nding might be used to help 
people with spinal cord  injuries. We will explore this a bit 
later in the section called “The Brain–Machine Interface.” 
Even though directional tuning and population vectors 
have become cornerstone concepts in motor neurophysi
ology, it is also important to consider that many cells do not show strong directional tuning. Even more puzzling, 
the tuning may be inconsistent: Th e tuning  exhibited by 
a cell before movement begins may shift  during the ac
tual movement (Figure 8.15a). What’s more, many cells 
that exhibit an increase of activity  during the delay phase 
show a brief drop in activity  just before movement begins 
(Figure 8.15b), or a diff erent fi ring patt ern in preparation 
and execution of a movement (Figure 8.15c). Th is result 
is at odds with the assumption that the planning phase is 
just a weaker, or subthreshold version of the cell’s activity  
during the movement phase. 
Perimovement activity
 What are we to make of these unexpected findings, 
in which the tuning properties change over the course of 
an action? Mark Churchland and his colleagues (2012) 
suggest that we need a radically diff erent perspective on 
motor neurophysiology. Rather than viewing neurons as 
static representational devices (e.g., with a fi xed direc
tional tuning), we should focus on the dynamic proper
ties of neurons, recognizing that movement arises as the 
set of neurons move from one state to another. By this 
view, we might see that neurons wear many hats, coding 
 different features depending on time and context. There 
need not be a simple mapping fr om behavior to neural 
activity. Indeed, given the challenge of using limbs with complex biomechanics to interact with a wide range of 
objects and environments, we might expect the nervous 
system to have evolved such that information is repre
sented in a multidimensional format, coding a wide range 
of variables such as force, velocity , and context. Th is form 
of representation may be harder for the experimenter to 
decode, but it is likely an important adaptation that gives 
the motor system maximum fl exibility  (not to mention 
job stability  for neurophysiologists). 
Although scientists refer to one part of the brain as 
motor cortex and another region as sensory cortex, we 
know that these areas are closely entw ined with one 
another. People produce movements in anticipation of 
their sensory consequences: We increase the force used 
to grip and lift  a full cup of coff ee  in anticipation of the 
weight we expect to experience. Similarly, we use sensory 
information to adjust our actions. If the cup is empty , we 
quickly reduce the grip force to avoid moving the cup up
ward too quickly. Physiologists observe this interdepen
dency and have recognized for some time that the motor 
cortex isn’t just “motor,” and the sensory cortex isn’t just 
“sensory.” For example, in rats, the neurons that control 
whisker movements are predominantly in somatosensory 
cortex. 
In monkeys, sensory inputs rapidly reshape motor 
activity  (reviewed in Hatsopoulos & Suminski, 2011). 
In fact, some evidence suggests that the directional 
tuning of some motor cortex neurons is more about 
“sensory” tuning. Consider the same shoulder movement 
induced by tw o diff erent sensory events. One is caused by 
a nudge to the elbow and the other following a nudge to 
the shoulder. As early as 50 ms, well before the sensory 
signals in sensory cortex would have bee n processed and 
sent to the motor system, M1 neurons show diff erential 
responses to the tw o ty pes of nudges. It appears that the 
sensory information was processed within M1 directly, 
allowing for fast, nearly real-time fee dback (Pruszynski 
et al., 2011a, b). 
Taken together, the neurophysiological evidence 
points to a more nuanced picture than we might have 
anticipated fr om our hierarchical control model. Rather 
than a linkage of diff erent neural regions with specifi c 
levels in a processing hierarchy, one that moves fr om 
abstract to more concrete representations, the picture 
reveals an interactive netw ork of motor areas that rep
resent multiple features. Th is complexity  becomes even 
more apparent in the next section, when we turn our attention to motor planning.
The Brain–Machine Interface 
Can neural signals be used to control a movement directly 
with the brain, bypassing the intermediate stage of  muscles? 
For instance, could you plan an action in your  motor cortex 
(e.g., let’s fold the laundry), somehow connect those motor 
cortex neurons to a computer, and send the planned action 
to a robot, which would fold the laundry? Sounds extraor
dinary? Yet it is happening. The process is called a  brain
machine interface (BMI). It uses decoding principles to control brain– machine interface systems, 
which have incredible potential to improve the lives of 
people with spinal cord injuries, amputations, and other 
diseases that have  aff ected their ability  to move at will. 
Early Work on Brain–Machine 
Interface Systems 
John Chapin of the State University  of New York ( Chapin 
et al., 1999) provided one of the fi rst demonstrations of 
the viability  of a BMI by using a simple motor task in a 
highly motivated population: thirsty  rats. He fi rst trained 
the rats to press a butt on that caused a lever arm to  rotate. 
The lever was connected to a computer, which measured 
the pressure on the butt on and used this signal to adjust 
the position of a robot arm. One end of the lever contained 
a small well; if positioned properly, a few drops of water 
would fi ll the well. Th us, by learning to vary the pressure of 
the butt on press, the rat controlled the lever arm and could 
replenish the water and then spin the lever to take a drink 
(Figure 8.20). Chapin recorded fr om neurons in the motor 
cortex during this task, measuring the correlation betw ee n 
each neuron and the force output the rat used to adjust 
and move the lever. Once the rat’s behavior had stabilized, 
Chapin could construct an online population vector, one 
that matched the animal’s force output rather than move
ment direction. With as few as 30 or so neurons, the match 
betw ee n the population vector and behavior was excellent. 
Here is where things get interesting. Chapin then discon
nected the input of the butt on to the computer and instead 
used the output of the time-varying population vector as in
put to the computer to control the position of the lever arm. 
The rats still pushed the butt on, but that no longer controlled 
the lever; it was now controlled by their brain activity. If the activity  level in the vector was high, the arm swiveled in one 
direction; if low, it swiveled in the other direction, or even 
stopped the lever arm entirely. Amazingly, population vectors 
generated fr om as few as 25 neurons proved suffi  cient for the 
rats to successfully control the robot arm to obtain water. 
As impressive as this result was, Chapin could not, of 
course, tell the animals about the shift  fr om arm control to 
brain control. Unaware of the switch to BMI, the rats con
tinued to press and release the butt on. Over time, though, 
the animals became sensitive to the lack of a precise corre
lation betw een their arm movements and the lever position 
(the correlation was not perfect). Amazingly, they contin
ued to generate the cortical signals necessary to control the 
lever, but they also stopped moving their limb. Th ey learned 
they could kick back, relax, and simply think about pushing 
the butt on with the precision required to satiate their thirst. 
Over the past 20 years, research on brain–machine 
interface (BMI) systems has sky rocketed. Th ree  elements 
are required: microelectrode arrays implanted on the cortex 
to record neural activity , a computer with decoding algo
rithms, and a prosthetic eff ector. In the fi rst primate stud
ies, monkeys were trained to control the tw o-dimensional 
position of a computer cursor. With more sophisticated al
gorithms, these animals have learned to use BMI systems 
that control a robotic arm with multiple joints, moving the 
prosthetic limb through three -dimensional space to grasp 
food and bring it to their mouth (Velliste et al., 2008). 
Videos are available at htt p:// motorlab.neurobio.pitt .edu
 /multimedia.php. Besides controlling BMI with output 
from primary motor cortex, BMI also works with cells 
in premotor, supplementary motor, and parietal cortex 
(Carmena et al., 2003). Th e control algorithms have also 
become more advanced, adopting ideas fr om work on 
computer learning. Rather than use a serial process in 
which the directional tuning of the neurons is fi xed dur
ing the initial fr ee-movement stage, researchers now use 
computer algorithms that allow the tuning to be updated 
by real-time visual fee dback as the animal learns to control 
the BMI device (D. Taylor et al., 2002). 
Making Brain–Machine Interface 
Systems Stable 
One major challenge facing BMI researchers is how to establish a stable control system, one that can last for years. 
In a typical experiment, the animal starts each daily session 
by performing real movements to allow the researcher to 
construct the tuning profi les of each neuron. The process is 
rather like a daily recalibration. Once the neuron profiles are 
established, the BMI system is implemented. This approach, 
though, is not practical for BMI use as a clinical treatment. 
First, it is very difficult to record a fi xed set of neurons over 
a long period of time. Moreover, construction of neuron pro
files using real movements won’t be possible for BMI to be 
useful for paralyzed individuals or people who have lost a limb.  
To address this issue, researchers have looked at 
both the stability and flexibility of neural representations. 
Karunesh Ganguly and Jose Carmena (2009) at the 
 University  of California, Berkeley, implanted a grid of 
128 microelectrodes in the motor cortex of a monkey. Th is 
device allowed them to make continuous daily recordings. 
Although the signal fr om some electrodes would change 
from day to day, a substantial number of neurons remained 
stable for days (Figure 8.21). Using the output fr om this 
stable set, a BMI system successfully performed center-out 
reaching movements over a 3-wee k period. Th e animals 
achieved close to 100 % accuracy in reaching the targets, 
and the time required to complete each movement became 
much shorter over the 3-wee k period. This result suggested 
that with a stable decoder, the motor cortex neurons used a 
remarkably stable activation patt ern for prosthetic control. 
 The shocker came in the next experiment. Using these 
well-trained animals, researchers randomly shuffl  ed the 
decoder. For example, if a neuron had a preferred direction 
of 90 degrees, the algorithm was  altered so that the out
put of this neuron was now treated as if it had a preferred 
direction of 130  degrees. Th is new “stable”  decoder, of 
course, played havoc with BMI performance. Th e monkey 
would think “move up,” and the cursor would move side
ways. Over a few days of practice, however, the monkey 
was able to adapt to the new decoder, again reaching near
perfect performance (Figure 8.21c). With visual fee dback, 
the animal could learn to use a  decoder unrelated to arm 
movements. As long as the algorithm remained stable, it 
could actually reshape the decoder. Even more impressive, when the original decoder was reinstated, the animal 
again quickly adapted. Interestingly, with this adaptive 
system, the tuning functions of each neuron varied fr om 
one  context to the next and even  deviated fr om their 
shape during natural movement  (Figure 8.21d). It appears, 
then, that long-term  neuroprosthetic control leads to the 
 formation of a remarkably stable cortical map that is readily recalled and resistant to the storage of a second map. 
 Th ese results hold great promise for the translation 
of BMI research into the clinic. They demonstrate that 
the representation of individual neurons can be highly 
 flexible, adapting to the current context. Such flexibility is essential for ensuring that the system will remain stable 
over time, and it is also essential for using a single BMI 
system to control a host of devices such as computer cur
sors or eating utensils. It is reasonable to assume that a 
single set of neurons can learn to incorporate the diff er
ent challenges presented by devices that have no fr iction 
or mass (the position of a mouse on a computer scree n) 
to ones with large mass and complicated moving parts 
(a prosthetic arm or a robot). There is great urgency to get BMI ideas into clinical 
practice. Th e numbers of patients who would benefi t fr om 
such systems are huge. In the United States alone, over 
5.5 million people suff er some form of paralysis, either fr om 
injury or disease, and 1.7 million have limb loss. Th is nee d 
has motivated some scientists to move toward clinical tri
als in humans. John Donoghue and his colleagues at Brown University  presented the fi rst such trial, working with a 
patient, M.N., who became quadriplegic following a stab 
wound that severed his spinal cord. Th e researchers im
planted an array of microchips in the patient’s motor cortex 
(Hochberg et al., 2006). Despite 3 years of paralysis, the 
cells were quite active. Moreover, the fi ring level of the neu
rons varied as M.N. imagined diff erent ty pes of movements. 
Some units were active when he imagined making move
ments that involved the shoulder, others while imagining 
moving his hand. Th e researchers were also able to deter
mine the directional tuning profi les of each neuron, asking 
M.N. to imagine movements over a range of directions. 
 From this data, they created population vectors and used 
them as control signals for BMI interface devices. Using 
the output of approximately 100 neurons, M.N. was able 
to move a cursor around a computer screen. 
His responses were relatively slow and the path of the cur
sor somewhat erratic. Nonetheless, M.N. could control the 
cursor to open his e-mail, use soft ware programs to make 
drawings, or play computer games such as PONG. When 
connected to a  prosthetic limb, M.N. could control the 
opening and closing of the hand, a fi rst step to perform
ing much more complicated tasks. Another patient has 
learned, aft er months of training, to use a BMI system to 
control a robotic arm to reach and grasp objects ( Hochberg 
et al., 2012). 
BMI research is still in its infancy. This work, though, 
provides a compelling example of how basic findings in 
neuroscience—the coding of movement direction and 
population vector representations—can be combined 
with principles from bioengineering to develop vital clinical therapies. 
Newton's Law of Motions
Newton's laws of motion are of central importance 
in classical physics. A large number of principles and 
results may be derived from Newton's laws. The first 
two laws relate to the type of motion of a system that 
results from a given set of forces. These laws may be 
interpreted in a variety of ways and it is slightly 
uninteresting and annoying at the outset to go into the 
technical details of the interpretation. The precise 
definitions of mass, force and acceleration should be 
given before we relate) them. And these definitions 
themselves need use of Newton's laws. Thus, these 
laws turn out to be definitions to some extent. We shall 
assume that we know how to assign mass to a body, 
how to assign the magnitude and direction to a force 
and how to measure the acceleration with respect to 
a given frame of reference. Some discussions of these 
aspects were given in the previous chapters. The 
development here does not follow the historical track 
these laws have gone through, but are explained to 
make them simple to apply.  
5.1 FIRST LAW OF MOTION 
If the (vector) sum of all the forces acting on a 
particle is zero then and only then the particle remains 
unaccelerated (i.e., remains at rest or moves with 
constant velocity). 
If the sum of all the forces on a given particle is 
F and its acceleration is a, the above statement may 
also be written as 
" a = 0 if and only if F = 0 ". 
Thus, if the sum of the forces acting on a particle 
is known to be zero, we can be sure that the particle 
is unaccelerated, or if we know that a particle is 
unaccelerated, we can be sure that the sum of the 
forces acting on the particle is zero. 
However, the concept of rest, motion or 
acceleration is meaningful only when a frame of 
reference is specified. Also the acceleration of the 
particle is, in general, different when measured from 
different frames. Is it possible then, that the first law 
is valid in all frames of reference ? 
Let us consider the situation shown in figure (5.1). 
An elevator cabin falls down after the cable breaks. 
The cabin and all the bodies fixed in the cabin are 
accelerated with respect to the earth and the 
acceleration is about 9.8 m/s 2  in the downward 
direction.  
Consider the lamp in the cabin. The forces acting 
on the lamp are (a) the gravitational force W by the 
earth and (b) the electromagnetic force T (tension) by 
the rope. The direction of W is downward and the 
directon of T is upward. The sum is (W — T) downward. 
Measure the acceleration of the lamp from the 
frame of reference of the cabin. The lamp is at rest. 
The acceleration of the lamp is zero. The person A who 
measured this acceleration is a learned one and uses 
Newton's first law to conclude that the sum of the 
forces acting on the particle is zero, i.e., 
W — T = 0 or, W = T. 
Instead, if we measure the acceleration from the 
ground, the lamp has an acceleration of 9.8 m/s 2 . 
Thus, 
a 0 and hence the person B who measured this 
acceleration, concludes from Newton's first law that 
the sum of the forces is not zero. Thus, W — T # 0 or 
W T. If A measures acceleration and applies the first law he gets W = T. If B measures acceleration and 
applies the same first law, he gets W # T. Both of 
them cannot be correct simultaneously as W and T can 
Example 5.1 
65 
A heavy particle of mass 0.50 kg is hanging from a string 
be either equal or unequal. At least one of the two 
frames is a bad frame and one should not apply the 
first law in that frame. 
There are some frames of reference in which 
Newton's first law is valid. Measure acceleration from 
such a frame and you are allowed to say that " a = 0 
if and only if F = 0 ". But there are frames in which 
Newton's first law is not valid. You may find that even 
if the sum of the forces is not zero, the acceleration is 
still zero. Or you may find that the sum of the forces 
is zero, yet the particle is accelerated. So the validity 
of Newton's first law depends on the frame of reference 
from which the observer measures the state of rest, 
motion and acceleration of the particle. 
A frame of reference in which Newton's first law 
is valid is called an inertial frame of reference. A frame 
in which Newton's first law is not valid is called a 
noninertial frame of reference. 
Newton's first law, thus, reduces to a definition 
of inertial frame. Why do we call it a law then ? 
Suppose after going through this lesson, you keep the 
book on your table fixed rigidly with the earth 
(figure 5.2). 
Figure 5.2 
The book is at rest with respect to the earth. The 
acceleration of the book with respect to the earth is 
zero. The forces on the book are (a) the gravitational 
force W exerted by the earth and (b) the cojitact force 
sV by the table. Is the sum of W and dV zero ? A 
very accurate measurement will give the answer "No". 
The sum of the forces is not zero although the book is 
at rest. The earth is not strictly an inertial frame. 
However, the sum is not too different from zero and 
we can say that the earth is an inertial frame of 
reference to a good approximation. Thus, for routine 
affairs, "a = 0 if and only if F = 0" is true in the earth 
frame of reference. This fact was identified and 
formulated by Newton and is known as Newton's first 
law. If we restrict that all measurements will be made 
from the earth frame, indeed it becomes a law. If we 
try to universalise this to different frames, it becomes 
a definition. We shall assume that unless stated 
otherwise, we are working from an inertial frame of 
fixed with the roof Find the force exerted by the string 
on the particle (Figure 5.3). Take g = 9.8 m/s 2. 
Figure 5.3 
Solution : The forces acting on the particle are 
(a) pull of the earth, 0.50 kg x 9.8 m/s 2  = 4.9 N, vertically 
downward 
(b) pull of the string, T vertically upward. 
The particle is at rest with respect to the earth (which 
we assume to be an inertial frame). Hence, the sum of 
the forces should be zero. Therefore, T is 4.9 N acting 
vertically upward. 
Inertial Frames other than Earth  
Thus, a train moving with uniform velocity with 
respect to the ground, a plane flying with uniform 
velocity with respect to a highway, etc., are examples 
of inertial frames. The sum of the forces acting on a 
suitcase kept on the shelf of a ship sailing smoothly 
and uniformly on a calm sea is zero. 
SECOND LAW OF MOTION
For a given action of force, there is a equal and opposite reactions. 
The acceleration of a particle as measured from an 
inertial frame is given by the (vector) sum of all the 
forces acting on the particle divided by its mass. 
NEWTON'S THIRD LAW OF MOTION 
Thus, the force exerted by A on B and that by B 
on A are equal in magnitude but opposite in direction. 
This law connects the forces exerted by two bodies on 
one another. The forces connected by the third law act 
on two different bodies and hence will never appear 
together in the list of forces at step 2 of applying 
Newton's first or second law. 
For example, suppose a table exerts an upward 
force SV on a block placed on it. This force should be 
accounted if we consider the block as the system. The 
block pushes the table down with an equal force dV. 
But this force acts on the table and should be 
considered only if we take the table as the system. 
Thus, only one of the two forces connected by the third 
law may appear in the equation of motion depending 
on the system chosen. The force exerted by the earth 
on a particle of mass M is Mg downward and therefore, 
by the particle on the earth is Mg upward. These two 
forces will not cancel each other. The downward force 
on the particle will cause acceleration of the particle 
and that on the earth

 
 





















