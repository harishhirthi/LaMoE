{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431146de",
   "metadata": {},
   "source": [
    "### Training and Evaluation of LaMoE language model.\n",
    "\n",
    "> This notebook walks through the training and evaluation of LaMoE model using the train and validation/test data.\n",
    "\n",
    "**!!Primary Note!!:** Make sure **train.h5**, **val.h5** and **Tokenizer.json** is created using *Dataset.ipynb*. \n",
    "\n",
    "**!!Caution Note!!:** This training and evluation uses **mlflow** for logging. So, make sure you started the mlfow\n",
    "                      tracking server using **mlflow ui** in terminal opened in current directory and then start the training.\n",
    "\n",
    "*Note*: To make changes in model, training and evaluation configuration, edit *config.py* to make changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ee7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import urllib.request\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fe4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Envs\\Projects\\Transformer_Decoder\\MOE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Envs\\torch_env\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0962cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=30, uuid=eafe3e1a-82bb-fec8-2948-094f6277e5f8, L2_cache_size=3MB)\n"
     ]
    }
   ],
   "source": [
    "from lamoe.transformer import Transformer\n",
    "from lamoe.config import ModelArgs, TrainEvalArgs\n",
    "from lamoe.utils import get_data, get_vocab_size, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dffc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity score is used to evaluate quality of text generation. It is computed as exponential of Cross entropy loss.\n",
    "# Lower the perplexity, model is good in text generation.\n",
    "def perplexity_score(loss: torch.Tensor) -> float:\n",
    "    return torch.exp(loss).item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to perform evaluation on train and validation data.\n",
    "@torch.no_grad()\n",
    "def estimate_model(train_data: np.ndarray,\n",
    "                   val_data: np.ndarray,\n",
    "                   model: Transformer,\n",
    "                   model_args: ModelArgs, \n",
    "                   train_eval_args: TrainEvalArgs, \n",
    "                   loss_criterion: nn.CrossEntropyLoss) -> defaultdict:\n",
    "    \n",
    "    out = defaultdict(dict)\n",
    "    data = {\"train\": train_data, \"val\": val_data}\n",
    "    model.eval()\n",
    "    for key, split in data.items():\n",
    "        losses = torch.zeros(train_eval_args.max_eval_iter)\n",
    "        perplexities = torch.zeros(train_eval_args.max_eval_iter)\n",
    "        for k in range(train_eval_args.max_eval_iter):\n",
    "\n",
    "            x, y = get_batch(split, model_args.max_seq_length, model_args.max_batch_size)\n",
    "            logits, aux_loss = model(x.to(model_args.device))\n",
    "            task_loss = loss_criterion(logits.view(-1, logits.size(2)), y.view(-1).to(model_args.device))\n",
    "\n",
    "            perplexity = perplexity_score(task_loss)\n",
    "            perplexities[k] = perplexity\n",
    "\n",
    "            total_loss = task_loss + aux_loss if aux_loss is not None else task_loss\n",
    "            losses[k] = total_loss.item()\n",
    "\n",
    "        out[key] = {\"loss\": losses.mean(), \"perplexity\": perplexities.mean().item()}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to train the model.\n",
    "def train(model_args: ModelArgs,\n",
    "          train_eval_args: TrainEvalArgs,\n",
    "          model: Transformer,\n",
    "          train_data: np.ndarray,\n",
    "          val_data: np.ndarray,\n",
    "          ckpt_dir: Path) -> None:\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlopen(\"http://127.0.0.1:5000/\").getcode()\n",
    "        run_name = f\"Experiment-{np.random.randint(1e6)}\"\n",
    "        mlflow.set_experiment(\"MoE Training\")\n",
    "        mlflow.set_tracking_uri(uri = \"http://127.0.0.1:5000/\") \n",
    "        print(\"Mlflow current run name inside MoE Training: \", run_name, \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"Mlflow tracking server is not initiated. Initiate the server to start the training.\")\n",
    "        print(\"To start the tracking server. Open terminal in parent directory and type mlflow ui\\n\")\n",
    "        sys.exit()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = train_eval_args.lr, weight_decay = train_eval_args.weight_decay, eps = 1e-8)\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    val_temp = 0\n",
    "    print(\".......................................Executing training of the model.......................................\\n\")\n",
    "    with mlflow.start_run(run_name = run_name):\n",
    "        params = {\"Num_layers\": model_args.n_layers, \"Num_Q_heads\": model_args.n_heads, \"Num_KV_heads\": model_args.n_kv_heads,\n",
    "                  \"Num_experts\": model_args.num_experts, \"Top_experts\": model_args.k, \"Vocab_size\": model_args.vocab_size,\n",
    "                  \"Dimension\": model_args.dim, \"batch_size\": model_args.max_batch_size , \"context_length\" : model_args.max_seq_length, \n",
    "                  \"Max_iters\": train_eval_args.max_train_iter, \"eval_interval\": train_eval_args.eval_interval,  \n",
    "                  \"Device\": model_args.device, \"eval_iters\": train_eval_args.max_eval_iter, \"aux_loss_coeff\": model_args.aux_loss_coeff, \n",
    "                  \"optimizer\": \"AdamW\", \"learning_rate\": train_eval_args.lr, \"weight_decay\": train_eval_args.weight_decay}\n",
    "        mlflow.log_params(params) # Logging of params\n",
    "\n",
    "        for iter in tqdm(range(train_eval_args.max_train_iter)):\n",
    "\n",
    "            x, y = get_batch(train_data, model_args.max_seq_length, model_args.max_batch_size)\n",
    "            model.train()\n",
    "            logits, aux_loss = model(x.to(model_args.device))\n",
    "            task_loss = loss_criterion(logits.view(-1, logits.size(2)), y.view(-1).to(model_args.device))\n",
    "            mlflow.log_metric(\"Task_Loss\", task_loss.item(), step = iter)\n",
    "\n",
    "            perplexity = perplexity_score(task_loss)\n",
    "            mlflow.log_metric(\"Perplexity\", perplexity, step = iter)\n",
    "\n",
    "            if aux_loss is not None:\n",
    "                total_loss = task_loss + aux_loss \n",
    "                mlflow.log_metric(\"Aux_Loss\", aux_loss.item(), step = iter)\n",
    "            else:\n",
    "                total_loss = task_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (iter % train_eval_args.eval_interval == 0) or (iter == train_eval_args.max_train_iter - 1):\n",
    "                estimates = estimate_model(train_data, val_data, model, model_args, train_eval_args, loss_criterion)\n",
    "                print(f\"\\nStep {iter}: Train Loss - {estimates['train']['loss']:.4f}, Train Perplexity - {estimates['train']['perplexity']:.4f}, Val Loss - {estimates['val']['loss']:.4f}, Val Perplexity -  {estimates['val']['perplexity']:.4f}\")\n",
    "                \n",
    "                if iter == 0:\n",
    "                    files_list = glob.glob(os.path.join(ckpt_dir, \"*.pth\"))\n",
    "                    if files_list:\n",
    "                        for file in files_list:\n",
    "                            os.remove(file)\n",
    "                    val_temp = estimates['val']['loss']\n",
    "                    training_state = dict()\n",
    "                    training_state['ckpt_state_dict'] = model.state_dict()\n",
    "                    training_state['optimizer_state'] = optimizer.state_dict()\n",
    "                    ckpt_name = f\"checkpoint-{iter}-{val_temp:.3f}.pth\"\n",
    "                    print(f\"Saving first checkpoint in {os.path.join(ckpt_dir, ckpt_name)}\")\n",
    "                    torch.save(training_state, os.path.join(ckpt_dir, ckpt_name))\n",
    "                    \n",
    "                if (iter > 0) and (estimates['val']['loss'] < val_temp):\n",
    "                    files_list = glob.glob(os.path.join(ckpt_dir, \"*.pth\"))\n",
    "                    if files_list:\n",
    "                        for file in files_list:\n",
    "                            os.remove(file)\n",
    "                    print(f\"Val loss improved from {val_temp:.4f} to {estimates['val']['loss']:.4f}.\")\n",
    "                    val_temp = estimates['val']['loss']\n",
    "                    training_state = dict()\n",
    "                    training_state['ckpt_state_dict'] = model.state_dict()\n",
    "                    training_state['optimizer_state'] = optimizer.state_dict()\n",
    "                    ckpt_name = f\"checkpoint-{iter}-{val_temp:.3f}.pth\"\n",
    "                    print(f\"Saving checkpoint in {os.path.join(ckpt_dir, ckpt_name)}\")\n",
    "                    torch.save(training_state, os.path.join(ckpt_dir, ckpt_name))\n",
    "\n",
    "\n",
    "                metrics = {\"Train_Loss\": float(estimates['train']['loss']), \"Train_Perplexity\": float(estimates['train']['perplexity']),\n",
    "                           \"Val_Loss\": float(estimates['val']['loss']), \"Val_Perplexity\": float(estimates['val']['perplexity'])}\n",
    "                mlflow.log_metrics(metrics, step = iter)\n",
    "\n",
    "        print()\n",
    "        model_name = \"MoE-LM.pth\"\n",
    "        torch.save(model.state_dict(), os.path.join(Path(ckpt_dir).parent, model_name))\n",
    "        print(f\"Training is completed successfully. Final model is saved in {os.path.join(Path(ckpt_dir).parent, model_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a37041",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(\"Saved\", \"model\")\n",
    "checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e27fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in training data is 2286222. Tokens in validtion/test data is 616263.\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved train and validation/test tokens.\n",
    "train_data = get_data(os.path.join('Saved', 'train.h5'))\n",
    "val_data = get_data(os.path.join('Saved', 'val.h5'))\n",
    "print(f\"Tokens in training data is {len(train_data)}. Tokens in validtion/test data is {len(val_data)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23ce5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Saved\\Tokenizer.json' exists. Loading dictionary values from 'Saved\\Tokenizer.json'.\n",
      "Size of Vocabulary:  29627\n"
     ]
    }
   ],
   "source": [
    "vocab_size = get_vocab_size(os.path.join('Saved', 'Tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d938c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Args:  ModelArgs(dim=512, ffn_hidden_dim=2048, n_layers=4, n_heads=8, n_kv_heads=4, vocab_size=29627, norm_eps=1e-05, num_experts=8, k=2, eos='<eos>', pad='<pad>', unk='<unk>', aux_loss=True, aux_loss_coeff=0.01, inference=False, cache=False, max_batch_size=32, max_seq_length=300, device=device(type='cuda'))\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArgs()\n",
    "model_args.vocab_size = vocab_size\n",
    "print(\"Model Args: \", model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec1ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters : 134.189 M\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(model_args)\n",
    "model.to(model_args.device)\n",
    "\n",
    "Num_of_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(\"Model Parameters : {:.3f} M\".format(Num_of_parameters / 1e6)) # Prints Total number of Model Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18655acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 300]) Output shape:  torch.Size([32, 300])\n",
      "\n",
      "Summary of the model:\n",
      " ===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "â”œâ”€Embedding: 1-1                              [-1, 300, 512]            15,169,024\n",
      "â”œâ”€ModuleList: 1                               []                        --\n",
      "|    â””â”€Block: 2-1                             [-1, 300, 512]            --\n",
      "|    |    â””â”€RMSNorm: 3-1                      [-1, 300, 512]            512\n",
      "|    |    â””â”€MHA: 3-2                          [-1, 300, 512]            787,456\n",
      "|    |    â””â”€RMSNorm: 3-3                      [-1, 300, 512]            512\n",
      "|    |    â””â”€MoE: 3-4                          [-1, 300, 512]            25,174,032\n",
      "|    â””â”€Block: 2-2                             [-1, 300, 512]            --\n",
      "|    |    â””â”€RMSNorm: 3-5                      [-1, 300, 512]            512\n",
      "|    |    â””â”€MHA: 3-6                          [-1, 300, 512]            787,456\n",
      "|    |    â””â”€RMSNorm: 3-7                      [-1, 300, 512]            512\n",
      "|    |    â””â”€MoE: 3-8                          [-1, 300, 512]            25,174,032\n",
      "|    â””â”€Block: 2-3                             [-1, 300, 512]            --\n",
      "|    |    â””â”€RMSNorm: 3-9                      [-1, 300, 512]            512\n",
      "|    |    â””â”€MHA: 3-10                         [-1, 300, 512]            787,456\n",
      "|    |    â””â”€RMSNorm: 3-11                     [-1, 300, 512]            512\n",
      "|    |    â””â”€MoE: 3-12                         [-1, 300, 512]            25,174,032\n",
      "|    â””â”€Block: 2-4                             [-1, 300, 512]            --\n",
      "|    |    â””â”€RMSNorm: 3-13                     [-1, 300, 512]            512\n",
      "|    |    â””â”€MHA: 3-14                         [-1, 300, 512]            787,456\n",
      "|    |    â””â”€RMSNorm: 3-15                     [-1, 300, 512]            512\n",
      "|    |    â””â”€MoE: 3-16                         [-1, 300, 512]            25,174,032\n",
      "â”œâ”€RMSNorm: 1-2                                [-1, 300, 512]            512\n",
      "â”œâ”€Linear: 1-3                                 [-1, 300, 29627]          15,169,024\n",
      "===============================================================================================\n",
      "Total params: 134,188,608\n",
      "Trainable params: 134,188,608\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 241.21\n",
      "===============================================================================================\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 93.59\n",
      "Params size (MB): 511.89\n",
      "Estimated Total Size (MB): 605.52\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data, model_args.max_seq_length, model_args.max_batch_size)\n",
    "print(\"Input shape: \", x.shape, \"Output shape: \", y.shape)\n",
    "print()\n",
    "print(\"Summary of the model:\\n\", summary(model, [(x)], device = model_args.device, verbose = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43fb91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/21 13:44:25 INFO mlflow.tracking.fluent: Experiment with name 'MoE Training' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mlflow current run name inside MoE Training:  Experiment-273889 \n",
      "\n",
      ".......................................Executing training of the model.......................................\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0: Train Loss - 10.1466, Train Perplexity - 25499.7773, Val Loss - 10.1594, Val Perplexity -  25829.4590\n",
      "Saving first checkpoint in Saved\\model\\checkpoints\\checkpoint-0-10.159.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [01:25<01:59,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10: Train Loss - 7.9938, Train Perplexity - 2947.1538, Val Loss - 8.0461, Val Perplexity -  3101.9954\n",
      "Val loss improved from 10.1594 to 8.0461.\n",
      "Saving checkpoint in Saved\\model\\checkpoints\\checkpoint-10-8.046.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [02:45<01:03,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 20: Train Loss - 7.8404, Train Perplexity - 2530.9663, Val Loss - 7.9065, Val Perplexity -  2701.8994\n",
      "Val loss improved from 8.0461 to 7.9065.\n",
      "Saving checkpoint in Saved\\model\\checkpoints\\checkpoint-20-7.906.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [04:01<00:06,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 29: Train Loss - 7.7290, Train Perplexity - 2266.8250, Val Loss - 7.7793, Val Perplexity -  2382.4126\n",
      "Val loss improved from 7.9065 to 7.7793.\n",
      "Saving checkpoint in Saved\\model\\checkpoints\\checkpoint-29-7.779.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [04:23<00:00,  8.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training is completed successfully. Final model is saved in Saved\\model\\MoE-LM.pth\n",
      "ðŸƒ View run Experiment-273889 at: http://127.0.0.1:5000/#/experiments/818474842340393126/runs/c0703b1bf90c4642898e9ef032caac4b\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/818474842340393126\n"
     ]
    }
   ],
   "source": [
    "train_eval_args = TrainEvalArgs()\n",
    "train(model_args, train_eval_args, model, train_data, val_data, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6260587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
