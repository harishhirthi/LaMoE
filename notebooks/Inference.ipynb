{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219579a6",
   "metadata": {},
   "source": [
    "### Inference of LaMoE language model.\n",
    "\n",
    "> This notebook walks through the inference using the trained LaMoE model. It uses top-p sampling and temperature for selecting new token.\n",
    "\n",
    "**Working:** First, it asks for the user prompt and model generates text and prints sequentially. \n",
    "\n",
    "**Note:** To exit the inference, type exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071a64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714a7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Envs\\Projects\\Transformer_Decoder\\MOE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Envs\\torch_env\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c141c3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=30, uuid=eafe3e1a-82bb-fec8-2948-094f6277e5f8, L2_cache_size=3MB)\n"
     ]
    }
   ],
   "source": [
    "from lamoe.transformer import Transformer\n",
    "from lamoe.config import ModelArgs\n",
    "from lamoe.tokenizer import BPE\n",
    "from lamoe.utils import preprocess_Eng, get_vocab, get_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f513af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs: torch.Tensor, top_p: float) -> torch.Tensor:\n",
    "\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim = -1, descending = True)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim = -1)\n",
    "        mask = probs_sum - probs_sort > top_p\n",
    "        probs_sort[mask] = 0.0\n",
    "        probs_sort.div_(probs_sort.sum(dim = -1, keepdim = True))\n",
    "        next_token = torch.multinomial(probs_sort, num_samples = 1)\n",
    "        next_token = torch.gather(probs_idx, -1, next_token)\n",
    "\n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0494279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Saved\\Tokenizer.json' exists. Loading dictionary values from 'Saved\\Tokenizer.json'.\n",
      "Size of Vocabulary:  29627\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPE()\n",
    "tokenizer_dict = get_vocab(os.path.join('Saved', 'Tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43775c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_completion(model: Transformer, \n",
    "                    args: ModelArgs, \n",
    "                    prompts: list[str], \n",
    "                    temperature: float = 0.6, \n",
    "                    top_p: float = 0.9, \n",
    "                    max_gen_len: Optional[int] = None) -> tuple[list, list]:\n",
    "\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = args.max_seq_length - 1\n",
    "\n",
    "        prompts = [preprocess_Eng(prompt) for prompt in prompts]\n",
    "        prompt_tokens = [tokenizer.encode(prompt, tokenizer_dict, None) for prompt in prompts]\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert batch_size <= args.max_batch_size, f\"Batch size must be less than or equal to {args.max_batch_size}\"\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        assert max_prompt_len <= args.max_seq_length, f\"Sequence length must be less than or equal to {args.max_seq_length}\"\n",
    "        total_len = min(args.max_seq_length, 64 + max_prompt_len)\n",
    "\n",
    "        pad_id = tokenizer_dict['vocab_dict'].get(args.pad)\n",
    "        eos_id = tokenizer_dict['vocab_dict'].get(args.eos)\n",
    "        tokens = torch.full((batch_size, total_len), pad_id, dtype = torch.long, device = args.device)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, :len(t)] = torch.tensor(t, dtype = torch.long, device = args.device)\n",
    "\n",
    "        eos_reached = torch.tensor([False] * batch_size, device = args.device)\n",
    "        prompt_tokens_mask = tokens != pad_id\n",
    "\n",
    "        dot_cycle = ['.', '..', '...', '....'] * total_len\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            with tqdm(total = total_len, desc = \"Generating Text\", bar_format=\"{desc} {postfix}\") as pbar:\n",
    "                for pos in range(1, total_len):\n",
    "                    with torch.no_grad():\n",
    "                            logits, _ = model.forward(token[pos - 1:pos].unsqueeze(0), start_pos = pos)\n",
    "\n",
    "                    if temperature > 0:\n",
    "                        probs = torch.softmax(logits[:, -1] / temperature, dim = -1)\n",
    "                        next_token = sample_top_p(probs, top_p)\n",
    "                    else:\n",
    "                        next_token = torch.argmax(logits[:, -1], dim = -1)\n",
    "\n",
    "                    next_token = next_token.reshape(-1)\n",
    "\n",
    "                    next_token = torch.where(prompt_tokens_mask[i, pos], tokens[i, pos], next_token)\n",
    "                    token[pos] = next_token\n",
    "\n",
    "                    eos_reached |= (~prompt_tokens_mask[:, pos]) & (next_token == eos_id)\n",
    "                    if all(eos_reached):\n",
    "                        break\n",
    "                    \n",
    "                    dots = dot_cycle[pos-1 % len(dot_cycle)]\n",
    "                    pbar.set_postfix_str(dots)\n",
    "                    time.sleep(0.01)\n",
    "                    pbar.update(1)\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            if eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(tokenizer.decode(current_prompt_tokens, tokenizer_dict))\n",
    "        return (out_tokens, out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75624ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Args:  ModelArgs(dim=512, ffn_hidden_dim=2048, n_layers=4, n_heads=8, n_kv_heads=4, vocab_size=29627, norm_eps=1e-05, num_experts=8, k=2, eos='<eos>', pad='<pad>', unk='<unk>', aux_loss=False, aux_loss_coeff=0.01, inference=True, cache=True, max_batch_size=32, max_seq_length=300, device=device(type='cuda'))\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArgs(inference = True)\n",
    "model_args.vocab_size = len(tokenizer_dict['vocab_dict'])\n",
    "model_args.aux_loss = False\n",
    "print(\"Model Args: \", model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f88e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded with trained weights successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(model_args) \n",
    "\n",
    "model_path = os.path.join('Saved', \"model\", \"MoE-LM.pth\")\n",
    "\n",
    "try:\n",
    "        model.load_state_dict(torch.load(model_path, weights_only = True, map_location = 'cpu'), strict = True)\n",
    "        model.to(model_args.device)\n",
    "        print(\"Model is loaded with trained weights successfully.\\n\")\n",
    "except Exception as e:\n",
    "        print(e, f\"\\n{model_path} is not present. Train the model to get final {model_path}\")\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87866835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------- Model Summary --------------------------------------------------------\n",
      "\n",
      "Model size: 512.232 MB\n",
      "Number of Experts: 8 (Loaded) vs 2 (Inference)\n",
      "Total params: 134.188 M (Loaded)                             || Total params: 58.692 M (Inference)\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Name          Parameters(M)                                  || Name          Parameters(M)\n",
      "Embeddings           15.169                                  || Embeddings           15.169\n",
      "Layers - 4          103.85                                   || Layers - 4           28.354\n",
      "LLM-Head             15.169                                  || LLM-Head             15.169\n",
      "\n",
      "Layer: 25.961 M                                              || Layer: 7.087 M\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Name      Parameters(M)                                      || Name      Parameters(M)\n",
      "MHA               0.787                                      || MHA               0.787\n",
      "MOE              25.174                                      || MOE               6.3\n",
      "\n",
      "MoE params: 25.174 M                                         || MoE params: 6.3 M\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Name       Parameters(M)                                     || Name       Parameters(M)\n",
      "Router             0.008                                     || Router             0.008\n",
      "Experts           25.166                                     || Experts            6.292\n",
      "\n",
      "Expert params: 3.146 M                                       || Expert params: 3.146 M\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Name      Parameters(M)                                      || Name      Parameters(M)\n",
      "Expert            3.146                                      || Expert            3.146\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_model_info(model, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d374121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  String theory states that \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Text , ...."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      "S"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tring theory states that the forces are considered to be a possible explanation for the weak interaction Conservation of nature. The weak interaction Conservation laws are associated with Maxwells field equations and the weak interaction Conservation laws and the weak interaction Conservation laws under the Lagrangian laws of nature. The twobody problem is that the exact solutions. From the Lagrangian of the Standard Model the Lagrangian and the\n",
      "User:  Artificial Intelligence is the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Text , ....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      "Artificial Intelligence is the ultimate goal of understanding the evidence of the cosmological principlet. Theoretical cosmologists led astronomers to the discovery of a void. Using the Copernican Revolution the Copernican Revolution and the Copernican Revolution and the Copernican Revolution and the Copernican Revolution as the Copernican principle is the basis for the Copernican principle for the Copernican principle and the Copernican principle of nature of\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input(\"User: \")\n",
    "    if text == \"exit\":\n",
    "            break\n",
    "    print(\"User: \", text)\n",
    "    prompts = [text]\n",
    "    out_tokens, out_texts = (text_completion(model, model_args, prompts, temperature = 0.6, top_p = 0.9, max_gen_len = 150))\n",
    "    assert len(out_texts) == len(prompts)       \n",
    "    print(\"Model: \")\n",
    "    for i in range(len(out_texts)):\n",
    "        for word in f'{out_texts[i]}':\n",
    "            print(word, end = '', flush = True)\n",
    "            time.sleep(0.05)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e845884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
